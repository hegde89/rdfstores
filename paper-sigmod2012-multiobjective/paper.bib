
@book{cormen_introduction_2001,
	title = {Introduction to algorithms},
	publisher = {The {MIT} press},
	author = {T. H Cormen and C. E Leiserson and R. L Rivest and C. Stein},
	year = {2001}
},

@inproceedings{natsev_supporting_2001,
	title = {Supporting Incremental Join Queries on Ranked Inputs},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/vldb/NatsevCSLV01},
	author = {Apostol Natsev and {Yuan-Chi} Chang and John R. Smith and {Chung-Sheng} Li and Jeffrey Scott Vitter},
	year = {2001},
	pages = {281--290}
},

@misc{tolga_urhan_xjoin:_2000,
	title = {{XJoin:} A {Reactively-Scheduled} Pipelined Join Operator},
	shorttitle = {{XJoin}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=?doi=10.1.1.35.7968},
	abstract = {Wide-area distribution raises significant performance
problems for traditional query processing techniques as data
access becomes less predictable due to link congestion, load
imbalances, and temporary outages. Pipelined query execution
is a promising approach to coping with unpredictability in
such environments as it allows scheduling to adjust to the
arrival properties of the data. We have developed a
non-blocking join operator, called {XJoin,} which has a small
memory footprint, allowing many such operators to be active
in parallel. {XJoin} is optimized to produce initial results
quickly and can hide intermittent delays in data arrival by
reactively scheduling background processing. We show that
{XJoin} is an effective solution for providing fast query
responses to users even in the presence of slow and bursty
remote sources. 1 {Wide-Area} Query Processing The explosive
growth of the Internet and the World Wide Web has made
tremendous amounts of data available on-line. Emerging
standards ...},
	author = {Tolga Urhan and Michael J. Franklin},
	year = {2000}
},

@inproceedings{kader_rox:_2009,
	address = {Providence, Rhode Island, {USA}},
	title = {{ROX:} run-time optimization of {XQueries}},
	isbn = {978-1-60558-551-2},
	shorttitle = {{ROX}},
	url = {http://portal.acm.org/citation.cfm?id=1559845.1559910},
	doi = {10.1145/1559845.1559910},
	abstract = {Optimization of complex {XQueries} combining many {XPath} steps and joins is currently hindered by the absence of good cardinality estimation and cost models for {XQuery.} Additionally, the state-of-the-art of even relational query optimization still struggles to cope with cost model estimation errors that increase with plan size, as well as with the effect of correlated joins and selections.},
	booktitle = {Proceedings of the 35th {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Riham Abdel Kader and Peter Boncz and Stefan Manegold and Maurice van Keulen},
	year = {2009},
	keywords = {optimization, xml, xquery},
	pages = {615--626}
},

@inproceedings{godfrey_exploiting_2001,
	address = {New York, {NY,} {USA}},
	series = {{SIGMOD} '01},
	title = {Exploiting constraint-like data characterizations in query optimization},
	isbn = {1-58113-332-4},
	location = {Santa Barbara, California, United States},
	doi = {10.1145/375663.375749},
	abstract = {Query optimizers nowadays draw upon many sources of information about the database to optimize queries. They employ runtime statistics in cost-based estimation of query plans. They employ integrity constraints in the query rewrite process. Primary and foreign key constraints have long played a role in the optimizer, both for rewrite opportunities and for providing more accurate cost predictions. More recently, other types of integrity constraints are being exploited by optimizers in commercial systems, for which certain semantic query optimization techniques have now been implemented.
These new optimization strategies that exploit constraints hold the promise for good improvement. Their weakness, however, is that often the “constraints” that would be useful for optimization for a given database and workload are not explicitly available for the optimizer. Data mining tools can find such “constraints” that are true of the database, but then there is the question of how this information can be kept by the database system, and how to make this information available to, and effectively usable by, the optimizer.
We present our work on soft constraints in {DB2.} A soft constraint is a syntactic statement equivalent to an integrity constraint declaration. A soft constraint is not really a constraint, per se, since future updates may undermine it. While a soft constraint is valid, however, it can be used by the optimizer in the same way integrity constraints are. We present two forms of soft constraint: absolute and statistical. An absolute soft constraint is consistent with respect to the current state of the database, just in the same way an integrity constraint must be. They can be used in rewrite, as well as in cost estimation. A statistical soft constraint differs in that it may have some degree of violation with respect to the state of the database. Thus, statistical soft constraints cannot be used in rewrite, but they can still be used in cost estimation.
We are working long-term on absolute soft constraints. We discuss the issues involved in implementing a facility for absolute soft constraints in a database system (and in {DB2),} and the strategies that we are researching. The current {DB2} optimizer is more amenable to adding facilities for statistical soft constraints. In the short-term, we have been implementing pathways in the optimizer for statistical soft constraints. We discuss this implementation.},
	booktitle = {{ACM} {SIGMOD} Record},
	publisher = {{ACM}},
	author = {Parke Godfrey and Jarek Gryz and Calisto Zuzarte},
	year = {2001},
	note = {{ACM} {ID:} 375749},
	keywords = {design, management, measurement, performance, query processing, theory},
	pages = {582–592}
},

@inproceedings{gou_query_2006,
	address = {New York, {NY,} {USA}},
	series = {{SIGMOD} '06},
	title = {Query evaluation using overlapping views: completeness and efficiency},
	isbn = {1-59593-434-0},
	location = {Chicago, {IL,} {USA}},
	shorttitle = {Query evaluation using overlapping views},
	doi = {10.1145/1142473.1142479},
	abstract = {We study the problem of finding efficient equivalent view-based rewritings of relational queries, focusing on query optimization using materialized views under the assumption that base relations cannot contain duplicate tuples. A lot of work in the literature addresses the problems of answering queries using views and query optimization. However, most of it proposes solutions for special cases, such as for conjunctive queries {(CQs)} or for aggregate queries only. In addition, most of it addresses the problems separately under set or bag-set semantics for query evaluation, and some of it proposes heuristics without formal proofs for completeness or soundness. In this paper we look at the two problems by considering {CQ/A} queries - that is, both pure conjunctive and aggregate queries, with aggregation functions {SUM,} {COUNT,} {MIN,} and {MAX;} the {DISTINCT} keyword in {(SQL} versions of) our queries is also allowed. We build on past work to provide algorithms that handle this general setting. This is possible because recent results on rewritings of {CQ/A} queries [1, 8] show that there are sound and complete algorithms based on containment tests of {CQs.Our} focus is that our algorithms are efficient as well as sound and complete. Besides the contribution we make in putting and addressing the problems in this general setting, we make two additional contributions for bag-set and set semantics. First, we propose efficient sound and complete tests for equivalence of {CQ/A} queries to rewritings that use overlapping views (the algorithms are complete with respect to the language of rewritings). These results apply not only to query optimization, but to all areas where the goal is to obtain efficient equivalent view-based query rewritings. Second, based on these results we propose two sound algorithms, {BDPV} and {CDPV,} that find efficient execution plans for {CQ/A} queries in terms of materialized views. Both algorithms extend the cost-based query-optimization approach of System R [19]. The efficient sound algorithm {BDPV} is also complete in some cases, whereas {CDPV} is sound and complete for all {CQ/A} queries we consider. We present a study of the completeness-efficiency tradeoff in the algorithms, and provide experimental results that show the viability of our approach and test the limits of query optimization using overlapping views.},
	booktitle = {Proceedings of the 2006 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Gang Gou and Maxim Kormilitsin and Rada Chirkova},
	year = {2006},
	note = {{ACM} {ID:} 1142479},
	keywords = {algorithms, experimentation, materialized views, performance, query optimization, relational databases, rewriting queries using views, theory},
	pages = {37–48}
},

@inproceedings{chaudhuri_optimizing_1995,
	title = {Optimizing queries with materialized views},
	doi = {10.1109/ICDE.1995.380392},
	abstract = {While much work has addressed the problem of maintaining
materialized views, the important question of optimizing queries in the
presence of materialised views has not been resolved. In this paper, we
analyze the optimization question and provide a comprehensive and
efficient solution. Our solution has the desirable property that it is a
simple generalization of the traditional query optimization algorithm},
	booktitle = {Data Engineering, 1995. Proceedings of the Eleventh International Conference on},
	author = {S. Chaudhuri and R. Krishnamurthy and S. Potamianos and K. Shim},
	year = {1995},
	keywords = {materialized views, optimisation, query optimization algorithm, query processing},
	pages = {190--200}
},

@inproceedings{cox_probably_2009,
	address = {Berlin, Heidelberg},
	series = {{ICTIR} '09},
	title = {Probably Approximately Correct Search},
	isbn = {978-3-642-04416-8},
	location = {Cambridge, {UK}},
	url = {http://dx.doi.org/10.1007/978-3-642-04417-5_2},
	doi = {http://dx.doi.org/10.1007/978-3-642-04417-5_2},
	abstract = {We consider the problem of searching a document collection using a set of independent computers. That is, the computers do {\textless}em{\textgreater}not{\textless}/em{\textgreater} cooperate with one another either (i) to acquire their local index of documents or (ii) during the retrieval of a document. During the acquisition phase, each computer is assumed to randomly sample a subset of the entire collection. During retrieval, the query is issued to a random subset of computers, each of which returns its results to the query-issuer, who consolidates the results. We examine how the number of computers, and the fraction of the collection that each computer indexes, affects performance in comparison to a traditional deterministic configuration. We provide analytic formulae that, given the number of computers and the fraction of the collection each computer indexes, provide the probability of an approximately correct search, where a "correct search" is defined to be the result of a deterministic search on the entire collection. We show that the randomized distributed search algorithm can have acceptable performance under a range of parameters settings. Simulation results confirm our analysis.},
	booktitle = {Proceedings of the 2nd International Conference on Theory of Information Retrieval: Advances in Information Retrieval Theory},
	publisher = {{Springer-Verlag}},
	author = {Ingemar J Cox and Ruoxun Fu and Lars Kai Hansen},
	year = {2009},
	note = {{ACM} {ID:} 1612337},
	pages = {2–16}
},

@inproceedings{deligiannidis_rdf_2007,
	address = {Lisbon, Portugal},
	title = {{RDF} data exploration and visualization},
	isbn = {978-1-59593-831-2},
	url = {http://portal.acm.org/citation.cfm?id=1317362},
	doi = {10.1145/1317353.1317362},
	abstract = {We present Paged Graph Visualization {(PGV),} a new semi-autonomous tool for {RDF} data exploration and visualization. {PGV} consists of two main components: a) the {"PGV} explorer" and b) the {"RDF} pager" module utilizing {BRAHMS,} our high per-formance main-memory {RDF} storage system. Unlike existing graph visualization techniques which attempt to display the entire graph and then filter out irrelevant data, {PGV} begins with a small graph and provides the tools to incrementally explore and visualize relevant data of very large {RDF} ontologies. We implemented several techniques to visualize and explore hot spots in the graph, i.e. nodes with large numbers of immediate neighbors. In response to the user-controlled, semantics-driven direction of the exploration, the {PGV} explorer obtains the necessary sub-graphs from the {RDF} pager and enables their incremental visualization leaving the previously laid out sub-graphs intact. We outline the problem of visualizing large {RDF} data sets, discuss our interface and its implementation, and through a controlled experiment we show the benefits of {PGV.}},
	booktitle = {Proceedings of the {ACM} first workshop on {CyberInfrastructure:} information management in {eScience}},
	publisher = {{ACM}},
	author = {Leonidas Deligiannidis and Krys J. Kochut and Amit P. Sheth},
	year = {2007},
	keywords = {incremental data exploration, ontology visualization},
	pages = {39--46}
},

@article{yang_hybmig:_2007,
	title = {{HybMig:} A Hybrid Approach to Dynamic Plan Migration for Continuous Queries},
	volume = {19},
	issn = {1041-4347},
	shorttitle = {{HybMig}},
	doi = {http://doi.ieeecomputersociety.org/10.1109/TKDE.2007.43},
	abstract = {In data stream environments, the initial plan of a long-running query may gradually become inefficient due to changes of the data characteristics. In this case, the query optimizer will generate a more efficient plan based on the current statistics. The online transition from the old to the new plan is called dynamic plan migration. In addition to correctness, an effective technique for dynamic plan migration should achieve the following objectives: 1) minimize the memory and {CPU} overhead of the migration, 2) reduce the duration of the transition, and 3) maintain a steady output rate. The only known solutions for this problem are the moving states {(MS)} and parallel track {(PT)} strategies, which have some serious shortcomings related to the above objectives. Motivated by these shortcomings, we first propose {HybMig,} which combines the merits of {MS} and {PT} and outperforms both in every aspect. As a second step, we extend {PT,} {MS,} and {HybMig} to the general problem of migration, where both the new and the old plans are treated as black boxes.},
	number = {3},
	journal = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Yin Yang and J? Kr?mer and Dimitris Papadias and Bernhard Seeger},
	year = {2007},
	keywords = {query processing.},
	pages = {398--411}
},

@inproceedings{das_answering_2006,
	title = {Answering Top-k Queries Using Views},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/vldb/DasGKT06},
	author = {Gautam Das and Dimitrios Gunopulos and Nick Koudas and Dimitris Tsirogiannis},
	year = {2006},
	keywords = {top-k},
	pages = {451--462}
},

@article{weiss_hexastore:_2008,
	title = {Hexastore: sextuple indexing for semantic web data management},
	volume = {1},
	shorttitle = {Hexastore},
	url = {http://portal.acm.org/citation.cfm?id=1453965},
	doi = {10.1145/1453856.1453965},
	abstract = {Despite the intense interest towards realizing the Semantic Web vision, most existing {RDF} data management schemes are constrained in terms of efficiency and scalability. Still, the growing popularity of the {RDF} format arguably calls for an effort to offset these drawbacks. Viewed from a relational-database perspective, these constraints are derived from the very nature of the {RDF} data model, which is based on a triple format. Recent research has attempted to address these constraints using a vertical-partitioning approach, in which separate two-column tables are constructed for each property. However, as we show, this approach suffers from similar scalability drawbacks on queries that are not bound by {RDF} property value. In this paper, we propose an {RDF} storage scheme that uses the triple nature of {RDF} as an asset. This scheme enhances the vertical partitioning idea and takes it to its logical conclusion. {RDF} data is indexed in six possible ways, one for each possible ordering of the three {RDF} elements. Each instance of an {RDF} element is associated with two vectors; each such vector gathers elements of one of the other types, along with lists of the third-type resources attached to each vector element. Hence, a sextuple-indexing scheme emerges. This format allows for quick and scalable general-purpose query processing; it confers significant advantages (up to five orders of magnitude) compared to previous approaches for {RDF} data management, at the price of a worst-case five-fold increase in index space. We experimentally document the advantages of our approach on real-world and synthetic data sets with practical queries.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {Cathrin Weiss and Panagiotis Karras and Abraham Bernstein},
	year = {2008},
	pages = {1008--1019}
},

@inproceedings{zhang_towards_1993,
	title = {Towards efficient parallelization of equivalence checking algorithms},
	isbn = {0-444-89282-6},
	url = {http://portal.acm.org/citation.cfm?id=646211.683778},
	booktitle = {Proceedings of the {IFIP} {TC6/WG6.1} Fifth International Conference on Formal Description Techniques for Distributed Systems and Communication Protocols: Formal Description Techniques, V},
	publisher = {{North-Holland} Publishing Co.},
	author = {Shipei Zhang and Scott A. Smolka},
	year = {1993},
	keywords = {bisimulation, parallel},
	pages = {121--135}
},

@article{agrawal_lazy-adaptive_2009,
	title = {{Lazy-Adaptive} Tree: an optimized index structure for flash devices},
	volume = {2},
	shorttitle = {{Lazy-Adaptive} Tree},
	url = {http://portal.acm.org/citation.cfm?id=1687627.1687669&coll=GUIDE&dl=GUIDE&idx=J1174&part=journal&WantType=Journals&title=Proceedings%20of%20the%20VLDB%20Endowment&CFID=82166857&CFTOKEN=58994927},
	abstract = {Flash memories are in ubiquitous use for storage on sensor nodes, mobile devices, and enterprise servers. However, they present significant challenges in designing tree indexes due to their fundamentally different read and write characteristics in comparison to magnetic disks.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {Devesh Agrawal and Deepak Ganesan and Ramesh Sitaraman and Yanlei Diao and Shashi Singh},
	year = {2009},
	pages = {361--372}
},

@inproceedings{lenzerini_data_2002,
	address = {New York, {NY,} {USA}},
	series = {{PODS} '02},
	title = {Data integration: a theoretical perspective},
	isbn = {1-58113-507-6},
	location = {Madison, Wisconsin},
	shorttitle = {Data integration},
	doi = {10.1145/543613.543644},
	abstract = {Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.},
	booktitle = {Proceedings of the twenty-first {ACM} {SIGMOD-SIGACT-SIGART} symposium on Principles of database systems},
	publisher = {{ACM}},
	author = {Maurizio Lenzerini},
	year = {2002},
	note = {{ACM} {ID:} 543644},
	keywords = {database processing, design, mathematics and statistics, performance, theory},
	pages = {233–246}
},

@inproceedings{neumann_scalable_2009,
	address = {Providence, Rhode Island, {USA}},
	title = {Scalable join processing on very large {RDF} graphs},
	isbn = {978-1-60558-551-2},
	url = {http://portal.acm.org/citation.cfm?doid=1559845.1559911},
	doi = {10.1145/1559845.1559911},
	abstract = {With the proliferation of the {RDF} data format, engines for {RDF} query processing are faced with very large graphs that contain hundreds of millions of {RDF} triples. This paper addresses the resulting scalability problems. Recent prior work along these lines has focused on indexing and other physical-design issues. The current paper focuses on join processing, as the fine-grained and schema-relaxed use of {RDF} often entails star- and chain-shaped join queries with many input streams from index scans.},
	booktitle = {Proceedings of the 35th {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Thomas Neumann and Gerhard Weikum},
	year = {2009},
	keywords = {query processing, rdf},
	pages = {627--640}
},

@inproceedings{papadimitriou_multiobjective_2001,
	address = {New York, {NY,} {USA}},
	series = {{PODS} '01},
	title = {Multiobjective query optimization},
	isbn = {1-58113-361-8},
	location = {Santa Barbara, California, United States},
	doi = {10.1145/375551.375560},
	abstract = {The optimization of queries in distributed database systems is known to be subject to delicate trade-offs. For example, the Mariposa database system allows users to specify a desired delay-cost tradeoff (that is, to supply a decreasing function u(d), specifying how much the user is willing to pay in order to receive the query results within time d); Mariposa divides a query graph into horizontal “strides,” analyzes each stride, and uses a greedy heuristic to find the “best” plan for all strides. We show that Mariposa's greedy heuristic can be arbitrarily far from the desired optimum. Applying a recent approach in multiobjective optimization algorithms to this problem, we show that the optimum cost-delay trade-off {(Pareto)} curve in Mariposa's framework can be approximated fast within any desired accuracy. We also present a polynomial algorithm for the general multiobjective query optimization problem, which approximates arbirarily well the optimum cost-delay tradeoff (without the restriction of Mariposa's heuristic stride subdivision).},
	booktitle = {Proceedings of the twentieth {ACM} {SIGMOD-SIGACT-SIGART} symposium on Principles of database systems},
	publisher = {{ACM}},
	author = {Christos H Papadimitriou and Mihalis Yannakakis},
	year = {2001},
	note = {{ACM} {ID:} 375560},
	keywords = {languages, management, optimization, theory, verification},
	pages = {52–59}
},

@inproceedings{schmidt_foundations_2010,
	address = {Lausanne, Switzerland},
	title = {Foundations of {SPARQL} query optimization},
	isbn = {978-1-60558-947-3},
	url = {http://portal.acm.org/citation.cfm?id=1804675},
	doi = {10.1145/1804669.1804675},
	abstract = {We study fundamental aspects related to the efficient processing of the {SPARQL} query language for {RDF,} proposed by the {W3C} to encode machine-readable information in the Semantic Web. Our key contributions are (i) a complete complexity analysis for all operator fragments of the {SPARQL} query language, which -- as a central result -- shows that the {SPARQL} operator Optional alone is responsible for the {PSpace-completeness} of the evaluation problem, (ii) a study of equivalences over {SPARQL} algebra, including both rewriting rules like filter and projection pushing that are well-known from relational algebra optimization as well as {SPARQL-specific} rewriting schemes, and (iii) an approach to the semantic optimization of {SPARQL} queries, built on top of the classical chase algorithm. While studied in the context of a theoretically motivated set semantics, almost all results carry over to the official, bag-based semantics and therefore are of immediate practical relevance.},
	booktitle = {Proceedings of the 13th International Conference on Database Theory},
	publisher = {{ACM}},
	author = {Michael Schmidt and Michael Meier and Georg Lausen},
	year = {2010},
	keywords = {complexity, query optimization, rdf, semantic query optimization, sparql, sparql algebra},
	pages = {4--33}
},

@incollection{schenkel_hopi:_2004,
	title = {{HOPI:} An Efficient Connection Index for Complex {XML} Document Collections},
	shorttitle = {{HOPI}},
	url = {http://www.springerlink.com/content/4a7j0p1ax8eeart5},
	abstract = {In this paper we present {HOPI,} a new connection index for {XML} documents based on the concept of the 2–hop cover of a directed graph introduced by Cohen et al. In contrast to most of the prior work on {XML} indexing we consider not only paths with child or parent relationships between the nodes, but also provide space– and time–efficient reachability tests along the ancestor, descendant, and link axes to support path expressions with wildcards in our {XXL} search engine. We improve the theoretical concept of a 2–hop cover by developing scalable methods for index creation on very large {XML} data collections with long paths and extensive cross–linkage. Our experiments show substantial savings in the query performance of the {HOPI} index over previously proposed index structures in combination with low space requirements.},
	booktitle = {Advances in Database Technology - {EDBT} 2004},
	author = {Ralf Schenkel and Anja Theobald and Gerhard Weikum},
	year = {2004},
	pages = {665--666}
},

@inproceedings{langegger_rdfstats_2009,
	title = {{RDFStats} - An Extensible {RDF} Statistics Generator and Library},
	isbn = {978-0-7695-3763-4},
	url = {http://portal.acm.org/citation.cfm?id=1674635.1674691},
	abstract = {In this paper {RDFStats} is introduced, which is a generator for statistics of {RDF} sources like {SPARQL} endpoints and {RDF} documents. {RDFStats} does not only provide a statistics generator, but also a powerful {API} for persisting and accessing statistics including several estimation functions that also support {SPARQL} filter-like expressions. For many Semantic Web applications like the Semantic Web Integrator and Query Engine {(SemWIQ),} which is currently developed at the University of Linz, detailed statistics about the contents of {RDF} data sources are very important. {RDFStats} has been primarily designed and implemented for the {SemWIQ} federator and optimizer, but it can also be used for other applications like linked data browsers, aggregators, or visualization tools. It is based on the popular Semantic Web framework Jena developed by {HP} Labs Bristol and can be easily extended and integrated into other applications.},
	booktitle = {Proceedings of the 2009 20th International Workshop on Database and Expert Systems Application},
	publisher = {{IEEE} Computer Society},
	author = {Andreas Langegger and Wolfram Woss},
	year = {2009},
	keywords = {rdf statistics, rdf stores, semantic Web, sparql},
	pages = {79--83}
},

@incollection{quilitz_querying_2008,
	title = {Querying Distributed {RDF} Data Sources with {SPARQL}},
	url = {http://dx.doi.org/10.1007/978-3-540-68234-9_39},
	abstract = {Integrated access to multiple distributed and autonomous {RDF} data sources is a key challenge for many semantic web applications.
As a reaction to this challenge, {SPARQL,} the {W3C} Recommendation for an {RDF} query language, supports querying of multiple {RDF}
graphs. However, the current standard does not provide transparent query federation, which makes query formulation hard and
lengthy. Furthermore, current implementations of {SPARQL} load all {RDF} graphs mentioned in a query to the local machine. This
usually incurs a large overhead in network traffic, and sometimes is simply impossible for technical or legal reasons. To
overcome these problems we present {DARQ,} an engine for federated {SPARQL} queries. {DARQ} provides transparent query access to
multiple {SPARQL} services, i.e., it gives the user the impression to query one single {RDF} graph despite the real data being
distributed on the web. A service description language enables the query engine to decompose a query into sub-queries, each
of which can be answered by an individual service. {DARQ} also uses query rewriting and cost-based query optimization to speed-up
query execution. Experiments show that these optimizations significantly improve query performance even when only a very limited
amount of statistical information is available. {DARQ} is available under {GPL} License at
http://darq.sf.net/
.},
	booktitle = {The Semantic Web: Research and Applications},
	author = {Bastian Quilitz and Ulf Leser},
	year = {2008},
	pages = {524--538}
},

@incollection{gounaris_adaptive_2002,
	title = {Adaptive Query Processing: A Survey},
	shorttitle = {Adaptive Query Processing},
	url = {http://dx.doi.org/10.1007/3-540-45495-0_2},
	abstract = {In wide-area database systems, which may be running on unpredictable and volatile environments (such as computational grids),
it is difficult to produce efficient database query plans based on information available solely at compile time. A solution
to this problem is to exploit information that becomes available at query runtime and adapt the query plan to changing conditions
during execution. This paper presents a survey on adaptive query processing techniques, examining the opportunities they offer
to modify a plan dynamically and classifying them into categories according to the problem they focus on, their objectives,
the nature of feedback they collect from the environment, the frequency at which they can adapt, their implementation environment
and which component is responsible for taking the adaptation decisions.},
	booktitle = {Advances in Databases},
	author = {Anastasios Gounaris and Norman Paton and Alvaro Fernandes and Rizos Sakellariou},
	year = {2002},
	pages = {882--940}
},

@inproceedings{haas_ripple_1999,
	address = {Philadelphia, Pennsylvania, United States},
	title = {Ripple joins for online aggregation},
	url = {http://portal.acm.org/citation.cfm?doid=304182.304208},
	doi = {10.1145/304182.304208},
	booktitle = {Proceedings of the 1999 {ACM} {SIGMOD} international conference on Management of data  - {SIGMOD} '99},
	author = {Peter J. Haas and Joseph M. Hellerstein},
	year = {1999},
	pages = {287--298}
},

@inproceedings{selinger_access_1979,
	address = {Boston, Massachusetts},
	title = {Access path selection in a relational database management system},
	isbn = {{0-89791-001-X}},
	url = {http://portal.acm.org/citation.cfm?id=582099},
	doi = {10.1145/582095.582099},
	abstract = {In a high level query and data manipulation language such as {SQL,} requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the {IBM} San Jose Research Laboratory.},
	booktitle = {Proceedings of the 1979 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {P. Griffiths Selinger and M. M. Astrahan and D. D. Chamberlin and R. A. Lorie and T. G. Price},
	year = {1979},
	pages = {23--34}
},

@misc{abdel_kader_run-time_2010,
	type = {Conference or Workshop Paper},
	title = {Run-time Optimization for Pipelined Systems},
	url = {http://eprints.eemcs.utwente.nl/17779/},
	author = {R. Abdel Kader and M. van Keulen and P. A Boncz and S. Manegold},
	month = may,
	year = {2010},
	note = {Traditional optimizers fail to pick good execution plans, when faced with increasingly complex queries and large data sets. This failure is even more acute in the context of {XQuery,} due to the structured nature of the {XML} language. To overcome the vulnerabilities of traditional optimizers, we have previously proposed {ROX,} a Run-time Optimizer for {XQueries,} which interleaves optimization and execution of full tables. {ROX} has proved to be robust, even in the presence of strong correlations, but it has one limitation: it uses full materialization of intermediate results making it unsuitable for pipelined systems. Therefore, this paper proposes {ROX-sampled,} a variant of {ROX,} which executes small data samples, thus generating smaller intermediates. We conduct extensive experiments which proved that {ROX-sampled} is comparable to {ROX} in performance, and that it is still robust against correlations. The main benefit of {ROX-sampled} is that it allows the large number of pipelined databases to import the {ROX} idea into their optimization paradigm.},
	howpublished = {http://eprints.eemcs.utwente.nl/17779/}
},

@inproceedings{zhang_efficient_2009,
	address = {Hong Kong, China},
	title = {An efficient multi-dimensional index for cloud data management},
	isbn = {978-1-60558-802-5},
	url = {http://portal.acm.org/citation.cfm?id=1651263.1651267&coll=GUIDE&dl=GUIDE&type=series&idx=SERIES772&part=series&WantType=Proceedings&title=CIKM&CFID=82166658&CFTOKEN=88298104},
	doi = {10.1145/1651263.1651267},
	abstract = {Recently, the cloud computing platform is getting more and more attentions as a new trend of data management. Currently there are several cloud computing products that can provide various services. However, currently the cloud platforms only support simple keyword-based queries and can't answer complex queries efficiently due to lack of efficient index techniques. In this paper we propose an efficient approach to build multi-dimensional index for Cloud computing system. We use the combination of R-tree and {KD-tree} to organize data records and offer fast query processing and efficient index maintenance. Our approach can process typical multi-dimensional queries including point queries and range queries efficiently. Besides, frequent change of data on big amount of machines makes the index maintenance a challenging problem, and to cope with this problem we proposed a cost estimation-based index update strategy that can effectively update the index structure. Our experiments show that our indexing techniques improve query efficiency by an order of magnitude compared with alternative approaches, and scale well with the size of the data. Our approach is quite general and independent from the underlying infrastructure and can be easily carried over for implementation on various Cloud computing platforms.},
	booktitle = {Proceeding of the first international workshop on Cloud data management},
	publisher = {{ACM}},
	author = {Xiangyu Zhang and Jing Ai and Zhongyuan Wang and Jiaheng Lu and Xiaofeng Meng},
	year = {2009},
	keywords = {distributed index, multi-dimensional index, query processing},
	pages = {17--24}
},

@incollection{harth_yars2:_2008,
	title = {{YARS2:} A Federated Repository for Querying Graph Structured Data from the Web},
	shorttitle = {{YARS2}},
	url = {http://dx.doi.org/10.1007/978-3-540-76298-0_16},
	abstract = {We present the architecture of an end-to-end semantic search engine that uses a graph data model to enable interactive query
answering over structured and interlinked data collected from many disparate sources on the Web. In particular, we study distributed
indexing methods for graph-structured data and parallel query evaluation methods on a cluster of computers. We evaluate the
system on a dataset with 430 million statements collected from the Web, and provide scale-up experiments on 7 billion synthetically
generated statements.},
	booktitle = {The Semantic Web},
	author = {Andreas Harth and Jürgen Umbrich and Aidan Hogan and Stefan Decker},
	year = {2008},
	pages = {211--224}
},

@inproceedings{goldberg_computing_2005,
	address = {Vancouver, British Columbia},
	title = {Computing the shortest path: A*-search meets graph theory},
	isbn = {0-89871-585-7},
	shorttitle = {Computing the shortest path},
	url = {http://portal.acm.org/citation.cfm?doid=1070432.1070455},
	abstract = {We propose shortest path algorithms that use A* search in combination with a new graph-theoretic lower-bounding technique based on landmarks and the triangle inequality. Our algorithms compute optimal shortest paths and work on any directed graph. We give experimental results showing that the most efficient of our new algorithms outperforms previous algorithms, in particular A* search with Euclidean bounds, by a wide margin on road networks and on some synthetic problem families.},
	booktitle = {Proceedings of the sixteenth annual {ACM-SIAM} symposium on Discrete algorithms},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Andrew V. Goldberg and Chris Harrelson},
	year = {2005},
	pages = {156--165}
},

@inproceedings{chu_combining_2009,
	address = {Providence, Rhode Island, {USA}},
	title = {Combining keyword search and forms for ad hoc querying of databases},
	isbn = {978-1-60558-551-2},
	url = {http://portal.acm.org/citation.cfm?id=1559845.1559883&coll=GUIDE&dl=GUIDE&type=series&idx=SERIES473&part=series&WantType=Proceedings&title=SIGMOD&CFID=82166658&CFTOKEN=88298104},
	doi = {10.1145/1559845.1559883},
	abstract = {A common criticism of database systems is that they are hard to query for users uncomfortable with a formal query language. To address this problem, form-based interfaces and keyword search have been proposed; while both have benefits, both also have limitations. In this paper, we investigate combining the two with the hopes of creating an approach that provides the best of both. Specifically, we propose to take as input a target database and then generate and index a set of query forms offline. At query time, a user with a question to be answered issues standard keyword search queries; but instead of returning tuples, the system returns forms relevant to the question. The user may then build a structured query with one of these forms and submit it back to the system for evaluation. In this paper, we address challenges that arise in form generation, keyword search over forms, and ranking and displaying these forms. We explore techniques to tackle these challenges, and present experimental results suggesting that the approach of combining keyword search and form-based interfaces is promising.},
	booktitle = {Proceedings of the 35th {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Eric Chu and Akanksha Baid and Xiaoyong Chai and {AnHai} Doan and Jeffrey Naughton},
	year = {2009},
	keywords = {keyword search, query forms, relational databases},
	pages = {349--360}
},

@inproceedings{heine_scalable_2006,
	address = {Hong Kong},
	title = {Scalable p2p based {RDF} querying},
	isbn = {1-59593-428-6},
	url = {http://portal.acm.org/citation.cfm?id=1146847.1146864&coll=GUIDE&dl=GUIDE&CFID=48497073&CFTOKEN=98816495},
	doi = {10.1145/1146847.1146864},
	abstract = {In large-scale distributed systems, information is typically generated decentralized. However, for many applications it is desirable to have a unified view on this knowledge, allowing to query it without regarding the heterogeneity of the underlying systems. In this context, two main requirements have to be fulfilled. On the one hand, we need a flexible knowledge representation, and on the other hand the underlying infrastructure and query evaluation algorithm has to be highly {scalable.The} combination of p2p networks as basic infrastructure with {RDF} as a knowledge representation is a promising approach to this problem. Within this paper, we focus on the evaluation of {RDF} queries with respect to {RDF} data stored in a {DHT-based} p2p network. We propose a query algorithm and research different optimizations based on a look-ahead technique and Bloom filters which aim at maximizing the throughput and scalability of the entire system.},
	booktitle = {Proceedings of the 1st international conference on Scalable information systems},
	publisher = {{ACM}},
	author = {Felix Heine},
	year = {2006},
	pages = {17}
},

@inproceedings{vu_graph_2008,
	address = {Vancouver, Canada},
	title = {A graph method for keyword-based selection of the {top-K} databases},
	isbn = {978-1-60558-102-6},
	url = {http://portal.acm.org/citation.cfm?id=1376616.1376707&coll=GUIDE&dl=GUIDE&CFID=35047418&CFTOKEN=16174083},
	doi = {10.1145/1376616.1376707},
	abstract = {While database management systems offer a comprehensive solution to data storage, they require deep knowledge of the schema, as well as the data manipulation language, in order to perform effective retrieval. Since these requirements pose a problem to lay or occasional users, several methods incorporate keyword search {(KS)} into relational databases. However, most of the existing techniques focus on querying a single {DBMS.} On the other hand, the proliferation of distributed databases in several conventional and emerging applications necessitates the support for keyword-based data sharing and querying over multiple {DMBSs.} In order to avoid the high cost of searching in numerous, potentially irrelevant, databases in such systems, we propose {G-KS,} a novel method for selecting the {top-K} candidates based on their potential to contain results for a given query. {G-KSsummarizes} each database by a keyword relationship graph, where nodes represent terms and edges describe relationships between them. Keyword relationship graphs are utilized for computing the similarity between each database and a {KS} query, so that, during query processing, only the most promising databases are searched. An extensive experimental evaluation demonstrates that {G-KS} outperforms the current state-of-the-art technique on all aspects, including precision, recall, efficiency, space overhead and flexibility of accommodating different semantics.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Quang Hieu Vu and Beng Chin Ooi and Dimitris Papadias and Anthony K. H. Tung},
	year = {2008},
	keywords = {database summary, distributed databases, graph, information retrieval, keyword search, relational databases},
	pages = {915--926}
},

@inproceedings{he_blinks:_2007,
	address = {Beijing, China},
	title = {{BLINKS:} ranked keyword searches on graphs},
	isbn = {978-1-59593-686-8},
	shorttitle = {{BLINKS}},
	url = {http://portal.acm.org/citation.cfm?doid=1247480.1247516},
	doi = {10.1145/1247480.1247516},
	abstract = {Query processing over graph-structured data is enjoying a growing number of applications. A top-k keyword search query on a graph finds the top k answers according to some ranking criteria, where each answer is a substructure of the graph containing all query keywords. Current techniques for supporting such queries on general graphs suffer from several drawbacks, e.g., poor worst-case performance, not taking full advantage of indexes, and high memory requirements. To address these problems, we propose {BLINKS,} a bi-level indexing and query processing scheme for top-k keyword search on graphs. {BLINKS} follows a search strategy with provable performance bounds, while additionally exploiting a bi-level index for pruning and accelerating the search. To reduce the index space, {BLINKS} partitions a data graph into blocks: The bi-level index stores summary information at the block level to initiate and guide search among blocks, and more detailed information for each block to accelerate search within blocks. Our experiments show that {BLINKS} offers orders-of-magnitude performance improvement over existing approaches.},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Hao He and Haixun Wang and Jun Yang and Philip S. Yu},
	year = {2007},
	keywords = {graphs, indexing, keyword search, ranking},
	pages = {305--316}
},

@inproceedings{kim_skip-and-prune:_2009,
	title = {Skip-and-prune: cosine-based top-k query processing for
               efficient context-sensitive document retrieval},
	shorttitle = {Skip-and-prune},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/sigmod/KimC09},
	author = {Jong Wook Kim and K. Selcuk C and an},
	year = {2009},
	pages = {115--126}
},

@incollection{battre_top_2006,
	title = {Top k {RDF} Query Evaluation in Structured {P2P} Networks},
	url = {http://dx.doi.org/10.1007/11823285_105},
	abstract = {{Berners-Lee’s} vision of the Semantic Web describes the idea of providing machine readable and processable information using
key technologies such as ontologies and automated reasoning in order to create intelligent agents.

The prospective amount of machine readable information available in the future will be large. Thus, heterogeneity and scalability
will be central issues, rendering exhaustive searches and central storage of data infeasible. This paper presents a scalable
peer-to-peer based approach to distributed querying of Semantic Web information that allows ordering of entries in result
sets and limiting the size of result sets which is necessary to prevent results with millions of matches. The system relies
on the graph-based {W3C} standard Resource Description Framework {(RDF)} for knowledge description. Thereby, it enables queries on large, distributed {RDF} graphs.1},
	booktitle = {{Euro-Par} 2006 Parallel Processing},
	author = {Dominic Battré and Felix Heine and Odej Kao},
	year = {2006},
	pages = {995--1004}
},

@article{abadi_aurora:_2003,
	title = {Aurora: a new model and architecture for data stream management},
	volume = {12},
	issn = {1066-8888},
	shorttitle = {Aurora},
	url = {http://www.springerlink.com/content/pjce12guqe5q6yme/},
	doi = {10.1007/s00778-003-0095-z},
	number = {2},
	journal = {The {VLDB} Journal The International Journal on Very Large Data Bases},
	author = {Daniel J. Abadi and Don Carney and Ugur �etintemel and Mitch Cherniack and Christian Convey and Sangdon Lee and Michael Stonebraker and Nesime Tatbul and Stan Zdonik},
	year = {2003},
	pages = {120--139}
},

@article{bramandia_optimizing_2009,
	title = {Optimizing updates of recursive {XML} views of relations},
	volume = {18},
	url = {http://dx.doi.org/10.1007/s00778-009-0141-6},
	doi = {10.1007/s00778-009-0141-6},
	abstract = {Abstract  {XML} publishing has been an emerging technique for transforming (portions of) a relational database into an {XML} document, for
example, to facilitate interoperability between heterogeneous applications. Such applications may update the {XML} document
and the source relational database must be updated accordingly. In this paper, we consider such {XML} documents as (possibly)
recursively defined {XML} views of relations. We propose new optimization techniques to efficiently support {XML} view updates
specified via an {XPATH} expression with recursion and complex filters. The main novelties of our techniques are: (1) we propose
a space-efficient relational encoding of recursive {XML} views; and (2) we push the bulk of update processing inside a relational
database. Specifically, a compressed representation of the {XML} views is stored as extended shared-inlining relations. A space-efficient
and updatable 2-hop index is used to optimize {XPATH} evaluation on {XML} views. Updates of the {XML} views are evaluated on these relations and index.
View update translation is handled by a heuristic procedure inside a relational database, as opposed to previous middleware
approaches. We present an experimental study to demonstrate the effectiveness of our proposed techniques.},
	number = {6},
	journal = {The {VLDB} Journal},
	author = {Ramadhana Bramandia and Jiefeng Cheng and Byron Choi and Jeffrey Yu},
	month = dec,
	year = {2009},
	pages = {1313--1333}
},

@inproceedings{schenkel_efficient_2005,
	address = {Los Alamitos, {CA,} {USA}},
	title = {Efficient Creation and Incremental Maintenance of the {HOPI} Index for Complex {XML} Document Collections},
	volume = {0},
	doi = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2005.57},
	abstract = {The {HOPI} index, a connection index for {XML} documents based on the concept of a 2-hop cover, provides space- and time-efficient reachability tests along the ancestor, descendant, and link axes to support path expressions with wildcards in {XML} search engines. This paper presents enhanced algorithms for building {HOPI,} shows how to augment the index with distance information, and discusses incremental index maintenance. Our experiments show substantial improvements over the existing divide-and-conquer algorithm for index creation, low space overhead for including distance information in the index, and efficient updates.},
	booktitle = {Data Engineering, International Conference on},
	publisher = {{IEEE} Computer Society},
	author = {Ralf Schenkel and Anja Theobald and Gerhard Weikum},
	year = {2005},
	pages = {360--371}
},

@inproceedings{viglas_rate-based_2002,
	address = {Madison, Wisconsin},
	title = {Rate-based query optimization for streaming information sources},
	url = {http://portal.acm.org/citation.cfm?id=564697&dl=},
	doi = {10.1145/564691.564697},
	booktitle = {Proceedings of the 2002 {ACM} {SIGMOD} international conference on Management of data  - {SIGMOD} '02},
	author = {Stratis D. Viglas and Jeffrey F. Naughton},
	year = {2002},
	pages = {37}
},

@inproceedings{jin_computing_2010,
	address = {New York, {NY,} {USA}},
	series = {{SIGMOD} '10},
	title = {Computing label-constraint reachability in graph databases},
	isbn = {978-1-4503-0032-2},
	location = {Indianapolis, Indiana, {USA}},
	doi = {10.1145/1807167.1807183},
	abstract = {Our world today is generating huge amounts of graph data such as social networks, biological networks, and the semantic web. Many of these real-world graphs are edge-labeled graphs, i.e., each edge has a label that denotes the relationship between the two vertices connected by the edge. A fundamental research problem on these labeled graphs is how to handle the label-constraint reachability query: Can vertex u reach vertex v through a path whose edge labels are constrained by a set of labels? In this work, we introduce a novel tree-based index framework which utilizes the directed maximal weighted spanning tree algorithm and sampling techniques to maximally compress the generalized transitive closure for the labeled graphs. An extensive experimental evaluation on both real and synthetic datasets demonstrates the efficiency of our approach in answering label-constraint reachability queries.},
	booktitle = {Proceedings of the 2010 international conference on Management of data},
	publisher = {{ACM}},
	author = {Ruoming Jin and Hui Hong and Haixun Wang and Ning Ruan and Yang Xiang},
	year = {2010},
	note = {{ACM} {ID:} 1807183},
	keywords = {database applications, generalized transitive closure, hoeffding and bernstein bounds, label-constraint reachability, maximal directed spanning tree, performance, to-read},
	pages = {123–134}
},

@inproceedings{xin_towards_2006,
	title = {Towards Robust Indexing for Ranked Queries},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/vldb/XinCH06},
	author = {Dong Xin and Chen Chen and Jiawei Han},
	year = {2006},
	pages = {235--246}
},

@inproceedings{yu_effective_2007,
	address = {Beijing, China},
	title = {Effective keyword-based selection of relational databases},
	isbn = {978-1-59593-686-8},
	url = {http://portal.acm.org/citation.cfm?id=1247480.1247498&coll=GUIDE&dl=GUIDE&CFID=75531709&CFTOKEN=65038685},
	doi = {10.1145/1247480.1247498},
	abstract = {The wide popularity of free-and-easy keyword based searches over World Wide Web has fueled the demand for incorporating keyword-based search over structured databases. However, most of the current research work focuses on keyword-based searching over a single structured data source. With the growing interest in distributed databases and service oriented architecture over the Internet, it is important to extend such a capability over multiple structured data sources. One of the most important problems for enabling such a query facility is to be able to select the most useful data sources relevant to the keyword query. Traditional database summary techniques used for selecting unstructured datasources developed in {IR} literature are inadequate for our problem, as they do not capture the structure of the data sources. In this paper, we study the database selection problem for relational data sources, and propose a method that effectively summarizes the relationships between keywords in a relational database based on its structure. We develop effective ranking methods based on the keyword relationship summaries in order to select the most useful databases for a given keyword query. We have implemented our system on {PlanetLab.} In that environment we use extensive experiments with real datasets to demonstrate the effectiveness of our proposed summarization method.},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Bei Yu and Guoliang Li and Karen Sollins and Anthony K. H. Tung},
	year = {2007},
	keywords = {database selection, keyword query, summarization},
	pages = {139--150}
},

@inproceedings{hristidis_prefer:_2001,
	title = {{PREFER:} A System for the Efficient Execution of Multi-parametric
               Ranked Queries},
	shorttitle = {{PREFER}},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/sigmod/HristidisKP01},
	author = {Vagelis Hristidis and Nick Koudas and Yannis Papakonstantinou},
	year = {2001},
	pages = {259--270}
},

@inproceedings{levy_querying_1996,
	title = {Querying heterogeneous information sources using source descriptions},
	booktitle = {Proceedings of the International Conference on Very Large Data Bases},
	author = {A. Levy and A. Rajaraman and J. Ordille and others},
	year = {1996},
	pages = {251–262}
},

@misc{abdel_kader_rox:_2010,
	type = {Conference or Workshop Paper},
	title = {{ROX:} The Robustness of a Run-time {XQuery} Optimizer Against Correlated Data},
	shorttitle = {{ROX}},
	url = {http://eprints.eemcs.utwente.nl/16035/},
	author = {R. Abdel Kader and P. Boncz and S. Manegold and M. van Keulen},
	month = mar,
	year = {2010},
	note = {We demonstrate {ROX,} a run-time optimizer of {XQueries,} that focuses on finding the best execution order of {XPath} steps and relational joins in an {XQuery.} The problem of join ordering has been extensively researched, but the proposed techniques are still unsatisfying. These either rely on a cost model which might result in inaccurate estimations, or explore only a restrictive number of plans from the search space. {ROX} is developed to tackle these problems. {ROX} does not need any cost model, and defers query optimization to run-time intertwining optimization and execution steps. In every optimization step, sampling techniques are used to estimate the cardinality of unexecuted steps and joins to make a decision which sequence of operators to process next. Consequently, each execution step will provide updated and accurate knowledge about intermediate results, which will be used during the next optimization round. This demonstration will focus on: (i) illustrating the steps that {ROX} follows and the decisions it makes to choose a good join order, (ii) showing {ROX’s} robustness in the face of data with different degree of correlation, (iii) comparing the performance of the plan chosen by {ROX} to different plans picked from the search space, (iv) proving that the run-time overhead needed by {ROX} is restricted to a small fraction of the execution time.},
	howpublished = {http://eprints.eemcs.utwente.nl/16035/}
},

@article{abadi_sw-store:_2009,
	title = {{SW-Store:} a vertically partitioned {DBMS} for Semantic Web data management},
	volume = {18},
	shorttitle = {{SW-Store}},
	url = {http://dx.doi.org/10.1007/s00778-008-0125-y},
	doi = {10.1007/s00778-008-0125-y},
	abstract = {Abstract  Efficient management of {RDF} data is an important prerequisite for realizing the Semantic Web vision. Performance and scalability
issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper,
we examine the reasons why current data management solutions for {RDF} data scale poorly, and explore the fundamental scalability
limitations of these approaches. We review the state of the art for improving performance of {RDF} databases and consider a
recent suggestion, “property tables”. We then discuss practically and empirically why this solution has undesirable features.
As an improvement, we propose an alternative solution: vertically partitioning the {RDF} data. We compare the performance of
vertical partitioning with prior art on queries generated by a Web-based {RDF} browser over a large-scale (more than 50 million
triples) catalog of library data. Our results show that a vertically partitioned schema achieves similar performance to the
property table technique while being much simpler to design. Further, if a column-oriented {DBMS} (a database architected specially
for the vertically partitioned case) is used instead of a row-oriented {DBMS,} another order of magnitude performance improvement
is observed, with query times dropping from minutes to several seconds. Encouraged by these results, we describe the architecture
of {SW-Store,} a new {DBMS} we are actively building that implements these techniques to achieve high performance {RDF} data management.},
	number = {2},
	journal = {The {VLDB} Journal},
	author = {Daniel Abadi and Adam Marcus and Samuel Madden and Kate Hollenbach},
	month = apr,
	year = {2009},
	pages = {385--406}
},

@inproceedings{chaudhuri_overview_1998,
	address = {Seattle, Washington, United States},
	title = {An overview of query optimization in relational systems},
	isbn = {0-89791-996-3},
	url = {http://portal.acm.org/citation.cfm?id=275487.275492&type=series},
	doi = {10.1145/275487.275492},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {Proceedings of the seventeenth {ACM} {SIGACT-SIGMOD-SIGART} symposium on Principles of database systems},
	publisher = {{ACM}},
	author = {Surajit Chaudhuri},
	year = {1998},
	pages = {34--43}
},

@article{ilyas_survey_2008,
	title = {A survey of top-k query processing techniques in relational database systems},
	volume = {40},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=journals/csur/IlyasBS08},
	number = {4},
	journal = {{ACM} Comput. Surv.},
	author = {Ihab F. Ilyas and George Beskales and Mohamed A. Soliman},
	year = {2008}
},

@incollection{matono_rdfcube:_2007,
	title = {{RDFCube:} A {P2P-Based} {Three-Dimensional} Index for Structural Joins on Distributed Triple Stores},
	shorttitle = {{RDFCube}},
	url = {http://dx.doi.org/10.1007/978-3-540-71661-7_31},
	abstract = {Today, {RDF} data/triples are scattered everywhere and their total size is rapidly increasing. Centralized {RDF} triple stores
have limitations on both their failure tolerance and scalability. Therefore, {RDF} query processing in a {P2P} environment is
an important issue. So far, several conventional {P2P-based} {RDF} triple stores have been proposed. They, however, are designed
merely for triple retrieval rather than for triple join query processing. Consequently, they suffer from an unnecessary data
transfer problem. This paper presents an {RDF} query processing technique based on a three-dimensional hash index. The triples
are mapped into the index; then bit information that represents the presence or absence of triples in the index is introduced.
We implemented our approach on the top of an emulated {P2P} environment. Evaluation results show that our approach can achieve
good performance and scalability.},
	booktitle = {Databases, Information Systems, and {Peer-to-Peer} Computing},
	author = {Akiyoshi Matono and Said Pahlevi and Isao Kojima},
	year = {2007},
	pages = {323--330}
},

@incollection{hartig_executing_2009,
	title = {Executing {SPARQL} Queries over the Web of Linked Data},
	url = {http://dx.doi.org/10.1007/978-3-642-04930-9_19},
	abstract = {The Web of Linked Data forms a single, globally distributed dataspace. Due to the openness of this dataspace, it is not possible
to know in advance all data sources that might be relevant for query answering. This openness poses a new challenge that is
not addressed by traditional research on federated query processing. In this paper we present an approach to execute {SPARQL}
queries over the Web of Linked Data. The main idea of our approach is to discover data that might be relevant for answering
a query during the query execution itself. This discovery is driven by following {RDF} links between data sources based on {URIs}
in the query and in partial results. The {URIs} are resolved over the {HTTP} protocol into {RDF} data which is continuously added
to the queried dataset. This paper describes concepts and algorithms to implement our approach using an iterator-based pipeline.
We introduce a formalization of the pipelining approach and show that classical iterators may cause blocking due to the latency
of {HTTP} requests. To avoid blocking, we propose an extension of the iterator paradigm. The evaluation of our approach shows
its strengths as well as the still existing challenges.},
	booktitle = {The Semantic Web - {ISWC} 2009},
	author = {Olaf Hartig and Christian Bizer and {Johann-Christoph} Freytag},
	year = {2009},
	pages = {293--309}
},

@inproceedings{deshpande_decoupled_2002,
	title = {Decoupled query optimization for federated database systems},
	doi = {10.1109/ICDE.2002.994788},
	abstract = {We study the problem of query optimization in federated relational
database systems. The nature of federated databases explicitly decouples
many aspects of the optimization process, often making it imperative for
the optimizer to consult underlying data sources while doing cost-based
optimization. This not only increases the cost of optimization, but also
changes the trade-offs involved in the optimization process
significantly. The dominant cost in the decoupled optimization process
is the "cost of costing" that traditionally has been considered
insignificant. The optimizer can only afford a few rounds of messages to
the underlying data sources and hence the optimization techniques in
this environment must be geared toward gathering all the required cost
information with minimal communication. In this paper, we explore the
design space for a query optimizer in this environment and demonstrate
the need for decoupling various aspects of the optimization process. We
present minimum-communication decoupled variants of various query
optimization techniques, and discuss tradeoffs in their performance in
this scenario. We have implemented these techniques in the Cohera
federated database system and our experimental results, somewhat
surprisingly, indicate that a simple two-phase optimization scheme
performs fairly well as long as the physical database design is known to
the optimizer, though more aggressive algorithms are required otherwise},
	booktitle = {Data Engineering, 2002. Proceedings. 18th International Conference on},
	author = {A. Deshpande and {J.M.} Hellerstein},
	year = {2002},
	keywords = {Cohera federated database, decoupled optimization, distributed databases, federated databases, federated relational database systems, query optimization, query optimizer, query processing, relational databases},
	pages = {716--727}
},

@article{chazelle_bloomier_2004,
	title = {The Bloomier Filter: An Efficient Data Structure for Static Support Lookup Tables},
	shorttitle = {The Bloomier Filter},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.7502},
	doi = {10.1.1.5.7502},
	journal = {{IN} {PROCEEDINGS} {OF} {THE} {FIFTEENTH} {ANNUAL} {ACM-SIAM} {SYMPOSIUM} {ON} {DISCRETE} {ALGORITHMS} {(SODA}},
	author = {Bernard Chazelle and Joe Kilian and Ronitt Rubinfeld and Ayellet Tal and Oh Boy},
	year = {2004},
	keywords = {bloom, data structure},
	pages = {30---39}
},

@inproceedings{yiu_efficient_2007,
	title = {Efficient Processing of Top-k Dominating Queries on {Multi-Dimensional}
               Data},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/vldb/YiuM07},
	author = {Man Lung Yiu and Nikos Mamoulis},
	year = {2007},
	keywords = {top-k},
	pages = {483--494}
},

@inproceedings{vu_graph_2008-1,
	address = {Vancouver, Canada},
	title = {A graph method for keyword-based selection of the {top-K} databases},
	isbn = {978-1-60558-102-6},
	url = {http://portal.acm.org/citation.cfm?id=1376616.1376707&coll=GUIDE&dl=GUIDE&CFID=75531709&CFTOKEN=65038685},
	doi = {10.1145/1376616.1376707},
	abstract = {While database management systems offer a comprehensive solution to data storage, they require deep knowledge of the schema, as well as the data manipulation language, in order to perform effective retrieval. Since these requirements pose a problem to lay or occasional users, several methods incorporate keyword search {(KS)} into relational databases. However, most of the existing techniques focus on querying a single {DBMS.} On the other hand, the proliferation of distributed databases in several conventional and emerging applications necessitates the support for keyword-based data sharing and querying over multiple {DMBSs.} In order to avoid the high cost of searching in numerous, potentially irrelevant, databases in such systems, we propose {G-KS,} a novel method for selecting the {top-K} candidates based on their potential to contain results for a given query. {G-KSsummarizes} each database by a keyword relationship graph, where nodes represent terms and edges describe relationships between them. Keyword relationship graphs are utilized for computing the similarity between each database and a {KS} query, so that, during query processing, only the most promising databases are searched. An extensive experimental evaluation demonstrates that {G-KS} outperforms the current state-of-the-art technique on all aspects, including precision, recall, efficiency, space overhead and flexibility of accommodating different semantics.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Quang Hieu Vu and Beng Chin Ooi and Dimitris Papadias and Anthony K. H. Tung},
	year = {2008},
	keywords = {database summary, distributed databases, graph, information retrieval, keyword search, relational databases},
	pages = {915--926}
},

@inproceedings{krishnamurthy_optimization_1986,
	title = {Optimization of Nonrecursive Queries},
	isbn = {0-934613-18-4},
	url = {http://portal.acm.org/citation.cfm?id=645913.671481},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {Proceedings of the 12th International Conference on Very Large Data Bases},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Ravi Krishnamurthy and Haran Boral and Carlo Zaniolo},
	year = {1986},
	pages = {128--137}
},

@incollection{pomares_dynamic_2008,
	series = {Lecture Notes in Computer Science},
	title = {Dynamic Source Selection in Large Scale Mediation Systems},
	volume = {5187},
	url = {http://dx.doi.org/10.1007/978-3-540-85176-9_6},
	booktitle = {Data Management in Grid and {Peer-to-Peer} Systems},
	publisher = {Springer Berlin / Heidelberg},
	author = {Alexandra Pomares and Claudia Roncancio and José Abásolo and Pilar Villamil},
	editor = {Abdelkader Hameurlain},
	year = {2008},
	note = {10.1007/978-3-540-85176-9\_6},
	pages = {58--69}
},

@inproceedings{nie_joint_2001,
	address = {New York, {NY,} {USA}},
	series = {{CIKM} '01},
	title = {Joint optimization of cost and coverage of query plans in data integration},
	isbn = {1-58113-436-3},
	location = {Atlanta, Georgia, {USA}},
	doi = {10.1145/502585.502623},
	abstract = {Existing approaches for optimizing queries in data integration use decoupled strategies--attempting to optimize coverage and cost in two separate phases. Since sources tend to have a variety of access limitations, such phased optimization of cost and coverage can unfortunately lead to expensive planning as well as highly inefficient plans. In this paper we present techniques for joint optimization of cost and coverage of the query plans. Our algorithms search in the space of parallel query plans that support multiple sources for each subgoal conjunct. The refinement of the partial plans takes into account the potential parallelism between source calls, and the binding compatibilities between the sources included in the plan. We start by introducing and motivating our query plan representation. We then briefly review how to compute the cost and coverage of a parallel plan. Next, we provide both a {System-R} style query optimization algorithm as well as a greedy local search algorithm for searching in the space of such query plans. Finally we present a simulation study that demonstrates that the plans generated by our approach will be significantly better, both in terms of planning cost, and in terms of plan execution cost, compared to the existing approaches.},
	booktitle = {Proceedings of the tenth international conference on Information and knowledge management},
	publisher = {{ACM}},
	author = {Zaiqing Nie and Subbarao Kambhampati},
	year = {2001},
	note = {{ACM} {ID:} 502623},
	keywords = {algorithms, design, linear programming, performance, query formulation, query processing},
	pages = {223–230}
},

@article{avnur_eddies:_2000,
	title = {Eddies: continuously adaptive query processing},
	volume = {29},
	shorttitle = {Eddies},
	url = {http://portal.acm.org/citation.cfm?id=335420},
	doi = {10.1145/335191.335420},
	abstract = {In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.},
	number = {2},
	journal = {{SIGMOD} Rec.},
	author = {Ron Avnur and Joseph M. Hellerstein},
	year = {2000},
	pages = {261--272}
},

@incollection{cheng_fast_2006,
	title = {Fast Computation of Reachability Labeling for Large Graphs},
	url = {http://dx.doi.org/10.1007/11687238_56},
	abstract = {The need of processing graph reachability queries stems from many applications that manage complex data as graphs. The applications
include transportation network, Internet traffic analyzing, Web navigation, semantic web, chemical informatics and bio-informatics
systems, and computer vision. A graph reachability query, as one of the primary tasks, is to find whether two given data objects,
u and v, are related in any ways in a large and complex dataset. Formally, the query is about to find if v is reachable from u in a directed graph which is large in size. In this paper, we focus ourselves on building a reachability labeling for a large
directed graph, in order to process reachability queries efficiently. Such a labeling needs to be minimized in size for the
efficiency of answering the queries, and needs to be computed fast for the efficiency of constructing such a labeling. As
such a labeling, 2-hop cover was proposed for arbitrary graphs with theoretical bounds on both the construction cost and the
size of the resulting labeling. However, in practice, as reported, the construction cost of 2-hop cover is very high even
with super power machines. In this paper, we propose a novel geometry-based algorithm which computes high-quality 2-hop cover
fast. Our experimental results verify the effectiveness of our techniques over large real and synthetic graph datasets.},
	booktitle = {Advances in Database Technology - {EDBT} 2006},
	author = {Jiefeng Cheng and Jeffrey Yu and Xuemin Lin and Haixun Wang and Philip Yu},
	year = {2006},
	pages = {961--979}
},

@article{lee_structural_2010,
	title = {Structural consistency: enabling {XML} keyword search to eliminate spurious results consistently},
	volume = {19},
	issn = {1066-8888},
	shorttitle = {Structural consistency},
	url = {http://www.springerlink.com/content/83338762l176955t/},
	doi = {10.1007/s00778-009-0177-7},
	number = {4},
	journal = {The {VLDB} Journal},
	author = {{Ki-Hoon} Lee and {Kyu-Young} Whang and {Wook-Shin} Han and {Min-Soo} Kim},
	year = {2010},
	pages = {503--529}
},

@article{zhang_streaming_2010,
	title = {Streaming multiple aggregations using phantoms},
	volume = {19},
	issn = {1066-8888},
	url = {http://www.springerlink.com/content/h7156145l351kp05/},
	doi = {10.1007/s00778-010-0180-z},
	number = {4},
	journal = {The {VLDB} Journal},
	author = {Rui Zhang and Nick Koudas and Beng Chin Ooi and Divesh Srivastava and Pu Zhou},
	year = {2010},
	pages = {557--583}
},

@inproceedings{cheng_fast_2008,
	address = {Nantes, France},
	title = {Fast computing reachability labelings for large graphs with high compression rate},
	isbn = {978-1-59593-926-5},
	url = {http://portal.acm.org/citation.cfm?doid=1353343.1353370},
	doi = {10.1145/1353343.1353370},
	abstract = {There are numerous applications that need to deal with a large graph and need to query reachability between nodes in the graph. A 2-hop cover can compactly represent the whole edge transitive closure of a graph in {O({\textbar}V{\textbar}} . {{\textbar}E{\textbar}1/2)} space, and be used to answer reachability query efficiently. However, it is challenging to compute a 2-hop cover. The existing approaches suffer from either large resource consumption or low compression rate. In this paper, we propose a hierarchical partitioning approach to partition a large graph G into two subgraphs repeatedly in a top-down fashion. The unique feature of our approach is that we compute 2-hop cover while partitioning. In brief, in every iteration of top-down partitioning, we provide techniques to compute the 2-hop cover for connections between the two subgraphs first. A cover is computed to cut the graph into two subgraphs, which results in an overall cover with high compression for the entire graph G. Two approaches are proposed, namely a node-oriented approach and an edge-oriented approach. Our approach can efficiently compute 2-hop cover for a large graph with high compression rate. Our extensive experiment studies show that the 2-hop cover for a graph with 1,700,000 nodes and 169 billion connections can be obtained in less than 30 minutes with a compression rate about 40,000 using a {PC.}},
	booktitle = {Proceedings of the 11th international conference on Extending database technology: Advances in database technology},
	publisher = {{ACM}},
	author = {Jiefeng Cheng and Jeffrey Xu Yu and Xuemin Lin and Haixun Wang and Philip S. Yu},
	year = {2008},
	pages = {193--204}
},

@inproceedings{stuckenschmidt_index_2004,
	address = {New York, {NY,} {USA}},
	title = {Index structures and algorithms for querying distributed {RDF} repositories},
	isbn = {{1-58113-844-X}},
	url = {http://portal.acm.org/citation.cfm?id=988672.988758&coll=GUIDE&dl=GUIDE&CFID=82166857&CFTOKEN=58994927},
	doi = {10.1145/988672.988758},
	abstract = {A technical infrastructure for storing, querying and managing {RDFdata} is a key element in the current semantic web development. Systems like Jena, Sesame or the {ICS-FORTH} {RDF} Suite are widelyused for building semantic web applications. Currently, none ofthese systems supports the integrated querying of distributed {RDF} repositories. We consider this a major shortcoming since the semanticweb is distributed by nature. In this paper we present an architecture for querying distributed {RDF} repositories by extending the existing Sesame system. We discuss the implications of our architectureand propose an index structure as well as algorithms forquery processing and optimization in such a distributed context.},
	booktitle = {Proceedings of the 13th international conference on World Wide Web},
	publisher = {{ACM}},
	author = {Heiner Stuckenschmidt and Richard Vdovjak and {Geert-Jan} Houben and Jeen Broekstra},
	year = {2004},
	keywords = {index structures, optimization, {RDF} querying},
	pages = {631--639}
},

@article{soliman_supporting_2010,
	title = {Supporting ranking queries on uncertain and incomplete data},
	volume = {19},
	issn = {1066-8888},
	url = {http://www.springerlink.com/content/b352881528v12624/},
	doi = {10.1007/s00778-009-0176-8},
	number = {4},
	journal = {The {VLDB} Journal},
	author = {Mohamed A. Soliman and Ihab F. Ilyas and Shalev {Ben-David}},
	year = {2010},
	pages = {477--501}
},

@inproceedings{arai_anytime_2007,
	title = {Anytime Measures for Top-k Algorithms},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/vldb/AraiDGK07},
	author = {Benjamin Arai and Gautam Das and Dimitrios Gunopulos and Nick Koudas},
	year = {2007},
	keywords = {top-k},
	pages = {914--925}
},

@inproceedings{leskovec_graphs_2005,
	address = {Chicago, Illinois, {USA}},
	title = {Graphs over time: densification laws, shrinking diameters and possible explanations},
	isbn = {{1-59593-135-X}},
	shorttitle = {Graphs over time},
	url = {http://portal.acm.org/citation.cfm?id=1081893},
	doi = {10.1145/1081870.1081893},
	abstract = {How do real graphs evolve over time? What are "normal" growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over {time.Here} we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time, with the number of edges growing super-linearly in the number of nodes. Second, the average distance between nodes often shrinks over time, in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log {n)).Existing} graph generation models do not exhibit these types of behavior, even at a qualitative level. We provide a new graph generator, based on a "forest fire" spreading process, that has a simple, intuitive justification, requires very few parameters (like the "flammability" of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study.},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on Knowledge discovery in data mining},
	publisher = {{ACM}},
	author = {Jure Leskovec and Jon Kleinberg and Christos Faloutsos},
	year = {2005},
	keywords = {densification power laws, graph generators, graph mining, heavy-tailed distributions, small-world phenomena},
	pages = {177--187}
},

@inproceedings{finger_robust_2009,
	address = {Providence, Rhode Island, {USA}},
	title = {Robust and efficient algorithms for rank join evaluation},
	isbn = {978-1-60558-551-2},
	url = {http://portal.acm.org/citation.cfm?id=1559845.1559890&coll=GUIDE&dl=GUIDE&type=series&idx=SERIES473&part=series&WantType=Proceedings&title=SIGMOD&CFID=82166658&CFTOKEN=88298104},
	doi = {10.1145/1559845.1559890},
	abstract = {In the rank join problem we are given a relational join R1 x R2 and a function that assigns numeric scores to the join tuples, and the goal is to return the tuples with the highest score. This problem lies at the core of processing top-k {SQL} queries, and recent studies have introduced specialized operators that solve the rank join problem by accessing only a subset of the input tuples. A desirable property for such operators is instance-optimality, i.e., their {I/O} cost should remain within a factor of the optimal for different inputs. However, a recent theoretical study has shown that existing rank join operators are not instance-optimal even though they have been shown to perform well empirically. The same study proposed the {PBRJRRoverFR} operator that was proved to be instance-optimal, but its performance was not tested empirically and in fact it was hinted that its complexity can be high. Thus, the following important question is raised: Is it possible to design a rank join operator that is both instance-optimal and computationally efficient?},
	booktitle = {Proceedings of the 35th {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Jonathan Finger and Neoklis Polyzotis},
	year = {2009},
	keywords = {adaptive pulling, feasible region bound, rank join, ranking queries},
	pages = {415--428}
},

@inproceedings{lawrence_early_2005,
	address = {Trondheim, Norway},
	title = {Early hash join: a configurable algorithm for the efficient and early production of join results},
	isbn = {1-59593-154-6},
	shorttitle = {Early hash join},
	url = {http://portal.acm.org/citation.cfm?id=1083592.1083689},
	abstract = {Minimizing both the response time to produce the first few thousand results and the overall execution time is important for interactive querying. Current join algorithms either minimize the execution time at the expense of response time or minimize response time by producing results early without optimizing the total time. We present a hash-based join algorithm, called early hash join, which can be dynamically configured at any point during join processing to tradeoff faster production of results for overall execution time. We demonstrate that varying how inputs are read has a major effect on these two factors and provide formulas that allow an optimizer to calculate the expected rate of join output and the number of {I/O} operations performed using different input reading strategies. Experimental results show that early hash join performs significantly fewer {I/O} operations and executes faster than other early join algorithms, especially for one-to-many joins. Its overall execution time is comparable to standard hybrid hash join, but its response time is an order of magnitude faster. Thus, early hash join can replace hybrid hash join in any situation where a fast initial response time is beneficial without the penalty in overall execution time exhibited by other early join algorithms.},
	booktitle = {Proceedings of the 31st international conference on Very large data bases},
	publisher = {{VLDB} Endowment},
	author = {Ramon Lawrence},
	year = {2005},
	pages = {841--852}
},

@inproceedings{agrawal_efficient_1989,
	address = {Portland, Oregon, United States},
	title = {Efficient management of transitive relationships in large data and knowledge bases},
	isbn = {0-89791-317-5},
	url = {http://portal.acm.org/citation.cfm?doid=67544.66950},
	doi = {10.1145/67544.66950},
	abstract = {We argue that accessing the transitive closure of relationships is an important component of both databases and knowledge representation systems in Artificial Intelligence. The demands for efficient access and management of large relationships motivate the need for explicitly storing the transitive closure in a compressed and local way, while allowing updates to the base relation to be propagated incrementally. We present a transitive closure compression technique, based on labeling spanning trees with numeric intervals, and provide both analytical and empirical evidence of its efficacy, including a proof of optimality.},
	booktitle = {Proceedings of the 1989 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {R. Agrawal and A. Borgida and H. V. Jagadish},
	year = {1989},
	pages = {253--262}
},

@inproceedings{jin_3-hop:_2009,
	address = {Providence, Rhode Island, {USA}},
	title = {{3-HOP:} a high-compression indexing scheme for reachability query},
	isbn = {978-1-60558-551-2},
	shorttitle = {{3-HOP}},
	url = {http://portal.acm.org/citation.cfm?id=1559845.1559930},
	doi = {10.1145/1559845.1559930},
	abstract = {Reachability queries on large directed graphs have attracted much attention recently. The existing work either uses spanning structures, such as chains or trees, to compress the complete transitive closure, or utilizes the 2-hop strategy to describe the reachability. Almost all of these approaches work well for very sparse graphs. However, the challenging problem is that as the ratio of the number of edges to the number of vertices increases, the size of the compressed transitive closure grows very large. In this paper, we propose a new 3-hop indexing scheme for directed graphs with higher density. The basic idea of 3-hop indexing is to use chain structures in combination with hops to minimize the number of structures that must be indexed. Technically, our goal is to find a 3-hop scheme over dense {DAGs} (directed acyclic graphs) with minimum index size. We develop an efficient algorithm to discover a transitive closure contour, which yields near optimal index size. Empirical studies show that our 3-hop scheme has much smaller index size than state-of-the-art reachability query schemes such as 2-hop and path-tree when {DAGs} are not very sparse, while our query time is close to path-tree, which is considered to be one of the best reachability query schemes.},
	booktitle = {Proceedings of the 35th {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Ruoming Jin and Yang Xiang and Ning Ruan and David Fuhry},
	year = {2009},
	keywords = {2-hop, 3-hop, graph indexing, path-tree, reachability queries, transitive closure},
	pages = {813--826}
},

@article{rodriguez-martinez_mocha:_2000,
	title = {{MOCHA:} a self-extensible database middleware system for distributed data sources},
	volume = {29},
	shorttitle = {{MOCHA}},
	url = {http://portal.acm.org/citation.cfm?id=335191.335413},
	doi = {10.1145/335191.335413},
	abstract = {We present {MOCHA,} a new self-extensible database middleware system designed to interconnect distributed data sources. {MOCHA} is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. {MOCHA} has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of {MOCHA,} the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that {MOCHA} provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.},
	number = {2},
	journal = {{SIGMOD} Rec.},
	author = {Manuel {Rodríguez-Martínez} and Nick Roussopoulos},
	year = {2000},
	pages = {213--224}
},

@book{sayyadian_efficient_2006,
	title = {Efficient keyword search across heterogeneous relational databases},
	publisher = {University of Illinois at {Urbana-Champaign}},
	author = {M. Sayyadian and H. {LeKhac} and A. H. Doan and L. Gravano},
	year = {2006},
	keywords = {federated, keyword search}
},

@article{jeong_faster_1998,
	title = {A Faster Parallel Implementation of the {Kanellakis-Smolka} Algorithm for Bisimilarity Checking},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.25.9635},
	doi = {10.1.1.25.9635},
	journal = {{IN} {PROCEEDINGS} {OF} {THE} {INTERNATIONAL} {COMPUTER} {SYMPOSIUM}},
	author = {Cheoljoo Jeong and Youngchan Kim and Youngbae Oh and Heungnam Kim},
	year = {1998},
	keywords = {bisimulation, parallel}
},

@inproceedings{moerkotte_dynamic_2008,
	address = {New York, {NY,} {USA}},
	series = {{SIGMOD} '08},
	title = {Dynamic programming strikes back},
	isbn = {978-1-60558-102-6},
	location = {Vancouver, Canada},
	doi = {10.1145/1376616.1376672},
	abstract = {Two highly efficient algorithms are known for optimally ordering joins while avoiding cross products: {DPccp,} which is based on dynamic programming, and {Top-Down} Partition Search, based on memoization. Both have two severe limitations: They handle only (1) simple (binary) join predicates and (2) inner joins. However, real queries may contain complex join predicates, involving more than two relations, and outer joins as well as other non-inner joins. Taking the most efficient known join-ordering algorithm, {DPccp,} as a starting point, we first develop a new algorithm, {DPhyp,} which is capable to handle complex join predicates efficiently. We do so by modeling the query graph as a (variant of a) hypergraph and then reason about its connected subgraphs. Then, we present a technique to exploit this capability to efficiently handle the widest class of non-inner joins dealt with so far. Our experimental results show that this reformulation of non-inner joins as complex predicates can improve optimization time by orders of magnitude, compared to known algorithms dealing with complex join predicates and non-inner joins. Once again, this gives dynamic programming a distinct advantage over current memoization techniques.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Guido Moerkotte and Thomas Neumann},
	year = {2008},
	note = {{ACM} {ID:} 1376672},
	keywords = {algorithms, complex joins, hypergraphs, query optimization, query processing, theory},
	pages = {539–552}
},

@incollection{torres_supporting_2009,
	title = {Supporting Personal Semantic Annotations in {P2P} Semantic Wikis},
	url = {http://dx.doi.org/10.1007/978-3-642-03573-9_26},
	abstract = {In this paper, we propose to extend {Peer-to-Peer} Semantic Wikis with personal semantic annotations. Semantic Wikis are one
of the most successful Semantic Web applications. In semantic wikis, wikis pages are annotated with semantic data to facilitate
the navigation, information retrieving and ontology emerging. Semantic data represents the shared knowledge base which describes
the common understanding of the community. However, in a collaborative knowledge building process the knowledge is basically
created by individuals who are involved in a social process. Therefore, it is fundamental to support personal knowledge building
in a differentiated way. Currently there are no available semantic wikis that support both personal and shared understandings.
In order to overcome this problem, we propose a {P2P} collaborative knowledge building process and extend semantic wikis with
personal annotations facilities to express personal understanding. In this paper, we detail the personal semantic annotation
model and show its implementation in {P2P} semantic wikis. We also detail an evaluation study which shows that personal annotations
demand less cognitive efforts than semantic data and are very useful to enrich the shared knowledge base.},
	booktitle = {Database and Expert Systems Applications},
	author = {Diego Torres and Hala {Skaf-Molli} and Alicia Díaz and Pascal Molli},
	year = {2009},
	pages = {317--331}
},

@incollection{skaf-molli_peer--peer_2009,
	title = {{Peer-to-Peer} Semantic Wikis},
	url = {http://dx.doi.org/10.1007/978-3-642-03573-9_16},
	abstract = {Wikis have demonstrated how it is possible to convert a community of strangers into a community of collaborators. Semantic
wikis have opened an interesting way to mix web 2.0 advantages with the semantic web approach. {P2P} wikis have illustrated
how wikis can be deployed on {P2P} wikis and take advantages of its intrinsic qualities: fault-tolerance, scalability and infrastructure
cost sharing. In this paper, we present the first {P2P} semantic wiki that combines advantages of semantic wikis and {P2P} wikis.
Building a {P2P} semantic wiki is challenging. It requires building an optimistic replication algorithm that is compatible with
{P2P} constraints, ensures an acceptable level of consistency and generic enough to handle semantic wiki pages. The contribution
of this paper is the definition of a clear model for building {P2P} semantic wikis. We define the data model, operations on
this model, intentions of these operations, algorithms to ensure consistency and finally we implement the {SWOOKI} prototype
based on these algorithms.},
	booktitle = {Database and Expert Systems Applications},
	author = {Hala {Skaf-Molli} and Charbel Rahhal and Pascal Molli},
	year = {2009},
	pages = {196--213}
},

@inproceedings{chirkova_materializing_2003,
	address = {New York, {NY,} {USA}},
	series = {{PODS} '03},
	title = {Materializing views with minimal size to answer queries},
	isbn = {1-58113-670-6},
	location = {San Diego, California},
	doi = {10.1145/773153.773158},
	abstract = {In this paper we study the following problem. Given a database and a set of queries, we want to find, in advance, a set of views that can compute the answers to the queries, such that the size of the viewset (i.e., the amount of space, in bytes, required to store the viewset) is minimal on the given database. This problem is important for many applications such as distributed databases, data warehousing, and data integration. We explore the decidability and complexity of the problem for workloads of conjunctive queries. We show that results differ significantly depending on whether the workload queries have self-joins. If queries can have self-joins, then a disjunctive viewset can be a better solution than any set of conjunctive views. We show that the problem of finding a minimal-size disjunctive viewset is decidable, and give an upper bound on its complexity. If workload queries cannot have self-joins, there is no need to consider disjunctive viewsets, and we show that the problem is in {NP.} We describe a very compact search space of conjunctive views, which contains all views in at least one optimal disjunctive viewset. We give a dynamic-programming algorithm for finding minimal-size disjunctive viewsets for queries without self-joins, and discuss heuristics to make the algorithm efficient.},
	booktitle = {Proceedings of the twenty-second {ACM} {SIGMOD-SIGACT-SIGART} symposium on Principles of database systems},
	publisher = {{ACM}},
	author = {Rada Chirkova and Chen Li},
	year = {2003},
	note = {{ACM} {ID:} 773158},
	keywords = {data warehouse and repository, distributed data structures, distributed databases},
	pages = {38–48}
},

@inproceedings{liu_identifying_2007,
	address = {Beijing, China},
	title = {Identifying meaningful return information for {XML} keyword search},
	isbn = {978-1-59593-686-8},
	url = {http://portal.acm.org/citation.cfm?id=1247480.1247518&coll=GUIDE&dl=GUIDE&CFID=75531709&CFTOKEN=65038685},
	doi = {10.1145/1247480.1247518},
	abstract = {Keyword search enables web users to easily access {XML} data without the need to learn a structured query language and to study possibly complex data schemas. Existing work has addressed the problem of selecting qualified data nodes that match keywords and connecting them in a meaningful way, in the spirit of inferring a where clause in {XQuery.} However, how to infer the return clause for keyword search is an open problem.},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Ziyang Liu and Yi Chen},
	year = {2007},
	keywords = {keyword search, xml},
	pages = {329--340}
},

@article{ibaraki_optimal_1984,
	title = {On the optimal nesting order for computing {{\textless}italic{\textgreater}N{\textless}/italic{\textgreater}-relational} joins},
	volume = {9},
	url = {http://portal.acm.org/citation.cfm?id=1498},
	doi = {10.1145/1270.1498},
	abstract = {Using the nested loops method, this paper addresses the problem of minimizing the number of page fetches necessary to evaluate a given query to a relational database. We first propose a data structure whereby the number of page fetches required for query evaluation is substantially reduced and then derive a formula for the expected number of page fetches. An optimal solution to our problem is the nesting order of relations in the evaluation program, which minimizes the number of page fetches. Since the minimization of the formula is {NP-hard,} as shown in the Appendix, we propose a heuristic algorithm which produces a good suboptimal solution in polynomial time. For the special case where the input query is a “tree query,” we present an efficient algorithm for finding an optimal nesting order.},
	number = {3},
	journal = {{ACM} Trans. Database Syst.},
	author = {Toshihide Ibaraki and Tiko Kameda},
	year = {1984},
	pages = {482--502}
},

@article{gavoille_distance_2004,
	title = {Distance labeling in graphs},
	volume = {53},
	issn = {0196-6774},
	url = {http://www.sciencedirect.com/science/article/B6WH3-4CMHT1S-2/2/bcdb86891409465ff98a44fb9816f977},
	doi = {10.1016/j.jalgor.2004.05.002},
	abstract = {We consider the problem of labeling the nodes of a graph in a way that will allow one to compute the distance between any two nodes directly from their labels (without using any additional information). Our main interest is in the minimal length of labels needed in different cases. We obtain upper and lower bounds for several interesting families of graphs. In particular, our main results are the following. For general graphs, we show that the length needed is {[Theta](n).} For trees, we show that the length needed is {[Theta](log2n).} For planar graphs, we show an upper bound of and a lower bound of {[Omega](n1/3).} For bounded degree graphs, we show a lower bound of .
The upper bounds for planar graphs and for trees follow by a more general upper bound for graphs with a r(n)-separator. The two lower bounds, however, are obtained by two different arguments that may be interesting in their own right.
We also show some lower bounds on the length of the labels, even if it is only required that distances be approximated to a multiplicative factor s. For example, we show that for general graphs the required length is {[Omega](n)} for every s{\textless}3. We also consider the problem of the time complexity of the distance function once the labels are computed. We show that there are graphs with optimal labels of length 3logn, such that if we use any labels with fewer than n bits per label, computing the distance function requires exponential time. A similar result is obtained for planar and bounded degree graphs.},
	number = {1},
	journal = {Journal of Algorithms},
	author = {Cyril Gavoille and David Peleg and Stéphane Pérennes and Ran Raz},
	month = oct,
	year = {2004},
	pages = {85--112}
},

@techreport{prudhommeaux_sparql_2008,
	type = {{W3C} {{R}ecommendation}},
	title = {{{SPARQL}} {{Q}uery} {{L}anguage} for {{RDF}}},
	url = {http://www.w3.org/TR/rdf-sparql-query/},
	author = {Eric Prud'hommeaux and Andy Seaborne},
	year = {2008},
	keywords = {bibtex-import, rdf, sparql, swaml}
},

@article{furche_labeling_????,
	title = {Labeling {RDF} Graphs for Linear Time and Space Querying},
	author = {T. Furche and A. Weinzierl and F. Bry}
},

@inproceedings{guo_xrank:_2003,
	title = {{XRANK:} Ranked Keyword Search over {XML} Documents},
	shorttitle = {{XRANK}},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/sigmod/GuoSBS03},
	author = {Lin Guo and Feng Shao and Chavdar Botev and Jayavel Shanmugasundaram},
	year = {2003},
	pages = {16--27}
},

@inproceedings{srivastava_query_2006,
	address = {Seoul, Korea},
	title = {Query optimization over web services},
	url = {http://portal.acm.org/citation.cfm?id=1164159},
	abstract = {Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System {(WSMS)} that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic {WSMS} problem: query optimization for {Select-Project-Join} queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is {NP-hard.} We also give an algorithm for determining the optimal granularity of data "chunks" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.},
	booktitle = {Proceedings of the 32nd international conference on Very large data bases},
	publisher = {{VLDB} Endowment},
	author = {Utkarsh Srivastava and Kamesh Munagala and Jennifer Widom and Rajeev Motwani},
	year = {2006},
	pages = {355--366}
},

@inproceedings{fagin_optimal_2001,
	address = {Santa Barbara, California, United States},
	title = {Optimal aggregation algorithms for middleware},
	isbn = {1-58113-361-8},
	url = {http://portal.acm.org/citation.cfm?doid=375551.375567},
	doi = {10.1145/375551.375567},
	abstract = {Assume that each object in a database has m grades, or scores, one for each of m attributes. For example, an object can have a color grade, that tells how red it is, and a shape grade, that tells how round it is. For each attribute, there is a sorted list, which lists each object and its grade under that attribute, sorted by grade (highest grade first). There is some monotone aggregation function, or combining rule, such as min or average, that combines the individual grades to obtain an overall grade.},
	booktitle = {Proceedings of the twentieth {ACM} {SIGMOD-SIGACT-SIGART} symposium on Principles of database systems},
	publisher = {{ACM}},
	author = {Ronald Fagin and Amnon Lotem and Moni Naor},
	year = {2001},
	pages = {102--113}
},

@article{dragut_hierarchical_2009,
	title = {A hierarchical approach to model web query interfaces for web source integration},
	volume = {2},
	url = {http://portal.acm.org/citation.cfm?id=1687627.1687665&coll=GUIDE&dl=GUIDE&idx=J1174&part=journal&WantType=Journals&title=Proceedings%20of%20the%20VLDB%20Endowment&CFID=82166857&CFTOKEN=58994927},
	abstract = {Much data in the Web is hidden behind Web query interfaces. In most cases the only means to "surface" the content of a Web database is by formulating complex queries on such interfaces. Applications such as Deep Web crawling and Web database integration require an automatic usage of these interfaces. Therefore, an important problem to be addressed is the automatic extraction of query interfaces into an appropriate model. We hypothesize the existence of a set of domain-independent "commonsense design rules" that guides the creation of Web query interfaces. These rules transform query interfaces into schema trees. In this paper we describe a Web query interface extraction algorithm, which combines {HTML} tokens and the geometric layout of these tokens within a Web page. Tokens are classified into several classes out of which the most significant ones are text tokens and field tokens. A tree structure is derived for text tokens using their geometric layout. Another tree structure is derived for the field tokens. The hierarchical representation of a query interface is obtained by iteratively merging these two trees. Thus, we convert the extraction problem into an integration problem. Our experiments show the promise of our algorithm: it outperforms the previous approaches on extracting query interfaces on about 6.5\% in accuracy as evaluated over three corpora with more than 500 Deep Web interfaces from 15 different domains.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {Eduard C. Dragut and Thomas Kabisch and Clement Yu and Ulf Leser},
	year = {2009},
	pages = {325--336}
},

@inproceedings{ladwig_linked_2010,
	title = {Linked Data Query Processing Strategies},
	doi = {10.1007/978-3-642-17746-0_29},
	booktitle = {Proceedings of the 9th International Semantic Web Conference ({ISWC})},
	author = {Günter Ladwig and Tran Thanh},
	year = {2010}
},

@article{aguilera_practical_2008,
	title = {A practical scalable distributed B-tree},
	volume = {1},
	url = {http://portal.acm.org/citation.cfm?id=1453856.1453922&coll=GUIDE&dl=GUIDE&CFID=48492463&CFTOKEN=96497791},
	doi = {10.1145/1453856.1453922},
	abstract = {Internet applications increasingly rely on scalable data structures that must support high throughput and store huge amounts of data. These data structures can be hard to implement efficiently. Recent proposals have overcome this problem by giving up on generality and implementing specialized interfaces and functionality (e.g., Dynamo [4]). We present the design of a more general and flexible solution: a fault-tolerant and scalable distributed B-tree. In addition to the usual B-tree operations, our B-tree provides some important practical features: transactions for atomically executing several operations in one or more B-trees, online migration of B-tree nodes between servers for load-balancing, and dynamic addition and removal of servers for supporting incremental growth of the system.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {Marcos K. Aguilera and Wojciech Golab and Mehul A. Shah},
	year = {2008},
	pages = {598--609}
},

@inproceedings{ioannidis_history_2003,
	address = {Berlin, Germany},
	title = {The history of histograms (abridged)},
	isbn = {0-12-722442-4},
	url = {http://portal.acm.org/citation.cfm?id=1315451.1315455},
	abstract = {The history of histograms is long and rich, full of detailed information in every step. It includes the course of histograms in different scientific fields, the successes and failures of histograms in approximating and compressing information, their adoption by industry, and solutions that have been given on a great variety of histogram-related problems. In this paper and in the same spirit of the histogram techniques themselves, we compress their entire history (including their "future history" as currently anticipated) in the given/fixed space budget, mostly recording details for the periods, events, and results with the highest (personally-biased) interest. In a limited set of experiments, the semantic distance between the compressed and the full form of the history was found relatively small!},
	booktitle = {Proceedings of the 29th international conference on Very large data bases - Volume 29},
	publisher = {{VLDB} Endowment},
	author = {Yannis Ioannidis},
	year = {2003},
	pages = {19--30}
},

@inproceedings{tsirogiannis_query_2009,
	address = {Providence, Rhode Island, {USA}},
	title = {Query processing techniques for solid state drives},
	isbn = {978-1-60558-551-2},
	url = {http://portal.acm.org/citation.cfm?id=1559845.1559854&coll=GUIDE&dl=GUIDE&type=series&idx=SERIES473&part=series&WantType=Proceedings&title=SIGMOD&CFID=82166658&CFTOKEN=88298104},
	doi = {10.1145/1559845.1559854},
	abstract = {Solid state drives perform random reads more than 100x faster than traditional magnetic hard disks, while offering comparable sequential read and write bandwidth. Because of their potential to speed up applications, as well as their reduced power consumption, these new drives are expected to gradually replace hard disks as the primary permanent storage media in large data centers. However, although they may benefit applications that stress random reads immediately, they may not improve database applications, especially those running long data analysis queries. Database query processing engines have been designed around the speed mismatch between random and sequential {I/O} on hard disks and their algorithms currently emphasize sequential accesses for disk-resident data.},
	booktitle = {Proceedings of the 35th {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Dimitris Tsirogiannis and Stavros Harizopoulos and Mehul A. Shah and Janet L. Wiener and Goetz Graefe},
	year = {2009},
	keywords = {columnar storage, flash memory, join index, late materialization, semi-join reduction, ssd},
	pages = {59--72}
},

@article{li_minimizing_2003,
	title = {Minimizing {Data-Communication} Costs by Decomposing Query Results in {Client-Server} Environments},
	volume = {1429},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.2272},
	journal = {{INFORMATION} {AND} {COMPUTER} {SCIENCE}},
	author = {Jia Li and Rada Chirkova and Chen Li},
	year = {2003},
	pages = {2---7}
},

@inproceedings{abadi_scalable_2007,
	address = {Vienna, Austria},
	title = {Scalable semantic web data management using vertical partitioning},
	isbn = {978-1-59593-649-3},
	url = {http://portal.acm.org/citation.cfm?id=1325851.1325900&coll=Portal&dl=GUIDE&CFID=49054946&CFTOKEN=32325599},
	abstract = {Efficient management of {RDF} data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for {RDF} data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for {RDF} databases and consider a recent suggestion, "property tables." We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution: vertically partitioning the {RDF} data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based {RDF} browser over a large-scale (more than 50 million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented {DBMS} (a database architected specially for the vertically partitioned case) is used instead of a row-oriented {DBMS,} another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.},
	booktitle = {Proceedings of the 33rd international conference on Very large data bases},
	publisher = {{VLDB} Endowment},
	author = {Daniel J. Abadi and Adam Marcus and Samuel R. Madden and Kate Hollenbach},
	year = {2007},
	pages = {411--422}
},

@inproceedings{amer-yahia_structure_2005,
	title = {Structure and Content Scoring for {XML}},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/vldb/Amer-YahiaKMST05},
	author = {Sihem {Amer-Yahia} and Nick Koudas and Amélie Marian and Divesh Srivastava and David Toman},
	year = {2005},
	pages = {361--372}
},

@phdthesis{Neumann_2005, title={Efficient Generation and Execution of DAG-Structured Query Graphs}, url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.9131&rep=rep1&type=pdf}, school={Citeseer}, author={Neumann, Thomas}, year={2005}}

@article{neumann_rdf-3x:_2008,
	title = {{RDF-3X:} a {RISC-style} engine for {RDF}},
	volume = {1},
	shorttitle = {{RDF-3X}},
	url = {http://portal.acm.org/citation.cfm?id=1453856.1453927&coll=Portal&dl=GUIDE&CFID=48494989&CFTOKEN=32474547},
	doi = {10.1145/1453856.1453927},
	abstract = {{RDF} is a data representation format for schema-free structured information that is gaining momentum in the context of {Semantic-Web} corpora, life sciences, and also Web 2.0 platforms. The "pay-as-you-go" nature of {RDF} and the flexible pattern-matching capabilities of its query language {SPARQL} entail efficiency and scalability challenges for complex queries including long join paths. This paper presents the {RDF-3X} engine, an implementation of {SPARQL} that achieves excellent performance by pursuing a {RISC-style} architecture with a streamlined architecture and carefully designed, puristic data structures and operations. The salient points of {RDF-3X} are: 1) a generic solution for storing and indexing {RDF} triples that completely eliminates the need for physical-design tuning, 2) a powerful yet simple query processor that leverages fast merge joins to the largest possible extent, and 3) a query optimizer for choosing optimal join orders using a cost model based on statistical synopses for entire join paths. The performance of {RDF-3X,} in comparison to the previously best state-of-the-art systems, has been measured on several large-scale datasets with more than 50 million {RDF} triples and benchmark queries that include pattern matching and long join paths in the underlying data graphs.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {Thomas Neumann and Gerhard Weikum},
	year = {2008},
	pages = {647--659}
},

@inproceedings{selinger_access_1979-1,
	address = {Boston, Massachusetts},
	title = {Access path selection in a relational database management system},
	isbn = {{0-89791-001-X}},
	url = {http://portal.acm.org/citation.cfm?id=582099&dl=GUIDE&coll=GUIDE&CFID=64839601&CFTOKEN=92982021},
	doi = {10.1145/582095.582099},
	abstract = {In a high level query and data manipulation language such as {SQL,} requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the {IBM} San Jose Research Laboratory.},
	booktitle = {Proceedings of the 1979 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {P. Griffiths Selinger and M. M. Astrahan and D. D. Chamberlin and R. A. Lorie and T. G. Price},
	year = {1979},
	pages = {23--34}
},

@inproceedings{cranor_gigascope:_2003,
	address = {San Diego, California},
	title = {Gigascope: a stream database for network applications},
	isbn = {{1-58113-634-X}},
	shorttitle = {Gigascope},
	url = {http://portal.acm.org/citation.cfm?id=872757.872838},
	doi = {10.1145/872757.872838},
	abstract = {We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the {AT\&T} network, including at {OC48} routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.},
	booktitle = {Proceedings of the 2003 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Chuck Cranor and Theodore Johnson and Oliver Spataschek and Vladislav Shkapenyuk},
	year = {2003},
	pages = {647--651}
},

@inproceedings{cheng_-line_2009,
	address = {Saint Petersburg, Russia},
	title = {On-line exact shortest distance query processing},
	isbn = {978-1-60558-422-5},
	url = {http://portal.acm.org/citation.cfm?id=1516417},
	doi = {10.1145/1516360.1516417},
	abstract = {Shortest-path query processing not only serves as a long established routine for numerous applications in the past but also is of increasing popularity to support novel graph applications in very large databases nowadays. For a large graph, there is the new scenario to query intensively against arbitrary nodes, asking to quickly return node distance or even shortest paths. And traditional main memory algorithms and shortest paths materialization become inadequate. We are interested in graph labelings to encode the underlying graphs and assign labels to nodes to support efficient query processing. Surprisingly, the existing work of this category mainly emphasizes on reachability query processing, while no sufficient effort has been given to distance labelings to support querying exact shortest distances between nodes. Distance labelings must be developed on the graph in whole to correctly retain node distance information. It makes many existing methods to be inapplicable. We focus on fast computing distance-aware 2-hop covers, which can encode the all-pairs shortest paths of a graph in {O({\textbar}V{\textbar}·{\textbar}E{\textbar}1/2)} space. Our approach exploits strongly connected components collapsing and graph partitioning to gain speed, while it can overcome the challenges in correctly retaining node distance information and appropriately encoding all-pairs shortest paths with small overhead. Furthermore, our approach avoids pre-computing all-pairs shortest paths, which can be prohibitive over large graphs. We conducted extensive performance studies, and confirm the efficiency of our proposed new approaches.},
	booktitle = {Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology},
	publisher = {{ACM}},
	author = {Jiefeng Cheng and Jeffrey Xu Yu},
	year = {2009},
	pages = {481--492}
},

@incollection{bouros_evaluating_2009,
	title = {Evaluating Reachability Queries over Path Collections},
	url = {http://dx.doi.org/10.1007/978-3-642-02279-1_29},
	abstract = {Several applications in areas such as biochemistry, {GIS,} involve storing and querying large volumes of sequential data stored
as path collections. There is a number of interesting queries that can be posed on such data. This work focuses on reachability queries: given
a path collection and two nodes v

s
, v

t
, determine whether a path from v

s
to v

t
exists and identify it. To answer these queries, the path-first search paradigm, which treats paths as first-class citizens, is proposed. To improve the performance of our techniques, two indexing
structures that capture the reachability information of paths are introduced. Further, methods for updating a path collection
and its indices are discussed. Finally, an extensive experimental evaluation verifies the advantages of our approach.},
	booktitle = {Scientific and Statistical Database Management},
	author = {Panagiotis Bouros and Spiros Skiadopoulos and Theodore Dalamagas and Dimitris Sacharidis and Timos Sellis},
	year = {2009},
	pages = {398--416}
},

@inproceedings{tran_query_2009,
	address = {Providence, Rhode Island, {USA}},
	title = {Query by output},
	isbn = {978-1-60558-551-2},
	url = {http://portal.acm.org/citation.cfm?id=1559845.1559902&coll=GUIDE&dl=GUIDE&type=series&idx=SERIES473&part=series&WantType=Proceedings&title=SIGMOD&CFID=82166658&CFTOKEN=88298104},
	doi = {10.1145/1559845.1559902},
	abstract = {It has recently been asserted that the usability of a database is as important as its capability. Understanding the database schema, the hidden relationships among attributes in the data all play an important role in this context. Subscribing to this viewpoint, in this paper, we present a novel data-driven approach, called Query By Output {(QBO),} which can enhance the usability of database systems. The central goal of {QBO} is as follows: given the output of some query Q on a database D, denoted by {Q(D),} we wish to construct an alternative query Q′ such that {Q(D)} and Q′ {(D)} are instance-equivalent. To generate instance-equivalent queries from {Q(D),} we devise a novel data classification-based technique that can handle the at-least-one semantics that is inherent in the query derivation. In addition to the basic framework, we design several optimization techniques to reduce processing overhead and introduce a set of criteria to rank order output queries by various notions of utility. Our framework is evaluated comprehensively on three real data sets and the results show that the instance-equivalent queries we obtain are interesting and that the approach is scalable and robust to queries of different selectivities.},
	booktitle = {Proceedings of the 35th {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Quoc Trung Tran and {Chee-Yong} Chan and Srinivasan Parthasarathy},
	year = {2009},
	keywords = {at-least-one semantics, instance-equivalent queries, query by output},
	pages = {535--548}
},

@inproceedings{dasgupta_unbiased_2010,
	address = {New York, {NY,} {USA}},
	series = {{SIGMOD} '10},
	title = {Unbiased estimation of size and other aggregates over hidden web databases},
	isbn = {978-1-4503-0032-2},
	location = {Indianapolis, Indiana, {USA}},
	doi = {10.1145/1807167.1807259},
	abstract = {Many websites provide restrictive form-like interfaces which allow users to execute search queries on the underlying hidden databases. In this paper, we consider the problem of estimating the size of a hidden database through its web interface. We propose novel techniques which use a small number of queries to produce unbiased estimates with small variance. These techniques can also be used for approximate query processing over hidden databases. We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach.},
	booktitle = {Proceedings of the 2010 international conference on Management of data},
	publisher = {{ACM}},
	author = {Arjun Dasgupta and Xin Jin and Bradley Jewell and Nan Zhang and Gautam Das},
	year = {2010},
	note = {{ACM} {ID:} 1807259},
	keywords = {aggregate query processing, algorithms, database administration, hidden databases, measurement, performance, to-read, web-based services},
	pages = {855–866}
},

@inproceedings{maduko_estimating_2007,
	address = {Banff, Alberta, Canada},
	title = {Estimating the cardinality of {RDF} graph patterns},
	isbn = {978-1-59593-654-7},
	url = {http://portal.acm.org/citation.cfm?id=1242572.1242782},
	doi = {10.1145/1242572.1242782},
	abstract = {Most {RDF} query languages allow for graph structure search through a conjunction of triples which is typically processed using join operations. A key factor in optimizing joins is determining the join order which depends on the expected cardinality of intermediate results. This work proposes a pattern-based summarization framework for estimating the cardinality of {RDF} graph patterns. We present experiments on real world and synthetic datasets which confirm the feasibility of our approach.},
	booktitle = {Proceedings of the 16th international conference on World Wide Web},
	publisher = {{ACM}},
	author = {Angela Maduko and Kemafor Anyanwu and Amit Sheth and Paul Schliekelman},
	year = {2007},
	keywords = {pattern cardinality estimation, statistical summaries},
	pages = {1233--1234}
},

@inproceedings{zou_dominant_2008,
	title = {Dominant Graph: An Efficient Indexing Structure to Answer
               {Top-K} Queries},
	shorttitle = {Dominant Graph},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/icde/ZouC08},
	author = {Lei Zou and Lei Chen 0002},
	year = {2008},
	keywords = {top-k},
	pages = {536--545}
},

@inproceedings{heine_processing_2005,
	address = {Bremen, Germany},
	title = {Processing complex {RDF} queries over {P2P} networks},
	isbn = {1-59593-164-3},
	url = {http://portal.acm.org/citation.cfm?id=1096952.1096960},
	doi = {10.1145/1096952.1096960},
	abstract = {In large-scale distributed systems, information is typically generated decentralized. However, for many applications it is desirable to have a unified view on this knowledge, allowing to reason about it and to query it without regarding the heterogeneity of the underlying systems. In this context, two main requirements have to be fulfilled. On the one hand, a retrieval system has to be semantically rich, in order to be able to cope with and mediate between different schemas, and on the other hand it has to be scalable to large numbers of information sources. The dynamic nature of information makes the problem even {worse.Within} this paper, we propose a solution to this problem. We describe a {DHT-based} peer-2-peer network storing knowledge in the form of {RDF} triples. The query evaluation algorithm allows to use arbitrary query patterns, and evaluates the query with respect to taxonomical reasoning. Thus the system combines expressivity and scalability. Although we describe the whole system, the focus of this paper is the query {evaluation.The} system is generic by nature and suitable for numerous different applications. We describe an example application stemming from the Semantic Grid.},
	booktitle = {Proceedings of the 2005 {ACM} workshop on Information retrieval in peer-to-peer networks},
	publisher = {{ACM}},
	author = {Felix Heine and Matthias Hovestadt and Odej Kao},
	year = {2005},
	keywords = {grid, p2p, query processing, rdf, rdfs, semantic Web},
	pages = {41--48}
},

@article{zhou_graph_2009,
	title = {Graph clustering based on structural/attribute similarities},
	volume = {2},
	url = {http://portal.acm.org/citation.cfm?id=1687627.1687709&coll=GUIDE&dl=GUIDE&idx=J1174&part=journal&WantType=Journals&title=Proceedings%20of%20the%20VLDB%20Endowment&CFID=82166857&CFTOKEN=58994927},
	abstract = {The goal of graph clustering is to partition vertices in a large graph into different clusters based on various criteria such as vertex connectivity or neighborhood similarity. Graph clustering techniques are very useful for detecting densely connected groups in a large graph. Many existing graph clustering methods mainly focus on the topological structure for clustering, but largely ignore the vertex properties which are often heterogenous. In this paper, we propose a novel graph clustering algorithm, {SA-Cluster,} based on both structural and attribute similarities through a unified distance measure. Our method partitions a large graph associated with attributes into k clusters so that each cluster contains a densely connected subgraph with homogeneous attribute values. An effective method is proposed to automatically learn the degree of contributions of structural similarity and attribute similarity. Theoretical analysis is provided to show that {SA-Cluster} is converging. Extensive experimental results demonstrate the effectiveness of {SA-Cluster} through comparison with the state-of-the-art graph clustering and summarization methods.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {Yang Zhou and Hong Cheng and Jeffrey Xu Yu},
	year = {2009},
	pages = {718--729}
},

@inproceedings{haixun_wang_dual_2006,
	title = {Dual Labeling: Answering Graph Reachability Queries in Constant Time},
	shorttitle = {Dual Labeling},
	doi = {10.1109/ICDE.2006.53},
	abstract = {Graph reachability is fundamental to a wide range of applications, including {XML} indexing, geographic navigation, Internet routing, ontology queries based on {RDF/OWL,} etc. Many applications involve huge graphs and require fast answering of reachability queries. Several reachability labeling methods have been proposed for this purpose. They assign labels to the vertices, such that the reachability between any two vertices may be decided using their labels only. For sparse graphs, 2-hop based reachability labeling schemes answer reachability queries efficiently using relatively small label space. However, the labeling process itself is often too time consuming to be practical for large graphs. In this paper, we propose a novel labeling scheme for sparse graphs. Our scheme ensures that graph reachability queries can be answered in constant time. Furthermore, for sparse graphs, the complexity of the labeling process is almost linear, which makes our algorithm applicable to massive datasets. Analytical and experimental results show that our approach is much more efficient than stateof- the-art approaches. Furthermore, our labeling method also provides an alternative scheme to tradeoff query time for label space, which further benefits applications that use tree-like graphs.},
	booktitle = {Data Engineering, 2006. {ICDE} '06. Proceedings of the 22nd International Conference on},
	author = {Haixun Wang and Hao He and Jun Yang and {P.S.} Yu},
	year = {2006},
	pages = {75}
},

@inproceedings{leskovec_sampling_2006,
	address = {Philadelphia, {PA,} {USA}},
	title = {Sampling from large graphs},
	isbn = {1-59593-339-5},
	url = {http://portal.acm.org/citation.cfm?id=1150402.1150479},
	doi = {10.1145/1150402.1150479},
	abstract = {Given a huge real graph, how can we derive a representative sample? There are many known algorithms to compute interesting measures (shortest paths, centrality, betweenness, etc.), but several of them become impractical for large graphs. Thus graph sampling is {essential.The} natural questions to ask are (a) which sampling method to use, (b) how small can the sample size be, and (c) how to scale up the measurements of the sample (e.g., the diameter), to get estimates for the large graph. The deeper, underlying question is subtle: how do we measure {success?.We} answer the above questions, and test our answers by thorough experiments on several, diverse datasets, spanning thousands nodes and edges. We consider several sampling methods, propose novel methods to check the goodness of sampling, and develop a set of scaling laws that describe relations between the properties of the original and the {sample.In} addition to the theoretical contributions, the practical conclusions from our work are: Sampling strategies based on edge selection do not perform well; simple uniform random node selection performs surprisingly well. Overall, best performing methods are the ones based on random-walks and "forest fire"; they match very accurately both static as well as evolutionary graph patterns, with sample sizes down to about 15\% of the original graph.},
	booktitle = {Proceedings of the 12th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {{ACM}},
	author = {Jure Leskovec and Christos Faloutsos},
	year = {2006},
	keywords = {graph mining, graph sampling, scaling laws},
	pages = {631--636}
},

@inproceedings{yangjun_chen_efficient_2008,
	title = {An Efficient Algorithm for Answering Graph Reachability Queries},
	abstract = {Given a directed graph G, to check whether a node v is reachable from another node u through a path is often required. In a database system, such an operation is called a recursion computation or reachability checking and not efficiently supported. The reason for this is that the space to store the whole transitive closure of G is prohibitively high. In this paper, we address this issue and propose an 0(n2 + bnradic(b)) time algorithm to decompose a directed acyclic graph {(DAG)} into a minimized set of disjoint chains to facilitate reachability checking, where n is the number of the nodes and b is the {DAG's} width, defined to be the size of a largest node subset U of the {DAG} such that for every pair of nodes u, v isin U, there does not exist a path from u to v or from v to u. Using this algorithm, we are able to label a graph in 0(be) time and store all the labels in O(bn) space with O(logb) reachability checking time, where e is the number of the edges of the {DAG.} The method can also be extended to handle cyclic directed graphs. Experiments have been performed, showing that our method is promising.},
	booktitle = {Data Engineering, 2008. {ICDE} 2008. {IEEE} 24th International Conference on},
	author = {Yangjun Chen and Yibin Chen},
	year = {2008},
	keywords = {computational complexity, database system, directed acyclic graph, directed graphs, graph reachability queries, query processing, reachability analysis, reachability checking, recursion computation},
	pages = {893--902}
},

@article{ilyas_supporting_2004,
	title = {Supporting top-k join queries in relational databases},
	volume = {13},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=journals/vldb/IlyasAE04},
	number = {3},
	journal = {{VLDB} J.},
	author = {Ihab F. Ilyas and Walid G. Aref and Ahmed K. Elmagarmid},
	year = {2004},
	keywords = {top-k},
	pages = {207--221}
},

@inproceedings{ding_mjoin:_2003,
	address = {San Diego, California},
	title = {{MJoin:} a metadata-aware stream join operator},
	isbn = {1-58113-843-1},
	shorttitle = {{MJoin}},
	url = {http://portal.acm.org/citation.cfm?id=966618.966639},
	doi = {10.1145/966618.966639},
	abstract = {Join algorithms must be re-designed when processing stream data instead of persistently stored data. Data streams are potentially infinite and the query result is expected to be generated incrementally instead of once only. Data arrival patterns are often unpredictable and the statistics of the data and other relevant metadata often are only known at runtime. In some cases they are supplied interleaved with the actual data in the form of stream markers. Recently, stream join algorithms, like Symmetric Hash Join and {XJoin,} have been designed to perform in a pipelined fashion to cope with the latent delivery of data. However, none of them to date takes metadata, especially runtime metadata, into consideration. Hence, the join execution logic defined statically before runtime may not be well suited to deal with varying types of dynamic runtime scenarios. Also the potentially unbounded state needs to be maintained by the join operator to guarantee the precision of the result. In this paper, we propose a metadata-aware stream join operator called {MJoin} which is able to exploit metadata to (1) detect and purge useless materialized data to save computation resources and (2) optimize the execution logic to target diferent optimization goals. We have implemented the {MJoin} operator. The experimental results validate our metadata-driven join optimization strategies.},
	booktitle = {Proceedings of the 2nd international workshop on Distributed event-based systems},
	publisher = {{ACM}},
	author = {Luping Ding and Elke A. Rundensteiner and George T. Heineman},
	year = {2003},
	keywords = {constraint, join algorithms, metadata, optimization, xml stream, xquery subscription},
	pages = {1--8}
},

@inproceedings{pomares_source_2010,
	title = {Source Selection in Large Scale Data Contexts: An Optimization Approach},
	booktitle = {Database and Expert Systems Applications},
	author = {A. Pomares and C. Roncancio and V. D Cung and J. Abásolo and M. P Villamil},
	year = {2010},
	pages = {46–61}
},

@inproceedings{cohen_reachability_2002,
	address = {San Francisco, California},
	title = {Reachability and distance queries via 2-hop labels},
	isbn = {{0-89871-513-X}},
	url = {http://portal.acm.org/citation.cfm?id=545503},
	abstract = {Reachability and distance queries in graphs are fundamental to numerous applications, ranging from geographic navigation systems to Internet routing. Some of these applications involve huge graphs and yet require fast query answering. We propose a new data structure for representing all distances in a graph. The data structure is distributed in the sense that it may be viewed as assigning labels to the vertices, such that a query involving vertices u and v may be answered using only the labels of u and {v.Our} labels are based on 2-hop covers of the shortest paths, or of all paths, in a graph. For shortest paths, such a cover is a collection S of shortest paths such that for every two vertices u and v, there is a shortest path from u to v that is a concatenation of two paths from S. We describe an efficient algorithm for finding an almost optimal 2-hop cover of a given collection of paths. Our approach is general and can be applied to directed or undirected graphs, exact or approximate shortest paths, or to reachability {queries.We} study the proposed data structure using a combination of theoretical and experimental means. We implemented our algorithm and checked the size of the resulting data structure on several real-life networks from different application areas. Our experiments show that the total size of the labels is typically not much larger than the network itself, and is usually considerably smaller than an explicit representation of the transitive closure of the network.},
	booktitle = {Proceedings of the thirteenth annual {ACM-SIAM} symposium on Discrete algorithms},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Edith Cohen and Eran Halperin and Haim Kaplan and Uri Zwick},
	year = {2002},
	pages = {937--946}
},

@inproceedings{haas_fixed-precision_1993,
	address = {Washington, {D.C.,} United States},
	title = {Fixed-precision estimation of join selectivity},
	isbn = {0-89791-593-3},
	url = {http://portal.acm.org/citation.cfm?id=153850.153875&type=series},
	doi = {10.1145/153850.153875},
	abstract = {We compare the performance of sampling-based procedures for estimation of the selectivity of an equijoin. While some of the procedures have been proposed in the database sampling literature, their relative performance has never been analyzed. A main result of this paper is a partial ordering that compares the variability of the estimators for the different procedures after an arbitrary fixed number of sampling steps. Prior to the current work, it was also unknown whether these fixed-step estimation procedures can be extended to asymptotically efficient fixed-precision estimation procedures. Our second main result is a general method for such an extension and a proof that the method is valid for all the estimation procedures under consideration. Finally, we show that, under reasonable assumptions on sampling costs, the partial ordering on the variability of the fixed-step estimation procedures implies a partial ordering on the cost of the corresponding fixed-precision estimation procedures. These results lead to a new algorithm for fixed-precision estimation of the selectivity of an equijoin. The algorithm appears to be the best available when there are no indices on the join key. Our results can be extended to general select-join queries.},
	booktitle = {Proceedings of the twelfth {ACM} {SIGACT-SIGMOD-SIGART} symposium on Principles of database systems},
	publisher = {{ACM}},
	author = {Peter J. Haas and Jeffrey F. Naughton and S. Seshadri and Arun N. Swami},
	year = {1993},
	pages = {190--201}
},

@inproceedings{tao_rpj:_2005,
	address = {New York, {NY,} {USA}},
	series = {{SIGMOD} '05},
	title = {{RPJ:} producing fast join results on streams through rate-based optimization},
	isbn = {1-59593-060-4},
	location = {Baltimore, Maryland},
	shorttitle = {{RPJ}},
	doi = {10.1145/1066157.1066200},
	abstract = {We consider the problem of "progressively" joining relations whose records are continuously retrieved from remote sources through an unstable network that may incur temporary failures. The objectives are to (i) start reporting the first output tuples as soon as possible (before the participating relations are completely received), and (ii) produce the remaining results at a fast rate. We develop a new algorithm {RPJ} {({\textless}u{\textgreater}R{\textless}/u{\textgreater}ate-based} {{\textless}u{\textgreater}P{\textless}/u{\textgreater}rogressive} {{\textless}u{\textgreater}J{\textless}/u{\textgreater}oin)} based on solid theoretical analysis. {RPJ} maximizes the output rate by optimizing its execution according to the characteristics of the join relations (e.g., data distribution, tuple arrival pattern, etc.). Extensive experiments prove that our technique delivers results significantly faster than the previous methods.},
	booktitle = {Proceedings of the 2005 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Yufei Tao and Man Lung Yiu and Dimitris Papadias and Marios Hadjieleftheriou and Nikos Mamoulis},
	year = {2005},
	note = {{ACM} {ID:} 1066200},
	pages = {371–382}
},

@inproceedings{li_supporting_2006,
	address = {Chicago, {IL,} {USA}},
	title = {Supporting ad-hoc ranking aggregates},
	isbn = {1-59593-434-0},
	url = {http://portal.acm.org/citation.cfm?doid=1142473.1142481},
	doi = {10.1145/1142473.1142481},
	abstract = {This paper presents a principled framework for efficient processing of ad-hoc top-k (ranking) aggregate queries, which provide the k groups with the highest aggregates as results. Essential support of such queries is lacking in current systems, which process the queries in a naïve materialize-group-sort scheme that can be prohibitively inefficient. Our framework is based on three fundamental principles. The {Upper-Bound} Principle dictates the requirements of early pruning, and the {Group-Ranking} and {Tuple-Ranking} Principles dictate group-ordering and tuple-ordering requirements. They together guide the query processor toward a provably optimal tuple schedule for aggregate query processing. We propose a new execution framework to apply the principles and requirements. We address the challenges in realizing the framework and implementing new query operators, enabling efficient group-aware and rank-aware query plans. The experimental study validates our framework by demonstrating orders of magnitude performance improvement in the new query plans, compared with the traditional plans.},
	booktitle = {Proceedings of the 2006 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Chengkai Li and Kevin {Chen-Chuan} Chang and Ihab F. Ilyas},
	year = {2006},
	keywords = {aggregate query, decision support, olap, ranking, top-k query processing},
	pages = {61--72}
},

@inproceedings{ives_sideways_2008,
	title = {Sideways Information Passing for {Push-Style} Query Processing},
	isbn = {978-1-4244-1836-7},
	url = {http://portal.acm.org/citation.cfm?id=1546682.1547103},
	abstract = {In many modern data management settings, data is queried from a central node or nodes, but is stored at remote sources. In such a setting it is common to perform "push-style" query processing, using multithreaded pipelined hash joins and bushy query plans to compute parts of the query in parallel; to avoid idling, the {CPU} can switch between them as delays are encountered. This works well for simple select-project-join queries, but increasingly, Web and integration applications require more complex queries with multiple joins and even nested subqueries. As we demonstrate in this paper, push-style execution of complex queries can be improved substantially via sideways information passing; push-style queries provide many opportunities for information passing that have not been studied in the past literature. We present adaptive information passing, a general runtime decision-making technique for reusing intermediate state from one query subresult to prune and reduce computation of other subresults. We develop two alternative schemes for performing adaptive information passing, which we study in several settings under a variety of workloads.},
	booktitle = {Proceedings of the 2008 {IEEE} 24th International Conference on Data Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Zachary G. Ives and Nicholas E. Taylor},
	year = {2008},
	pages = {774--783}
},

@inproceedings{li_ease:_2008,
	address = {Vancouver, Canada},
	title = {{EASE:} an effective 3-in-1 keyword search method for unstructured, semi-structured and structured data},
	isbn = {978-1-60558-102-6},
	shorttitle = {{EASE}},
	url = {http://portal.acm.org/citation.cfm?id=1376706},
	doi = {10.1145/1376616.1376706},
	abstract = {Conventional keyword search engines are restricted to a given data model and cannot easily adapt to unstructured, semi-structured or structured data. In this paper, we propose an efficient and adaptive keyword search method, called {EASE,} for indexing and querying large collections of heterogenous data. To achieve high efficiency in processing keyword queries, we first model unstructured, semi-structured and structured data as graphs, and then summarize the graphs and construct graph indices instead of using traditional inverted indices. We propose an extended inverted index to facilitate keyword-based search, and present a novel ranking mechanism for enhancing search effectiveness. We have conducted an extensive experimental study using real datasets, and the results show that {EASE} achieves both high search efficiency and high accuracy, and outperforms the existing approaches significantly.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Guoliang Li and Beng Chin Ooi and Jianhua Feng and Jianyong Wang and Lizhu Zhou},
	year = {2008},
	keywords = {graph index, indexing, keyword search, ranking},
	pages = {903--914}
},

@incollection{ge_object_2010,
	title = {Object Link Structure in the Semantic Web},
	url = {http://dx.doi.org/10.1007/978-3-642-13489-0_18},
	abstract = {Lots of {RDF} data have been published in the Semantic Web. The {RDF} data model, together with the decentralized linkage nature
of the Semantic Web, brings object link structure to the worldwide scope. Object links are critical to the Semantic Web and
the macroscopic properties of object links are helpful for better understanding the current Data Web. In this paper, we propose
a notion of object link graph {(OLG)} in the Semantic Web, and analyze the complex network structure of an {OLG} constructed from
the latest dataset {(FC09)} collected by the Falcons search engine. We find that the {OLG} has the scale-free nature and the approximate
effective diameter of the graph is small compared to its scale, which are also consistent with the experimental result based
on our last year’s dataset {(FC08).} The amount of {RDF} documents and objects by Falcons both doubled during the past year, but
the object link graph remains the same density while the diameter is getting shrinking. We also repeat the complex network
analysis on the two largest domain-specific subsets of {FC09,} namely {Bio2RDF(FC09)} and {DBpedia(FC09).} The results show that
both {Bio2RDF(FC09)} and {DBpedia(FC09)} have low density in object links, which contribute to the low density of object links
in {FC09.}},
	booktitle = {The Semantic Web: Research and Applications},
	author = {Weiyi Ge and Jianfeng Chen and Wei Hu and Yuzhong Qu},
	year = {2010},
	pages = {257--271}
},

@inproceedings{gounaris_robust_2008,
	title = {Robust Runtime Optimization of Data Transfer in Queries over Web Services},
	url = {10.1109/ICDE.2008.4497468},
	doi = {10.1109/ICDE.2008.4497468},
	abstract = {Self-managing solutions have recently attracted a lot of interest from the database community. The need for self-* properties is more evident in distributed applications comprising heterogeneous and autonomous databases and functionality providers. Such resources are typically exposed as Web Services {(WSs),} which encapsulate remote {DBMSs} and functions called from within database queries. In this setting, database queries are over {WSs,} and the data transfer cost becomes the main bottleneck. To reduce this cost, data is shipped to and from {WSs} in chunks; however the optimum chunk size is volatile, depending on both the resources' runtime properties and the query. In this paper we propose a robust control theoretical solution to the problem of optimizing the data transfer in queries over {WSs,} by continuously tuning at runtime the block size and thus tracking the optimum point. Also, we develop online system identification mechanisms that are capable of estimating the optimum block size analytically. Both contributions are evaluated via both empirical experimentation in a real environment and simulations, and have been proved to be more effective and efficient than static solutions.},
	booktitle = {Data Engineering, 2008. {ICDE} 2008. {IEEE} 24th International Conference on},
	author = {A. Gounaris and C. Yfoulis and R. Sakellariou and {M.D.} Dikaiakos},
	year = {2008},
	keywords = {database queries, query data transfer runtime optimization, query processing, Web services},
	pages = {596--605}
},

@inproceedings{xiao_efficiently_2009,
	address = {Saint Petersburg, Russia},
	title = {Efficiently indexing shortest paths by exploiting symmetry in graphs},
	isbn = {978-1-60558-422-5},
	url = {http://portal.acm.org/citation.cfm?id=1516360.1516418},
	doi = {10.1145/1516360.1516418},
	abstract = {Shortest path queries {(SPQ)} are essential in many graph analysis and mining tasks. However, answering shortest path queries on-the-fly on large graphs is costly. To online answer shortest path queries, we may materialize and index shortest paths. However, a straightforward index of all shortest paths in a graph of N vertices takes {O(N2)} space. In this paper, we tackle the problem of indexing shortest paths and online answering shortest path queries. As many large real graphs are shown richly symmetric, the central idea of our approach is to use graph symmetry to reduce the index size while retaining the correctness and the efficiency of shortest path query answering. Technically, we develop a framework to index a large graph at the orbit level instead of the vertex level so that the number of breadth-first search trees materialized is reduced from {O(N)} to O({\textbar}Δ{\textbar}), where {\textbar}Δ{\textbar} ≤ N is the number of orbits in the graph. We explore orbit adjacency and local symmetry to obtain compact breadth-first-search trees (compact {BFS-trees).} An extensive empirical study using both synthetic data and real data shows that compact {BFS-trees} can be built efficiently and the space cost can be reduced substantially. Moreover, online shortest path query answering can be achieved using compact {BFS-trees.}},
	booktitle = {Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology},
	publisher = {{ACM}},
	author = {Yanghua Xiao and Wentao Wu and Jian Pei and Wei Wang and Zhenying He},
	year = {2009},
	pages = {493--504}
},

@inproceedings{qi_sum-max_2007,
	address = {Vienna, Austria},
	title = {Sum-max monotonic ranked joins for evaluating top-k twig queries on weighted data graphs},
	isbn = {978-1-59593-649-3},
	url = {http://portal.acm.org/citation.cfm?id=1325851.1325910&coll=Portal&dl=GUIDE&CFID=43551959&CFTOKEN=51943809},
	abstract = {In many applications, the underlying data (the web, an {XML} document, or a relational database) can be seen as a graph. These graphs may be enriched with weights, associated with the nodes and edges of the graph, denoting application specific desirability/penalty assessments, such as popularity, trust, or cost. A particular challenge when considering such weights in query processing is that results need to be ranked accordingly. Answering keyword-based queries on weighted graphs is shown to be computationally expensive. In this paper, we first show that answering queries with further structure imposed on them remains {NP-hard.} We next show that, while the query evaluation task can be viewed in terms of ranked structural-joins along query axes, the monotonicity property, necessary for ranked join algorithms, is violated. Consequently, traditional ranked join algorithms are not directly applicable. Thus, we establish an alternative, sum-max monotonicity property and show how to leverage this for developing a self-punctuating, horizon-based ranked join {(HR-Join)} operator for ranked twig-query execution on data graphs. We experimentally show the effectiveness of the proposed evaluation schemes and the {HR-join} operator for merging ranked sub-results under sum-max monotonicity.},
	booktitle = {Proceedings of the 33rd international conference on Very large data bases},
	publisher = {{VLDB} Endowment},
	author = {Yan Qi and K. Selçuk Candan and Maria Luisa Sapino},
	year = {2007},
	pages = {507--518}
},

@inproceedings{harth_data_2010,
	address = {Raleigh, North Carolina, {USA}},
	title = {Data summaries for on-demand queries over linked data},
	isbn = {978-1-60558-799-8},
	url = {http://portal.acm.org/citation.cfm?id=1772733},
	doi = {10.1145/1772690.1772733},
	abstract = {Typical approaches for querying structured Web Data collect (crawl) and pre-process (index) large amounts of data in a central data repository before allowing for query answering. However, this time-consuming pre-processing phase however leverages the benefits of Linked Data -- where structured data is accessible live and up-to-date at distributed Web resources that may change constantly -- only to a limited degree, as query results can never be current. An ideal query answering system for Linked Data should return current answers in a reasonable amount of time, even on corpora as large as the Web. Query processors evaluating queries directly on the live sources require knowledge of the contents of data sources. In this paper, we develop and evaluate an approximate index structure summarising graph-structured content of sources adhering to Linked Data principles, provide an algorithm for answering conjunctive queries over Linked Data on {theWeb} exploiting the source summary, and evaluate the system using synthetically generated queries. The experimental results show that our lightweight index structure enables complete and up-to-date query results over Linked Data, while keeping the overhead for querying low and providing a satisfying source ranking at no additional cost.},
	booktitle = {Proceedings of the 19th international conference on World wide web},
	publisher = {{ACM}},
	author = {Andreas Harth and Katja Hose and Marcel Karnstedt and Axel Polleres and {Kai-Uwe} Sattler and Jürgen Umbrich},
	year = {2010},
	keywords = {index structures, linked data, {RDF} querying},
	pages = {411--420}
},

@article{jeffrey_evaluating_2003,
	title = {Evaluating Window Joins over Unbounded Streams},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.1391},
	journal = {{IN} {ICDE}},
	author = {Jaewoo Kang Jeffrey and Jeffrey F Naughton and Stratis D Viglas},
	year = {2003},
	pages = {341---352}
},

@misc{klyne_resource_2004,
	title = {Resource Description Framework {(RDF):}
Concepts and Abstract Syntax},
	url = {http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/},
	publisher = {World Wide Web Consortium},
	author = {Graham Klyne and Jeremey J. Carroll and Brian {McBride}},
	year = {2004}
},

@article{liu_reasoning_2008,
	title = {Reasoning and identifying relevant matches for {XML} keyword search},
	volume = {1},
	url = {http://portal.acm.org/citation.cfm?id=1453856.1453956&coll=GUIDE&dl=GUIDE&CFID=75531709&CFTOKEN=65038685},
	doi = {10.1145/1453856.1453956},
	abstract = {Keyword search is a user-friendly mechanism for retrieving {XML} data in web and scientific applications. An intuitively compelling but vaguely defined goal is to identify matches to query keywords that are relevant to the user. However, it is hard to directly evaluate the relevance of query results due to the inherent ambiguity of search semantics. In this work, we investigate an axiomatic framework that includes two intuitive and non-trivial properties that an {XML} keyword search technique should ideally satisfy: monotonicity and consistency, with respect to data and query. This is the first work that reasons about keyword search strategies from a formal perspective.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {Ziyang Liu and Yi Cher},
	year = {2008},
	pages = {921--932}
},

@article{byers_simple_2003,
	title = {Simple load balancing for distributed hash tables},
	journal = {{Peer-to-Peer} Systems {II}},
	author = {J. Byers and J. Considine {\textasciicircum}⋆ and M. Mitzenmacher},
	year = {2003},
	pages = {80–87}
},

@inproceedings{goldberg_computing_2005-1,
	title = {Computing {Point-to-Point} Shortest Paths from External Memory},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/alenex/GoldbergW05},
	author = {Andrew V. Goldberg and Renato Fonseca F. Werneck},
	year = {2005},
	pages = {26--40}
},

@misc{gang_gou_efficient_2008,
	title = {Efficient Algorithms for Querying {Large-Scale} Data in Relational, {XML,} and {Graph-Structured} Data Repositories},
	url = {http://www.lib.ncsu.edu/theses/available/etd-08132008-121025/},
	author = {Gang Gou},
	month = aug,
	year = {2008},
	keywords = {Databases, Views, {SQL,} Stream, Top-k, Algorithm, {XML,} Graph, Dissertation, {PhD}},
	howpublished = {http://www.lib.ncsu.edu/theses/available/etd-08132008-121025/}
},

@article{kementsietsidis_scalable_2008,
	title = {Scalable multi-query optimization for exploratory queries over federated scientific databases},
	volume = {1},
	issn = {2150-8097},
	url = {http://dx.doi.org/10.1145/1453856.1453864},
	doi = {http://dx.doi.org/10.1145/1453856.1453864},
	abstract = {The diversity and large volumes of data processed in the Natural Sciences today has led to a proliferation of highly-specialized and autonomous scientific databases with inherent and often intricate relationships. As a user-friendly method for querying this complex, ever-expanding network of sources for correlations, we propose exploratory queries. Exploratory queries are loosely-structured, hence requiring only minimal user knowledge of the source network. Evaluating an exploratory query usually involves the evaluation of many distributed queries. As the number of such distributed queries can quickly become large, we attack the optimization problem for exploratory queries by proposing several multi-query optimization algorithms that compute a global evaluation plan while minimizing the total communication cost, a key bottleneck in distributed settings. The proposed algorithms are necessarily heuristics, as computing an optimal global evaluation plan is shown to be {NP-hard.} Finally, we present an implementation of our algorithms, along with experiments that illustrate their potential not only for the optimization of exploratory queries, but also for the multiquery optimization of large batches of standard queries.},
	journal = {Proceedings of the {VLDB} Endowment},
	author = {Anastasios Kementsietsidis and Frank Neven and Dieter Van de Craen and Stijn Vansummeren},
	month = aug,
	year = {2008},
	note = {{ACM} {ID:} 1453864},
	keywords = {algorithms, design, distributed databases, distributed systems, global optimization, heuristic methods, management, performance, performance evaluation, query formulation, query processing},
	pages = {16–27}
},

@inproceedings{moerkotte_analysis_2006,
	address = {Seoul, Korea},
	title = {Analysis of two existing and one new dynamic programming algorithm for the generation of optimal bushy join trees without cross products},
	url = {http://portal.acm.org/citation.cfm?id=1164207},
	abstract = {Two approaches to derive dynamic programming algorithms for constructing join trees are described in the literature. We show analytically and experimentally that these two variants exhibit vastly diverging runtime behaviors for different query graphs. More specifically, each variant is superior to the other for one kind of query graph (chain or clique), but fails for the other. Moreover, neither of them handles star queries well. This motivates us to derive an algorithm that is superior to the two existing algorithms because it adapts to the search space implied by the query graph.},
	booktitle = {Proceedings of the 32nd international conference on Very large data bases},
	publisher = {{VLDB} Endowment},
	author = {Guido Moerkotte and Thomas Neumann},
	year = {2006},
	pages = {930--941}
},

@inproceedings{wang_query_2008,
	title = {Query planning for searching inter-dependent deep-web databases},
	booktitle = {Scientific and Statistical Database Management},
	author = {F. Wang and G. Agrawal and R. Jin},
	year = {2008},
	pages = {24–41}
},

@inproceedings{mokbel_hash-merge_2004,
	title = {Hash-merge join: a non-blocking join algorithm for producing fast and early join results},
	isbn = {1063-6382},
	shorttitle = {Hash-merge join},
	doi = {10.1109/ICDE.2004.1320002},
	abstract = {We introduce the hash-merge join algorithm {(HMJ,} for short); a new nonblocking join algorithm that deals with data items from remote sources via unpredictable, slow, or bursty network traffic. The {HMJ} algorithm is designed with two goals in mind: (1) minimize the time to produce the first few results, and (2) produce join results even if the two sources of the join operator occasionally get blocked. The {HMJ} algorithm has two phases: The hashing phase and the merging phase. The hashing phase employs an in-memory hash-based join algorithm that produces join results as quickly as data arrives. The merging phase is responsible for producing join results if the two sources are blocked. Both phases of the {HMJ} algorithm are connected via a flushing policy that flushes in-memory parts into disk storage once the memory is exhausted. Experimental results show that {HMJ} combines the advantages of two state-of-the-art nonblocking join algorithms {(XJoin} and Progressive Merge Join) while avoiding their shortcomings.},
	booktitle = {Data Engineering, 2004. Proceedings. 20th International Conference on},
	author = {{M.F.} Mokbel and M. Lu and {W.G.} Aref},
	year = {2004},
	keywords = {algorithm correctness proving, disk storage, file organisation, hashing phase, in-memory hash-based join algorithm, join operator, merging, merging phase, network traffic, nonblocking join algorithm, program verification, Progressive Merge Join, query processing, {XJoin}},
	pages = {251--262}
},

@article{bleiholder_query_2006,
	title = {Query planning in the presence of overlapping sources},
	journal = {Advances in Database {Technology-EDBT} 2006},
	author = {J. Bleiholder and S. Khuller and F. Naumann and L. Raschid and Y. Wu},
	year = {2006},
	pages = {811–828}
},

@inproceedings{tran_top-k_2009,
	title = {Top-k Exploration of Query Candidates for Efficient Keyword Search on {Graph-Shaped} {(RDF)} Data},
	isbn = {1084-4627},
	abstract = {Keyword queries enjoy widespread usage as they represent an intuitive way of specifying information needs. Recently, answering keyword queries on graph-structured data has emerged as an important research topic. The prevalent approaches build on dedicated indexing techniques as well as search algorithms aiming at finding substructures that connect the data elements matching the keywords. In this paper, we introduce a novel keyword search paradigm for graph-structured data, focusing in particular on the {RDF} data model. Instead of computing answers directly as in previous approaches, we first compute queries from the keywords, allowing the user to choose the appropriate query, and finally, process the query using the underlying database engine. Thereby, the full range of database optimization techniques can be leveraged for query processing. For the computation of queries, we propose a novel algorithm for the exploration of top-k matching subgraphs. While related techniques search the best answer trees, our algorithm is guaranteed to compute all k subgraphs with lowest costs, including cyclic graphs. By performing exploration only on a summary data structure derived from the data graph, we achieve promising performance improvements compared to other approaches.},
	booktitle = {Data Engineering, 2009. {ICDE} '09. {IEEE} 25th International Conference on},
	author = {T. Tran and Haofen Wang and S. Rudolph and P. Cimiano},
	year = {2009},
	keywords = {cyclic graphs, data graph, data handling, database optimization techniques, dedicated indexing techniques, graph theory, graph-shaped data, keyword queries, keyword search, optimisation, query processing, rdf, {RDF} data model, search algorithms, top-k, top-k matching subgraphs},
	pages = {405--416}
},

@article{rajasekaran_parallel_1998,
	title = {Parallel algorithms for relational coarsest partition problems},
	volume = {9},
	number = {7},
	journal = {{IEEE} Transactions on Parallel and Distributed Systems},
	author = {S. Rajasekaran and I. Lee},
	year = {1998},
	keywords = {bisimulation, parallel},
	pages = {687--699}
},

@inproceedings{kossmann_shooting_2002,
	series = {{VLDB} '02},
	title = {Shooting stars in the sky: an online algorithm for skyline queries},
	location = {Hong Kong, China},
	url = {http://portal.acm.org/citation.cfm?id=1287369.1287394},
	booktitle = {Proceedings of the 28th international conference on Very Large Data Bases},
	publisher = {{VLDB} Endowment},
	author = {Donald Kossmann and Frank Ramsak and Steffen Rost},
	year = {2002},
	pages = {275–286}
},

@inproceedings{tadepalli_incrementally_2009,
	address = {Clemson, South Carolina},
	title = {Incrementally distributed B+ trees: approaches and challenges},
	isbn = {978-1-60558-421-8},
	shorttitle = {Incrementally distributed B+ trees},
	url = {http://portal.acm.org/citation.cfm?id=1566445.1566451&coll=GUIDE&dl=GUIDE&CFID=48492463&CFTOKEN=96497791},
	doi = {10.1145/1566445.1566451},
	abstract = {B+ trees have proven efficient and effective in the role of indexes for data stored in databases. With the explosion in the number of datasets being stored in a distributed manner, a scalable and efficient index is needed to locate data. In this paper, the issues in designing a distributed B+ tree are examined with a specific emphasis on incrementally distributing the tree across a network.},
	booktitle = {Proceedings of the 47th Annual Southeast Regional Conference},
	publisher = {{ACM}},
	author = {Pallavi Tadepalli and H. Conrad Cunningham},
	year = {2009},
	keywords = {distributed b+ tree, grid nodes},
	pages = {1--6}
},

@inproceedings{lawrence_early_2005-1,
	title = {Early hash join: a configurable algorithm for the efficient and early production of join results},
	isbn = {1595931546},
	booktitle = {Proceedings of the 31st international conference on Very large data bases},
	author = {R. Lawrence},
	year = {2005},
	pages = {841–852}
},

@inproceedings{deshpande_lifting_2004,
	address = {Toronto, Canada},
	title = {Lifting the burden of history from adaptive query processing},
	isbn = {0-12-088469-0},
	url = {http://portal.acm.org/citation.cfm?id=1316689.1316771&coll=GUIDE&dl=GUIDE&CFID=89791710&CFTOKEN=37673622},
	abstract = {Adaptive query processing schemes attempt to re-optimize query plans during the course of query execution. A variety of techniques for adaptive query processing have been proposed, varying in the granularity at which they can make decisions [8]. The eddy [1] is the most aggressive of these techniques, with the flexibility to choose tuple-by-tuple how to order the application of operators. In this paper we identify and address a fundamental limitation of the original eddies proposal: the burden of history in routing. We observe that routing decisions have long-term effects on the state of operators in the query, and can severely constrain the ability of the eddy to adapt over time. We then propose a mechanism we call {STAIRs} that allows the query engine to manipulate the state stored inside the operators and undo the effects of past routing decisions. We demonstrate that eddies with {STAIRs} achieve both high adaptivity and good performance in the face of uncertainty, outperforming prior eddy proposals by orders of magnitude.},
	booktitle = {Proceedings of the Thirtieth international conference on Very large data bases - Volume 30},
	publisher = {{VLDB} Endowment},
	author = {Amol Deshpande and Joseph M. Hellerstein},
	year = {2004},
	pages = {948--959}
},

@inproceedings{siegenthaler_sharing_2009,
	title = {Sharing Private Information Across Distributed Databases},
	url = {10.1109/NCA.2009.33},
	doi = {10.1109/NCA.2009.33},
	abstract = {In industries such as healthcare, there is a need to electronically share privacy-sensitive data across distinct organizations. We show how this can be done while allowing organizations to keep their legacy databases and maintain ownership of the data that they currently store. Without sending or mirroring data to any trusted, centralized entity, we demonstrate how queries can be answered in a distributed manner that preserves the privacy of the original data. This paper explains our distributed query execution engine, outlines how to bootstrap the system when only real world identifiers such as a name and date-of-birth are initially known, and offers details on the tradeoff between privacy and performance. We evaluate the scalability of this approach through simulation.},
	booktitle = {Network Computing and Applications, 2009. {NCA} 2009. Eighth {IEEE} International Symposium on},
	author = {M. Siegenthaler and K. Birman},
	year = {2009},
	keywords = {bootstrap, computer bootstrapping, data privacy, distributed databases, distributed query execution engine, legacy databases, private information sharing, query processing, software maintenance},
	pages = {82--89}
},

@article{haas_selectivity_1996,
	title = {Selectivity and Cost Estimation for Joins Based on Random Sampling},
	volume = {52},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/B6WJ0-45PV45V-F/2/a6156ff8a9aa912d005b5f959793134f},
	doi = {10.1006/jcss.1996.0041},
	abstract = {We compare the performance of sampling-based procedures for estimating the selectivity of a join. While some of the procedures have been proposed in the database literature, their relative performance has never been analyzed. A main result of this paper is a partial ordering that compares the variability of the estimators for the different procedures after an arbitrary fixed number of sampling steps. Prior to the current work, it was also unknown whether these fixed-step procedures could be extended to fixed-precision procedures that are both asymptotically consistent and asymptotically efficient. Our second main result is a general method for such an extension and a proof that the method is valid for all the procedures under consideration. We show that, under plausible assumptions on sampling costs, the partial ordering of the fixed-step procedures with respect to variability of the selectivity estimator implies a partial ordering of the corresponding fixed-precision procedures with respect to sampling cost. Our final result is a collection of fixed-step and fixed-precision procedures for estimating the cost of processing a join query according to a fixed join plan.},
	number = {3},
	journal = {Journal of Computer and System Sciences},
	author = {Peter J. Haas and Jeffrey F. Naughton and S. Seshadri and Arun N. Swami},
	month = jun,
	year = {1996},
	pages = {550--569}
},

@inproceedings{harth_optimized_2005,
	title = {Optimized index structures for querying {RDF} from the Web},
	doi = {10.1109/LAWEB.2005.25},
	abstract = {Storing and querying resource description framework {(RDF)} data is one of the basic tasks within any semantic Web application. A number of storage systems provide assistance for this task. However, current {RDF} database systems do not use optimized indexes, which results in a poor performance behavior for querying {RDF.} In this paper we describe optimized index structures for {RDF,} show how to process and evaluate queries based on the index structure, describe a lightweight adaptable implementation in Java, and provide a performance comparison with existing {RDF} databases.},
	booktitle = {Web Congress, 2005. {LA-WEB} 2005. Third Latin American},
	author = {A. Harth and S. Decker},
	year = {2005},
	keywords = {data models, database indexing, Java, optimized index structures, query processing, {RDF} database system, {RDF} querying, resource description framework, semantic Web, semantic Web application},
	pages = {10 pp.}
},

@inproceedings{golenberg_keyword_2008,
	address = {Vancouver, Canada},
	title = {Keyword proximity search in complex data graphs},
	isbn = {978-1-60558-102-6},
	url = {http://portal.acm.org/citation.cfm?id=1376616.1376708&coll=GUIDE&dl=GUIDE&CFID=75531709&CFTOKEN=65038685},
	doi = {10.1145/1376616.1376708},
	abstract = {In keyword search over data graphs, an answer is a nonredundant subtree that includes the given keywords. An algorithm for enumerating answers is presented within an architecture that has two main components: an engine that generates a set of candidate answers and a ranker that evaluates their score. To be effective, the engine must have three fundamental properties. It should not miss relevant answers, has to be efficient and must generate the answers in an order that is highly correlated with the desired ranking. It is shown that none of the existing systems has implemented an engine that has all of these properties. In contrast, this paper presents an engine that generates all the answers with provable guarantees. Experiments show that the engine performs well in practice. It is also shown how to adapt this engine to queries under the {OR} semantics. In addition, this paper presents a novel approach for implementing rankers destined for eliminating redundancy. Essentially, an answer is ranked according to its individual properties (relevancy) and its intersection with the answers that have already been presented to the user. Within this approach, experiments with specific rankers are described.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Konstantin Golenberg and Benny Kimelfeld and Yehoshua Sagiv},
	year = {2008},
	keywords = {approximate top-k answers, information retrieval on graphs, keyword proximity search, redundancy elimination, subtree enumeration by height},
	pages = {927--940}
},

@inproceedings{li_leveraging_2009,
	address = {Hong Kong, China},
	title = {Leveraging a scalable row store to build a distributed text index},
	isbn = {978-1-60558-802-5},
	url = {http://portal.acm.org/citation.cfm?id=1651263.1651270&coll=GUIDE&dl=GUIDE&type=series&idx=SERIES772&part=series&WantType=Proceedings&title=CIKM&CFID=82166658&CFTOKEN=88298104},
	doi = {10.1145/1651263.1651270},
	abstract = {Many content-oriented applications require a scalable text index. Building such an index is challenging. In addition to the logic of inserting and searching documents, developers have to worry about issues in a typical distributed environment, such as fault tolerance, incrementally growing the index cluster, and load balancing. We developed a distributed text index called {HIndex,} by judiciously exploiting the control layer of {HBase,} which is an open source implementation of Google's Bigtable. Such leverage enables us to inherit the support on availability, elasticity and load balancing in {HBase.} We present the design, implementation, and a performance evaluation of {HIndex} in this paper.},
	booktitle = {Proceeding of the first international workshop on Cloud data management},
	publisher = {{ACM}},
	author = {Ning Li and Jun Rao and Eugene Shekita and Sandeep Tata},
	year = {2009},
	keywords = {incremental bigtable hbase},
	pages = {29--36}
},

@article{madden_tinydb:_2005,
	title = {{TinyDB:} an acquisitional query processing system for sensor networks},
	volume = {30},
	shorttitle = {{TinyDB}},
	url = {http://portal.acm.org/citation.cfm?id=1061318.1061322},
	doi = {10.1145/1061318.1061322},
	abstract = {We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to {SQL} for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of {TinyDB,} a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices.},
	number = {1},
	journal = {{ACM} Trans. Database Syst.},
	author = {Samuel R. Madden and Michael J. Franklin and Joseph M. Hellerstein and Wei Hong},
	year = {2005},
	keywords = {data acquisition, query processing, sensor networks},
	pages = {122--173}
},

@article{jagadish_compression_1990,
	title = {A compression technique to materialize transitive closure},
	volume = {15},
	url = {http://portal.acm.org/citation.cfm?doid=99935.99944},
	doi = {10.1145/99935.99944},
	abstract = {An important feature of database support for expert systems is the ability of the database to answer queries regarding the existence of a path from one node to another in the directed graph underlying some database relation. Given just the database relation, answering such a query is time-consuming, but given the transitive closure of the database relation a table look-up suffices. We present an indexing scheme that permits the storage of the pre-computed transitive closure of a database relation in a compressed form. The existence of a specified tuple in the closure can be determined from this compressed store by a single look-up followed by an index comparision. We show how to add nodes and arcs to the compressed closure incrementally. We also suggest how this compression technique can be used to reduce the effort required to compute the transitive closure.},
	number = {4},
	journal = {{ACM} Trans. Database Syst.},
	author = {H. V. Jagadish},
	year = {1990},
	pages = {558--598}
},

@article{fagin_optimal_2003,
	title = {Optimal aggregation algorithms for middleware},
	volume = {66},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=journals/jcss/FaginLN03},
	number = {4},
	journal = {J. Comput. Syst. Sci.},
	author = {Ronald Fagin and Amnon Lotem and Moni Naor},
	year = {2003},
	pages = {614--656}
},

@inproceedings{yu_effective_2007-1,
	address = {Beijing, China},
	title = {Effective keyword-based selection of relational databases},
	isbn = {978-1-59593-686-8},
	url = {http://portal.acm.org/citation.cfm?id=1247480.1247498},
	doi = {10.1145/1247480.1247498},
	abstract = {The wide popularity of free-and-easy keyword based searches over World Wide Web has fueled the demand for incorporating keyword-based search over structured databases. However, most of the current research work focuses on keyword-based searching over a single structured data source. With the growing interest in distributed databases and service oriented architecture over the Internet, it is important to extend such a capability over multiple structured data sources. One of the most important problems for enabling such a query facility is to be able to select the most useful data sources relevant to the keyword query. Traditional database summary techniques used for selecting unstructured datasources developed in {IR} literature are inadequate for our problem, as they do not capture the structure of the data sources. In this paper, we study the database selection problem for relational data sources, and propose a method that effectively summarizes the relationships between keywords in a relational database based on its structure. We develop effective ranking methods based on the keyword relationship summaries in order to select the most useful databases for a given keyword query. We have implemented our system on {PlanetLab.} In that environment we use extensive experiments with real datasets to demonstrate the effectiveness of our proposed summarization method.},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Bei Yu and Guoliang Li and Karen Sollins and Anthony K. H. Tung},
	year = {2007},
	keywords = {database selection, keyword query, summarization},
	pages = {139--150}
},

@inproceedings{bornea_double_2009,
	title = {Double Index {NEsted-Loop} Reactive Join for Result Rate Optimization},
	isbn = {1084-4627},
	doi = {10.1109/ICDE.2009.101},
	abstract = {Adaptive join algorithms have recently attracted a lot of attention in emerging applications where data is provided by autonomous data sources through heterogeneous network environments. Their main advantage over traditional join techniques is that they can start producing join results as soon as the first input tuples are available, thus improving pipelining by smoothing join result production and by masking source or network delays. In this paper we propose double index nested loops reactive join {(DINER),} a new adaptive join algorithm for result rate maximization. {DINER} combines two key elements: an intuitive flushing policy that aims to increase the productivity of in-memory tuples in producing results during the online phase of the join, and a novel re-entrant join technique that allows the algorithm to rapidly switch between processing in-memory and disk-resident tuples, thus better exploiting temporary delays when new data is not available. Our experiments using real and synthetic data sets demonstrate that {DINER} outperforms previous adaptive join algorithms in producing result tuples at a significantly higher rate, while making better use of the available memory.},
	booktitle = {Data Engineering, 2009. {ICDE} '09. {IEEE} 25th International Conference on},
	author = {{M.A.} Bornea and V. Vassalos and Y. Kotidis and A. Deligiannakis},
	year = {2009},
	keywords = {adaptive join algorithms, autonomous data sources, data handling, disk-resident tuples, double index nested loops reactive join, heterogeneous network, intuitive flushing policy, optimisation, query processing},
	pages = {481--492}
},

@article{deshpande_adaptive_2007,
	title = {Adaptive query processing},
	volume = {1},
	url = {http://portal.acm.org/citation.cfm?id=1331940},
	abstract = {As the data management field has diversified to consider settings in which queries are increasingly complex, statistics are less available, or data is stored remotely, there has been an acknowledgment that the traditional optimize-then-execute paradigm is insufficient. This has led to a plethora of new techniques, generally placed under the common banner of adaptive query processing, that focus on using runtime feed-back to modify query processing in a way that provides better response time or more efficient {CPU} utilization.},
	number = {1},
	journal = {Found. Trends databases},
	author = {Amol Deshpande and Zachary Ives and Vijayshankar Raman},
	year = {2007},
	pages = {1--140}
},

@incollection{zhu_uniform_2009,
	title = {A Uniform Framework for {Ad-Hoc} Indexes to Answer Reachability Queries on Large Graphs},
	url = {http://dx.doi.org/10.1007/978-3-642-00887-0_12},
	abstract = {Graph-structured databases and related problems such as reachability query processing have been increasingly relevant to many
applications such as {XML} databases, biological databases, social network analysis and the Semantic Web. To efficiently evaluate
reachability queries on large graph-structured databases, there has been a host of recent research on graph indexing. To date,
reachability indexes are generally applied to the entire graph. This can often be suboptimal if the graph is large or/and
its subgraphs are diverse in structure. In this paper, we propose a uniform framework to support existing reachability indexing
for subgraphs of a given graph. This in turn supports fast reachability query processing in large graph-structured databases.
The contributions of our uniform framework are as follows: (1) We formally define a graph framework that facilitates indexing
subgraphs, as opposed to the entire graph. (2) We propose a heuristic algorithm to partition a given graph into subgraphs
for indexing. (3) We demonstrate how reachability queries are evaluated in the graph framework. Our preliminary experimental
results showed that the framework yields a smaller total index size and is more efficient in processing reachability queries
on large graphs than a fixed index scheme on the entire graphs.},
	booktitle = {Database Systems for Advanced Applications},
	author = {Linhong Zhu and Byron Choi and Bingsheng He and Jeffrey Yu and Wee Ng},
	year = {2009},
	pages = {138--152}
},

@inproceedings{tata_sqak:_2008,
	address = {Vancouver, Canada},
	title = {{SQAK:} doing more with keywords},
	isbn = {978-1-60558-102-6},
	shorttitle = {{SQAK}},
	url = {http://portal.acm.org/citation.cfm?id=1376616.1376705&coll=GUIDE&dl=GUIDE&CFID=75531709&CFTOKEN=65038685},
	doi = {10.1145/1376616.1376705},
	abstract = {Today's enterprise databases are large and complex, often relating hundreds of entities. Enabling ordinary users to query such databases and derive value from them has been of great interest in database research. Today, keyword search over relational databases allows users to find pieces of information without having to write complicated {SQL} queries. However, in order to compute even simple aggregates, a user is required to write a {SQL} statement and can no longer use simple keywords. This not only requires the ordinary user to learn {SQL,} but also to learn the schema of the complex database in detail in order to correctly construct the required query. This greatly limits the options of the user who wishes to examine a database in more depth.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Sandeep Tata and Guy M. Lohman},
	year = {2008},
	keywords = {aggregates, keyword queries, query tools, relational database, sql},
	pages = {889--902}
},

@inproceedings{battr_query_2008,
	address = {Los Alamitos, {CA,} {USA}},
	title = {Query Planning in {DHT} Based {RDF} Stores},
	volume = {0},
	isbn = {978-0-7695-3493-0},
	doi = {http://doi.ieeecomputersociety.org/10.1109/SITIS.2008.15},
	abstract = {Implementing scalable {RDF} triple stores that can store many triples and process many queries concurrently is challenging. Several projects have investigated the use of distributed hash tables for this task but query planning has received little attention in this context so far. Given the distributed nature of {DHTs,} latencies of messages and limited network bandwidth are crucial factors to consider. Also due to a lack of global knowledge in {DHTs,} query planning is different from centralized databases. This paper discusses a set of heuristics and evaluates their performance on the Lehigh University Benchmark with emphasis on the network traffic. The results show the importance of query planning in {DHT} based {RDF} triple stores.},
	booktitle = {{Signal-Image} Technologies and {Internet-Based} System, International {IEEE} Conference on},
	publisher = {{IEEE} Computer Society},
	author = {Dominic Battr},
	year = {2008},
	keywords = {babelpeers, dht, query evaluation, query planning, rdf triple store},
	pages = {187--194}
},

@inproceedings{lipton_practical_1990,
	address = {Atlantic City, New Jersey, United States},
	title = {Practical selectivity estimation through adaptive sampling},
	isbn = {0-89791-365-5},
	url = {http://portal.acm.org/citation.cfm?id=93597.93611},
	doi = {10.1145/93597.93611},
	abstract = {Recently we have proposed an adaptive, random sampling algorithm for general query size estimation. In earlier work we analyzed the asymptotic efficiency and accuracy of the algorithm, in this paper we investigate its practicality as applied to selects and joins. First, we extend our previous analysis to provide significantly improved bounds on the amount of sampling necessary for a given level of accuracy. Next, we provide “sanity bounds” to deal with queries for which the underlying data is extremely skewed or the query result is very small. Finally, we report on the performance of the estimation algorithm as implemented in a host language on a commercial relational system. The results are encouraging, even with this loose coupling between the estimation algorithm and the {DBMS.}},
	booktitle = {Proceedings of the 1990 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Richard J. Lipton and Jeffrey F. Naughton and Donovan A. Schneider},
	year = {1990},
	pages = {1--11}
},

@inproceedings{wei_tedi:_2010,
	address = {New York, {NY,} {USA}},
	series = {{SIGMOD} '10},
	title = {{TEDI:} efficient shortest path query answering on graphs},
	isbn = {978-1-4503-0032-2},
	location = {Indianapolis, Indiana, {USA}},
	shorttitle = {{TEDI}},
	doi = {10.1145/1807167.1807181},
	abstract = {Efficient shortest path query answering in large graphs is enjoying a growing number of applications, such as ranked keyword search in databases, social networks, ontology reasoning and bioinformatics. A shortest path query on a graph finds the shortest path for the given source and target vertices in the graph. Current techniques for efficient evaluation of such queries are based on the pre-computation of compressed Breadth First Search trees of the graph. However, they suffer from drawbacks of scalability. To address these problems, we propose {TEDI,} an indexing and query processing scheme for the shortest path query answering. {TEDI} is based on the tree decomposition methodology. The graph is first decomposed into a tree in which the node (a.k.a. bag) contains more than one vertex from the graph. The shortest paths are stored in such bags and these local paths together with the tree are the components of the index of the graph. Based on this index, a bottom-up operation can be executed to find the shortest path for any given source and target vertices. Our experimental results show that {TEDI} offers orders-of-magnitude performance improvement over existing approaches on the index construction time, the index size and the query answering.},
	booktitle = {Proceedings of the 2010 international conference on Management of data},
	publisher = {{ACM}},
	author = {Fang Wei},
	year = {2010},
	note = {{ACM} {ID:} 1807181},
	keywords = {algorithms, design, graphs, indexing, indexing methods, search process, shortest path, to-read, tree decomposition},
	pages = {99–110}
},

@article{babu_exploiting_2004,
	title = {Exploiting {\textless}i{\textgreater}k{\textless}/i{\textgreater}-constraints to reduce memory overhead in continuous queries over data streams},
	volume = {29},
	url = {http://portal.acm.org/citation.cfm?id=1016032},
	doi = {10.1145/1016028.1016032},
	abstract = {Continuous queries often require significant run-time state over arbitrary data streams. However, streams may exhibit certain data or arrival patterns, or constraints, that can be detected and exploited to reduce state considerably without compromising correctness. Rather than requiring constraints to be satisfied precisely, which can be unrealistic in a data streams environment, we introduce k-constraints, where k is an adherence parameter specifying how closely a stream adheres to the constraint. {(Smaller} k's are closer to strict adherence and offer better memory reduction.) We present a query processing architecture, called {k-Mon,} that detects useful k-constraints automatically and exploits the constraints to reduce run-time state for a wide range of continuous queries. Experimental results showed dramatic state reduction, while only modest computational overhead was incurred for our constraint monitoring and query execution algorithms.},
	number = {3},
	journal = {{ACM} Trans. Database Syst.},
	author = {Shivnath Babu and Utkarsh Srivastava and Jennifer Widom},
	year = {2004},
	keywords = {constraints, continuous queries, data streams},
	pages = {545--580}
},

@inproceedings{gou_efficient_2008,
	address = {Vancouver, Canada},
	title = {Efficient algorithms for exact ranked twig-pattern matching over graphs},
	isbn = {978-1-60558-102-6},
	url = {http://portal.acm.org/citation.cfm?id=1376616.1376676},
	doi = {10.1145/1376616.1376676},
	abstract = {Querying large-scale graph-structured data with twig patterns is attracting growing interest. Generally, a twig pattern could have an extremely large, potentially exponential, number of matches in a graph. Retrieving and returning to the user this many answers may both incur high computational overhead and overwhelm the user.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Gang Gou and Rada Chirkova},
	year = {2008},
	keywords = {graph, top-k, twig pattern matching, xml},
	pages = {581--594}
},

@inproceedings{jiefeng_cheng_querying_2007,
	title = {Querying {Graph-Structured} Data},
	doi = {10.1109/NPC.2007.166},
	abstract = {Graphs have great expressive power to describe the complex relationships among data objects, and there are large graph datasets available such as Web data, semi-structured data and {XML} data. In this paper, we describe our work on querying graph-structured data, including graph labeling methods, reachability joins, and graph pattern matching. We show that we can base on the graph labeling of complex {XML} and semi-structured data to process path queries and we devise join primitives for matching graph patterns. Novel aspects about using such join primitives for graph pattern matching are addressed.},
	booktitle = {Network and Parallel Computing Workshops, 2007. {NPC} Workshops. {IFIP} International Conference on},
	author = {Jiefeng Cheng and {J.X.} Yu},
	year = {2007},
	keywords = {data object relationship, data structures, graph labeling method, graph pattern matching, graph reachability join, graph theory, graph-structured data querying, pattern matching, query processing, xml, {XML} data},
	pages = {23--27}
},

@article{dalvi_keyword_2008,
	title = {Keyword search on external memory data graphs},
	volume = {1},
	url = {http://portal.acm.org/citation.cfm?id=1453856.1453982},
	doi = {10.1145/1453856.1453982},
	abstract = {Keyword search on graph structured data has attracted a lot of attention in recent years. Graphs are a natural "lowest common denominator" representation which can combine relational, {XML} and {HTML} data. Responses to keyword queries are usually modeled as trees that connect nodes matching the keywords.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {Bhavana Bharat Dalvi and Meghana Kshirsagar and S. Sudarshan},
	year = {2008},
	pages = {1189--1204}
},

@article{narayanan_delay_2007,
	title = {Delay aware querying with Seaweed},
	volume = {17},
	issn = {1066-8888},
	url = {http://www.springerlink.com/content/jj71186512p07122/},
	doi = {10.1007/s00778-007-0060-3},
	number = {2},
	journal = {The {VLDB} Journal},
	author = {Dushyanth Narayanan and Austin Donnelly and Richard Mortier and Antony Rowstron},
	year = {2007},
	pages = {315--331}
},

@inproceedings{ives_adapting_2004,
	address = {Paris, France},
	title = {Adapting to source properties in processing data integration queries},
	isbn = {1-58113-859-8},
	url = {http://portal.acm.org/citation.cfm?id=1007613},
	doi = {10.1145/1007568.1007613},
	abstract = {An effective query optimizer finds a query plan that exploits the characteristics of the source data. In data integration, little is known in advance about sources' properties, which necessitates the use of adaptive query processing techniques to adjust query processing on-the-fly. Prior work in adaptive query processing has focused on compensating for delays and adjusting for mis-estimated cardinality or selectivity values. In this paper, we present a generalized architecture for adaptive query processing and introduce a new technique, called adaptive data partitioning {(ADP),} which is based on the idea of dividing the source data into regions, each executed by different, complementary plans. We show how this model can be applied in novel ways to not only correct for underestimated selectivity and cardinality values, but also to discover and exploit order in the source data, and to detect and exploit source data that can be effectively pre-aggregated. We experimentally compare a number of alternative strategies and show that our approach is effective.},
	booktitle = {Proceedings of the 2004 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Zachary G. Ives and Alon Y. Halevy and Daniel S. Weld},
	year = {2004},
	pages = {395--406}
},

@inproceedings{zhu_dynamic_2004,
	address = {Paris, France},
	title = {Dynamic plan migration for continuous queries over data streams},
	isbn = {1-58113-859-8},
	url = {http://portal.acm.org/citation.cfm?id=1007617},
	doi = {10.1145/1007568.1007617},
	abstract = {Dynamic plan migration is concerned with the on-the-fly transition from one continuous query plan to a semantically equivalent yet more efficient plan. Migration is important for stream monitoring systems where long-running queries may have to withstand fluctuations in stream workloads and data characteristics. Existing migration methods generally adopt a pause-drain-resume strategy that pauses the processing of new data, purges all old data in the existing plan, until finally the new plan can be plugged into the system. However, these existing strategies do not address the problem of migrating query plans that contain stateful operators, such as joins. We now develop solutions for online plan migration for continuous stateful plans. In particular, in this paper, we propose two alternative strategies, called the moving state strategy and the parallel track strategy, one exploiting reusability and the second employs parallelism to seamlessly migrate between continuous join plans without affecting the results of the query. We develop cost models for both migration strategies to analytically compare them. We embed these migration strategies into the {CAPE} [7], a prototype system of a stream query engine, and conduct a comparative experimental study to evaluate these two strategies for window-based join plans. Our experimental results illustrate that the two strategies can vary significantly in terms of output rates and intermediate storage spaces given distinct system configurations and stream workloads.},
	booktitle = {Proceedings of the 2004 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Yali Zhu and Elke A. Rundensteiner and George T. Heineman},
	year = {2004},
	pages = {431--442}
},

@inproceedings{eickler_performance_1995,
	title = {A Performance Evaluation of {OID} Mapping Techniques},
	isbn = {1-55860-379-4},
	url = {http://portal.acm.org/citation.cfm?id=645921.673165},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {Proceedings of the 21th International Conference on Very Large Data Bases},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {André Eickler and Carsten Andreas Gerlhof and Donald Kossmann},
	year = {1995},
	pages = {18--29}
},

@inproceedings{babu_adaptive_2005,
	address = {Los Alamitos, {CA,} {USA}},
	title = {Adaptive Caching for Continuous Queries},
	volume = {0},
	doi = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2005.15},
	abstract = {We address the problem of executing continuous multiway join queries in unpredictable and volatile environments. Our query class captures windowed join queries in data stream systems as well as conventional maintenance of materialized join views. Our adaptive approach handles streams of updates whose rates and data characteristics may change over time, as well as changes in system conditions such as memory availability. In this paper we focus specifically on the problem of adaptive placement and removal of caches to optimize join performance. Our approach automatically considers conventional tree-shaped join plans with materialized subresults at every intermediate node, subresult-free {MJoins,} and the entire spectrum between them. We provide algorithms for selecting caches, monitoring their cost and benefits in current conditions, allocating memory to caches, and adapting as conditions change. All of our algorithms are implemented in the {STREAM} prototype Data Stream Management System and a thorough experimental evaluation is included.},
	booktitle = {Data Engineering, International Conference on},
	publisher = {{IEEE} Computer Society},
	author = {Shivnath Babu and Kamesh Munagala and Jennifer Widom and Rajeev Motwani},
	year = {2005},
	pages = {118--129}
},

@inproceedings{bizarro_content-based_2005,
	address = {Trondheim, Norway},
	title = {Content-based routing: different plans for different data},
	isbn = {1-59593-154-6},
	shorttitle = {Content-based routing},
	url = {http://portal.acm.org/citation.cfm?id=1083592.1083680},
	abstract = {Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing {(CBR)} that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented {CBR} as an extension to the Eddies query processor in the {TelegraphCQ} system, and we present an extensive experimental evaluation showing the significant performance benefits of {CBR.}},
	booktitle = {Proceedings of the 31st international conference on Very large data bases},
	publisher = {{VLDB} Endowment},
	author = {Pedro Bizarro and Shivnath Babu and David {DeWitt} and Jennifer Widom},
	year = {2005},
	pages = {757--768}
},

@article{ewen_progressive_2006,
	title = {Progressive query optimization for federated queries},
	journal = {Advances in Database {Technology-EDBT} 2006},
	author = {S. Ewen and H. Kache and V. Markl and V. Raman},
	year = {2006},
	pages = {847–864}
},

@article{pottinger_minicon:_2001,
	title = {{MiniCon:} A scalable algorithm for answering queries using views},
	volume = {10},
	issn = {1066-8888},
	url = {http://portal.acm.org/citation.cfm?id=767141.767146},
	journal = {The {VLDB} Journal},
	author = {Rachel Pottinger and Alon Halevy},
	month = sep,
	year = {2001},
	keywords = {Data integration, Materialized views, Query optimization, Web and databases},
	pages = {182–198}
},

@incollection{goldberg_better_2007,
	title = {Better Landmarks Within Reach},
	url = {http://dx.doi.org/10.1007/978-3-540-72845-0_4},
	abstract = {We present significant improvements to a practical algorithm for the point-to-point shortest path problem on road networks
that combines A* search, landmark-based lower bounds, and reach-based pruning. Through reach-aware landmarks, better use of cache, and improved
algorithms for reach computation, we make preprocessing and queries faster while reducing the overall space requirements.
On the road networks of the {USA} or Europe, the shortest path between two random vertices can be found in about one millisecond
after one or two hours of preprocessing. The algorithm is also effective on two-dimensional grids.},
	booktitle = {Experimental Algorithms},
	author = {Andrew Goldberg and Haim Kaplan and Renato Werneck},
	year = {2007},
	pages = {38--51}
},

@inproceedings{agrawal_dbxplorer:_2002,
	address = {Madison, Wisconsin},
	title = {{DBXplorer:} enabling keyword search over relational databases},
	isbn = {1-58113-497-5},
	shorttitle = {{DBXplorer}},
	url = {http://portal.acm.org/citation.cfm?id=564691.564782&coll=GUIDE&dl=GUIDE&CFID=76177120&CFTOKEN=69707907},
	doi = {10.1145/564691.564782},
	abstract = {Note: {OCR} errors may be found in this Reference List extracted from the full text article. {ACM} has opted to expose the complete List rather than only correct and linked references.},
	booktitle = {Proceedings of the 2002 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Sanjay Agrawal and Surajit Chaudhuri and Gautam Das},
	year = {2002},
	pages = {627--627}
},

@inproceedings{tian_efficient_2008,
	address = {Vancouver, Canada},
	title = {Efficient aggregation for graph summarization},
	isbn = {978-1-60558-102-6},
	url = {http://portal.acm.org/citation.cfm?id=1376675},
	doi = {10.1145/1376616.1376675},
	abstract = {Graphs are widely used to model real world objects and their relationships, and large graph datasets are common in many application domains. To understand the underlying characteristics of large graphs, graph summarization techniques are critical. However, existing graph summarization methods are mostly statistical (studying statistics such as degree distributions, hop-plots and clustering coefficients). These statistical methods are very useful, but the resolutions of the summaries are hard to control.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Yuanyuan Tian and Richard A. Hankins and Jignesh M. Patel},
	year = {2008},
	keywords = {aggregation, graphs, social networks, summarization},
	pages = {567--580}
},

@article{kim_sort_2009,
	title = {Sort vs. Hash revisited: fast join implementation on modern multi-core {CPUs}},
	volume = {2},
	shorttitle = {Sort vs. Hash revisited},
	url = {http://portal.acm.org/citation.cfm?id=1687553.1687564&coll=GUIDE&dl=GUIDE&idx=J1174&part=journal&WantType=Journals&title=Proceedings%20of%20the%20VLDB%20Endowment&CFID=82166857&CFTOKEN=58994927},
	abstract = {Join is an important database operation. As computer architectures evolve, the best join algorithm may change hand. This paper re-examines two popular join algorithms -- hash join and sort-merge join -- to determine if the latest computer architecture trends shift the tide that has favored hash join for many years. For a fair comparison, we implemented the most optimized parallel version of both algorithms on the latest Intel Core i7 platform. Both implementations scale well with the number of cores in the system and take advantages of latest processor features for performance. Our hash-based implementation achieves more than {100M} tuples per second which is {17X} faster than the best reported performance on {CPUs} and {8X} faster than that reported for {GPUs.} Moreover, the performance of our hash join implementation is consistent over a wide range of input data sizes from {64K} to {128M} tuples and is not affected by data skew. We compare this implementation to our highly optimized sort-based implementation that achieves {47M} to {80M} tuples per second. We developed analytical models to study how both algorithms would scale with upcoming processor architecture trends. Our analysis projects that current architectural trends of wider {SIMD,} more cores, and smaller memory bandwidth per core imply better scalability potential for sort-merge join. Consequently, sort-merge join is likely to outperform hash join on upcoming chip multiprocessors. In summary, we offer multicore implementations of hash join and sort-merge join which consistently outperform all previously reported results. We further conclude that the tide that favors the hash join algorithm has not changed yet, but the change is just around the corner.},
	number = {2},
	journal = {Proc. {VLDB} Endow.},
	author = {Changkyu Kim and Tim Kaldewey and Victor W. Lee and Eric Sedlar and Anthony D. Nguyen and Nadathur Satish and Jatin Chhugani and Andrea Di Blas and Pradeep Dubey},
	year = {2009},
	pages = {1378--1389}
},

@article{theobald_topx:_2008,
	title = {{TopX:} efficient and versatile top-k query processing for semistructured data},
	volume = {17},
	shorttitle = {{TopX}},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=journals/vldb/TheobaldBMSW08},
	number = {1},
	journal = {{VLDB} J.},
	author = {Martin Theobald and Holger Bast and Debapriyo Majumdar and Ralf Schenkel and Gerhard Weikum},
	year = {2008},
	pages = {81--115}
},

@inproceedings{cai_rdfpeers:_2004,
	address = {New York, {NY,} {USA}},
	title = {{RDFPeers:} a scalable distributed {RDF} repository based on a structured peer-to-peer network},
	isbn = {{1-58113-844-X}},
	shorttitle = {{RDFPeers}},
	url = {http://portal.acm.org/citation.cfm?id=988760},
	doi = {10.1145/988672.988760},
	abstract = {Centralized Resource Description Framework {(RDF)} repositories have limitations both in their failure tolerance and in their scalability. Existing {Peer-to-Peer} {(P2P)} {RDF} repositories either cannot guarantee to find query results, even if these results exist in the network, or require up-front definition of {RDF} schemas and designation of super peers. We present a scalable distributed {RDF} repository {(RDFPeers)} that stores each triple at three places in a multi-attribute addressable network by applying globally known hash functions to its subject predicate and object. Thus all nodes know which node is responsible for storing triple values they are looking for and both exact-match and range queries can be efficiently routed to those nodes. {RDFPeers} has no single point of failure nor elevated peers and does not require the prior definition of {RDF} schemas. Queries are guaranteed to find matched triples in the network if the triples exist. In {RDFPeers} both the number of neighbors per node and the number of routing hops for inserting {RDF} triples and for resolving most queries are logarithmic to the number of nodes in the network. We further performed experiments that show that the triple-storing load in {RDFPeers} differs by less than an order of magnitude between the most and the least loaded nodes for real-world {RDF} data.},
	booktitle = {Proceedings of the 13th international conference on World Wide Web},
	publisher = {{ACM}},
	author = {Min Cai and Martin Frank},
	year = {2004},
	keywords = {distributed rdf repositories, peer-to-peer, semantic Web},
	pages = {650--657}
},

@inproceedings{vu_graph_2008-2,
	title = {A graph method for keyword-based selection of the {top-K}
               databases},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/sigmod/VuOPT08},
	author = {Quang Hieu Vu and Beng Chin Ooi and Dimitris Papadias and Anthony K. H. Tung},
	year = {2008},
	keywords = {top-k},
	pages = {915--926}
},

@article{wilschut_dataflow_1993,
	title = {Dataflow query execution in a parallel main-memory environment},
	volume = {1},
	url = {http://dx.doi.org/10.1007/BF01277522},
	doi = {10.1007/BF01277522},
	abstract = {In this paper, the performance and characteristics of the execution of various join-trees on a parallel {DBMS} are studied. The results of this study are a step into the direction of the design of a query optimization strategy that is fit for parallel execution of complex queries.},
	number = {1},
	journal = {Distributed and Parallel Databases},
	author = {Annita N. Wilschut and Peter M. G. Apers},
	month = jan,
	year = {1993},
	pages = {103--128}
},

@inproceedings{vijayshankar_raman_using_2003,
	title = {Using state modules for adaptive query processing},
	url = {10.1109/ICDE.2003.1260805},
	doi = {10.1109/ICDE.2003.1260805},
	abstract = {We present a query architecture in which join operators are decomposed into their constituent data structures {(State} Modules, or {SteMs),} and dataflow among these {SteMs} is managed adaptively by an eddy routing operator {[R.} Avnur et al., (2000)]. Breaking the encapsulation of joins serves two purposes. First, it allows the eddy to observe multiple physical operations embedded in a join algorithm, allowing for better calibration and control of these operations. Second, the {SteM} on a relation serves as a shared materialization point, enabling multiple competing access methods to share results, which can be leveraged by multiple competing join algorithms. Our architecture extends prior work significantly, allowing continuously adaptive decisions for most major aspects of traditional query optimization: choice of access methods and join algorithms, ordering of operators, and choice of a query spanning tree. {SteMs} introduce significant routing flexibility to the eddy, enabling more opportunities for adaptation, but also introducing the possibility of incorrect query results. We present constraints on eddy routing through {SteMs} that ensure correctness while preserving a great deal of flexibility. We also demonstrate the benefits of our architecture via experiments in the Telegraph dataflow system. We show that even a simple routing policy allows significant flexibility in adaptation, including novel effects like automatic "hybridization " of multiple algorithms for a single join.},
	booktitle = {Data Engineering, 2003. Proceedings. 19th International Conference on},
	author = {Vijayshankar Raman and A. Deshpande and {J.M.} Hellerstein},
	year = {2003},
	keywords = {adaptive query processing, data encapsulation, data structure, eddy routing, eddy routing operator, join operator, multiple algorithm automatic hybridization, multiple competing join algorithm, query architecture, query processing, query spanning tree, routing policy, shared materialization point, State Module, {SteMs,} Telegraph dataflow system, tree data structures},
	pages = {353--364}
},

@inproceedings{zeinalipour-yazti_threshold_2005,
	title = {The threshold join algorithm for top-k queries in distributed
               sensor networks},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/dmsn/Zeinalipour-YaztiVGKTVKS05},
	author = {Demetrios {Zeinalipour-Yazti} and Zografoula Vagena and Dimitrios Gunopulos and Vana Kalogeraki and Vassilis J. Tsotras and Michail Vlachos and Nick Koudas and Divesh Srivastava},
	year = {2005},
	pages = {61--66}
},

@inproceedings{dunlu_peng_wsprc:_2009,
	title = {{WSPRC:} An Adaptive Model for Query Optimization over Web Services},
	shorttitle = {{WSPRC}},
	url = {10.1109/WISM.2009.78},
	doi = {10.1109/WISM.2009.78},
	abstract = {Query optimization over Web services is very important for data integration with Web services and has gained much attention in recent year. In this work, we propose an adaptive query optimization model for Web services, that is, Web Service {Profiler-Reoptimizer-Cache} {(WSPRC).} One of the core components of the model is Reoptimizer which is based on the adaptive greedy algorithm {(A-Greedy)} and analyzes the implementation of Web service query optimization process. We compare the traditional Greedy with {A-Greedy} from several aspects. The comparison and experimental results show that the {WSPRC} model and {A-Greedy} improve the efficiency of querying optimization over Web services.},
	booktitle = {Web Information Systems and Mining, 2009. {WISM} 2009. International Conference on},
	author = {Dunlu Peng and Chen Li and Feng Zhu and Ning Zhang},
	year = {2009},
	keywords = {a-greedy, adaptive greedy algorithm, adaptive query optimization model, data integration, greedy algorithms, query processing, Web Service {Profiler-Reoptimizer-Cache,} Web services, wsprc},
	pages = {347--351}
},

@article{pu_keyword_2008,
	title = {Keyword query cleaning},
	volume = {1},
	url = {http://portal.acm.org/citation.cfm?id=1453856.1453955&coll=GUIDE&dl=GUIDE&CFID=75531709&CFTOKEN=65038685},
	doi = {10.1145/1453856.1453955},
	abstract = {Unlike traditional database queries, keyword queries do not adhere to predefined syntax and are often dirty with irrelevant words from natural languages. This makes accurate and efficient keyword query processing over databases a very challenging task.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {Ken Q. Pu and Xiaohui Yu},
	year = {2008},
	pages = {909--920}
},

@inproceedings{marian_adaptive_2005,
	title = {Adaptive Processing of {Top-K} Queries in {XML}},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/icde/MarianAKS05},
	author = {Amélie Marian and Sihem {Amer-Yahia} and Nick Koudas and Divesh Srivastava},
	year = {2005},
	pages = {162--173}
},

@incollection{liarou_evaluating_2006,
	title = {Evaluating Conjunctive Triple Pattern Queries over Large Structured Overlay Networks},
	url = {http://dx.doi.org/10.1007/11926078_29},
	abstract = {We study the problem of evaluating conjunctive queries composed of triple patterns over {RDF} data stored in distributed hash
tables. Our goal is to develop algorithms that scale to large amounts of {RDF} data, distribute the query processing load evenly
and incur little network traffic. We present and evaluate two novel query processing algorithms with these possibly conflicting
goals in mind. We discuss the various tradeoffs that occur in our setting through a detailed experimental evaluation of the
proposed algorithms.},
	booktitle = {The Semantic Web - {ISWC} 2006},
	author = {Erietta Liarou and Stratos Idreos and Manolis Koubarakis},
	year = {2006},
	pages = {399--413}
},

@article{halevy_answering_2001,
	title = {Answering queries using views: A survey},
	volume = {10},
	issn = {1066-8888},
	shorttitle = {Answering queries using views},
	url = {http://dx.doi.org/10.1007/s007780100054},
	doi = {http://dx.doi.org/10.1007/s007780100054},
	abstract = {The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.},
	journal = {The {VLDB} Journal — The International Journal on Very Large Data Bases},
	author = {Alon Y Halevy},
	month = dec,
	year = {2001},
	note = {{ACM} {ID:} 767151},
	keywords = {algorithms, data integration, data warehouse and repository, date warehousing, experimentation, materialized views, query optimization, query processing, search process, survey, theory, web-site management},
	pages = {270–294}
},

@inproceedings{haas_ripple_1999-1,
	address = {Philadelphia, Pennsylvania, United States},
	title = {Ripple joins for online aggregation},
	isbn = {1-58113-084-8},
	url = {http://portal.acm.org/citation.cfm?id=304182.304208&coll=GUIDE&dl=GUIDE&CFID=87027543&CFTOKEN=93311932},
	doi = {10.1145/304182.304208},
	abstract = {We present a new family of join algorithms, called ripple joins, for online processing of multi-table aggregation queries in a relational database management system {(DBMS).} Such queries arise naturally in interactive exploratory decision-support applications. Traditional offline join algorithms are designed to minimize the time to completion of the query. In contrast, ripple joins are designed to minimize the time until an acceptably precise estimate of the query result is available, as measured by the length of a confidence interval. Ripple joins are adaptive, adjusting their behavior during processing in accordance with the statistical properties of the data. Ripple joins also permit the user to dynamically trade off the two key performance factors of on-line aggregation: the time between successive updates of the running aggregate, and the amount by which the confidence-interval length decreases at each update. We show how ripple joins can be implemented in an existing {DBMS} using iterators, and we give an overview of the methods used to compute confidence intervals and to adaptively optimize the ripple join “aspect-ratio” parameters. In experiments with an initial implementation of our algorithms in the {POSTGRES} {DBMS,} the time required to produce reasonably precise online estimates was up to two orders of magnitude smaller than the time required for the best offline join algorithms to produce exact answers.},
	booktitle = {Proceedings of the 1999 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Peter J. Haas and Joseph M. Hellerstein},
	year = {1999},
	pages = {287--298}
},

@inproceedings{ilyas_joining_2002,
	title = {Joining Ranked Inputs in Practice},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/vldb/IlyasAE02},
	author = {Ihab F. Ilyas and Walid G. Aref and Ahmed K. Elmagarmid},
	year = {2002},
	pages = {950--961}
},

@inproceedings{schmidt_experimental_2008,
	address = {Karlsruhe, Germany},
	title = {An Experimental Comparison of {RDF} Data Management Approaches in a {SPARQL} Benchmark Scenario},
	isbn = {978-3-540-88563-4},
	url = {http://portal.acm.org/citation.cfm?id=1483155.1483163},
	abstract = {Efficient {RDF} data management is one of the cornerstones in realizing the Semantic Web vision. In the past, different {RDF} storage strategies have been proposed, ranging from simple triple stores to more advanced techniques like clustering or vertical partitioning on the predicates. We present an experimental comparison of existing storage strategies on top of the {SP2Bench} {SPARQL} performance benchmark suite and put the results into context by comparing them to a purely relational model of the benchmark scenario. We observe that (1) in terms of performance and scalability, a simple triple store built on top of a column-store {DBMS} is competitive to the vertically partitioned approach when choosing a physical (predicate, subject, object) sort order, (2) in our scenario with real-world queries, none of the approaches scales to documents containing tens of millions of {RDF} triples, and (3) none of the approaches can compete with a purely relational model. We conclude that future research is necessary to further bring forward {RDF} data management.},
	booktitle = {Proceedings of the 7th International Conference on The Semantic Web},
	publisher = {{Springer-Verlag}},
	author = {Michael Schmidt and Thomas Hornung and Norbert Küchlin and Georg Lausen and Christoph Pinkel},
	year = {2008},
	pages = {82--97}
},

@inproceedings{stocker_sparql_2008,
	address = {Beijing, China},
	title = {{SPARQL} basic graph pattern optimization using selectivity estimation},
	isbn = {978-1-60558-085-2},
	url = {http://portal.acm.org/citation.cfm?id=1367497.1367578},
	doi = {10.1145/1367497.1367578},
	abstract = {In this paper, we formalize the problem of Basic Graph Pattern {(BGP)} optimization for {SPARQL} queries and main memory graph implementations of {RDF} data. We define and analyze the characteristics of heuristics for selectivity-based static {BGP} optimization. The heuristics range from simple triple pattern variable counting to more sophisticated selectivity estimation techniques. Customized summary statistics for {RDF} data enable the selectivity estimation of joined triple patterns and the development of efficient heuristics. Using the Lehigh University Benchmark {(LUBM),} we evaluate the performance of the heuristics for the queries provided by the {LUBM} and discuss some of them in more details.},
	booktitle = {Proceeding of the 17th international conference on World Wide Web},
	publisher = {{ACM}},
	author = {Markus Stocker and Andy Seaborne and Abraham Bernstein and Christoph Kiefer and Dave Reynolds},
	year = {2008},
	keywords = {query optimization, selectivity estimation, sparql},
	pages = {595--604}
},

@article{poosala_selectivity_1997,
	title = {Selectivity Estimation Without the Attribute Value Independence Assumption},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.105.8126},
	author = {Viswanath Poosala},
	year = {1997},
	pages = {486---495}
},

@inproceedings{tril_fast_2007,
	address = {Beijing, China},
	title = {Fast and practical indexing and querying of very large graphs},
	isbn = {978-1-59593-686-8},
	url = {http://portal.acm.org/citation.cfm?doid=1247480.1247573},
	doi = {10.1145/1247480.1247573},
	abstract = {Many applications work with graph-structured data. As graphs grow in size, indexing becomes essential to ensure sufficient query performance. We present the {GRIPP} index structure {(GRaph} Indexing based on Pre- and Postorder numbering) for answering reachability queries in graphs.},
	booktitle = {Proceedings of the 2007 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Silke Trißl and Ulf Leser},
	year = {2007},
	keywords = {databases, graph indexing, reachability queries},
	pages = {845--856}
},

@article{kossmann_state_2000,
	title = {The state of the art in distributed query processing},
	volume = {32},
	url = {http://portal.acm.org/citation.cfm?doid=371578.371598},
	doi = {10.1145/371578.371598},
	abstract = {Distributed data processing is becoming a reality. Businesses want to do it for many reasons, and they often must do it in order to stay competitive. While much of the infrastructure for distributed data processing is already there (e.g., modern network technology), a number of issues make distributed data processing still a complex undertaking: (1) distributed systems can become very large, involving thousands of heterogeneous sites including {PCs} and mainframe server machines; (2) the state of a distributed system changes rapidly because the load of sites varies over time and new sites are added to the system; (3) legacy systems need to be integrated—such legacy systems usually have not been designed for distributed data processing and now need to interact with other (modern) systems in a distributed environment. This paper presents the state of the art of query processing for distributed database and information systems. The paper presents the “textbook” architecture for distributed query processing and a series of techniques that are particularly useful for distributed database systems. These techniques include special join techniques, techniques to exploit intraquery paralleli sm, techniques to reduce communication costs, and techniques to exploit caching and replication of data. Furthermore, the paper discusses different kinds of distributed systems such as client-server, middleware (multitier), and heterogeneous database systems, and shows how query processing works in these systems.},
	number = {4},
	journal = {{ACM} Comput. Surv.},
	author = {Donald Kossmann},
	year = {2000},
	keywords = {caching, client-server databases, database application systems, dissemination-based information systems, economic models for query processing, middleware, multitier architectures, query execution, query optimization, replication, wrappers},
	pages = {422--469}
},

@inproceedings{sullivan_tribeca:_1998,
	address = {New Orleans, Louisiana},
	title = {Tribeca: a system for managing large databases of network traffic},
	shorttitle = {Tribeca},
	url = {http://portal.acm.org/citation.cfm?id=1268258},
	abstract = {The engineers who analyze traffic on high bandwidth networks must filter and aggregate either recorded traces of network packets or live traffic from the network itself. These engineers perform operations similar to database queries, but cannot use conventional data managers because of performance concerns and a semantic mismatch between the analysis operations and the operations supported by commercial {DBMSs.} Traffic analysis does not require fast random access, transactional update, or relational joins. Rather, it needs fast sequential access to a stream of traffic records and the ability to filter, aggregate, define windows, demultiplex, and remultiplex the stream.},
	booktitle = {Proceedings of the annual conference on {USENIX} Annual Technical Conference},
	publisher = {{USENIX} Association},
	author = {Mark Sullivan and Andrew Heybey},
	year = {1998},
	pages = {2--2}
},

@inproceedings{ivanova_architecture_2009,
	address = {Providence, Rhode Island, {USA}},
	title = {An architecture for recycling intermediates in a column-store},
	isbn = {978-1-60558-551-2},
	url = {http://portal.acm.org/citation.cfm?id=1559879},
	doi = {10.1145/1559845.1559879},
	abstract = {Automatically recycling (intermediate) results is a grand challenge for state-of-the-art databases to improve both query response time and throughput. Tuples are loaded and streamed through a tuple-at-a-time processing pipeline avoiding materialization of intermediates as much as possible. This limits the opportunities for reuse of overlapping computations to {DBA-defined} materialized views and function/result cache tuning.},
	booktitle = {Proceedings of the 35th {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Milena G. Ivanova and Martin L. Kersten and Niels J. Nes and Romulo {A.P.} Gonçalves},
	year = {2009},
	keywords = {caching, column-stores, databasekernels},
	pages = {309--320}
},

@inproceedings{hristidis_ranked_2010,
	title = {Ranked queries over sources with Boolean query interfaces without ranking support},
	doi = {10.1109/ICDE.2010.5447918},
	abstract = {Many online or local data sources provide powerful querying mechanisms but limited ranking capabilities. For instance, {PubMed} allows users to submit highly expressive Boolean keyword queries, but ranks the query results by date only. However, a user would typically prefer a ranking by relevance, measured by an Information Retrieval {(IR)} ranking function. The naive approach would be to submit a disjunctive query with all query keywords, retrieve the returned documents, and then re-rank them. Unfortunately, such an operation would be very expensive due to the large number of results returned by disjunctive queries. In this paper we present algorithms that return the top results for a query, ranked according to an {IR-style} ranking function, while operating on top of a source with a Boolean query interface with no ranking capabilities (or a ranking capability of no interest to the end user). The algorithms generate a series of conjunctive queries that return only documents that are candidates for being highly ranked according to a relevance metric. Our approach can also be applied to other settings where the ranking is monotonic on a set of factors (query keywords in {IR)} and the source query interface is a Boolean expression of these factors. Our comprehensive experimental evaluation on the {PubMed} database and {TREC} dataset show that we achieve order of magnitude improvement compared to the current baseline approaches.},
	booktitle = {Data Engineering {(ICDE),} 2010 {IEEE} 26th International Conference on},
	author = {V. Hristidis and Yuheng Hu and {P.G.} Ipeirotis},
	year = {2010},
	keywords = {Boolean query interfaces, conjunctive queries, disjunctive query, information retrieval ranking function, {PubMed} algorithm, query keywords, query processing, query ranking, querying mechanisms, ranking support, relevance feedback, relevance metric, relevance ranking, user interfaces},
	pages = {872--875}
},

@article{kossmann_iterative_2000,
	title = {Iterative dynamic programming: a new class of query optimization algorithms},
	volume = {25},
	shorttitle = {Iterative dynamic programming},
	url = {http://portal.acm.org/citation.cfm?id=352982&dl=GUIDE&coll=GUIDE&CFID=64839601&CFTOKEN=92982021},
	doi = {10.1145/352958.352982},
	abstract = {The query optimizer is one of the most important components of a database system. Most commercial query optimizers today are based on a dynamic-programming algorithm, as proposed in Selinger et al. [1979]. While this algorithm produces good optimization results (i.e, good plans), its high complexity can be prohibitive if complex queries need to be processed, new query execution techniques need to be integrated, or in certain programming environments (e.g., distributed database systems). In this paper, we present and thoroughly evaluate a new class of query optimization algorithms that are based on a principle that we call iterative dynamic programming, or {IDP} for short. {IDP} has several important advantages: First, {IDP-algorithms} produce the best plans of all known algorithms in situations in which dynamic programming is not viable because of its high complexity. Second, some {IDP} variants are adaptive and produce as good plans as dynamic programming if dynamic programming is viable and as good-as possible plans if dynamic programming turns out to be not viable. Three, all {IDP-algorithms} can very easily be integrated into an existing optimizer which is based on dynamic programming.},
	number = {1},
	journal = {{ACM} Trans. Database Syst.},
	author = {Donald Kossmann and Konrad Stocker},
	year = {2000},
	keywords = {dynamic programming, greedy algorithm, iterative dynamic programming, plan evaluation function, query optimization, randomized optimization},
	pages = {43--82}
},

@article{mahalingam_multi-criteria_2004,
	title = {{Multi-Criteria} Query Optimization in the Presence of Result Size and Quality Tradeoffs},
	volume = {23},
	issn = {1380-7501},
	url = {http://www.springerlink.com/content/k55543l124364182/},
	doi = {10.1023/B:MTAP.0000031755.50716.2a},
	number = {3},
	journal = {Multimedia Tools and Applications},
	author = {Lakshmi Priya Mahalingam and K. Selçuk Candan},
	year = {2004},
	pages = {167--183}
},

@inproceedings{viglas_maximizing_2003,
	address = {Berlin, Germany},
	title = {Maximizing the output rate of multi-way join queries over streaming information sources},
	isbn = {0-12-722442-4},
	url = {http://portal.acm.org/citation.cfm?id=1315477},
	abstract = {Recently there has been a growing interest in join query evaluation for scenarios in which inputs arrive at highly variable and unpredictable rates. In such scenarios, the focus shifts from completing the computation as soon as possible to producing a prefix of the output as soon as possible. To handle this shift in focus, most solutions to date rely upon some combination of streaming binary operators and "on-the-fly" execution plan reorganization. In contrast, we consider the alternative of extending existing symmetric binary join operators to handle more than two inputs. Toward this end, we have completed a prototype implementation of a multi-way join operator, which we term the {"MJoin"} operator, and explored its performance. Our results show that in many instances the {MJoin} produces outputs sooner than any tree of binary operators. Additionally, since {MJoins} are completely symmetric with respect to their inputs, they can reduce the need for expensive runtime plan reorganization. This suggests that supporting multiway joins in a single, symmetric, streaming operator may be a useful addition to systems that support queries over input streams from remote sites.},
	booktitle = {Proceedings of the 29th international conference on Very large data bases - Volume 29},
	publisher = {{VLDB} Endowment},
	author = {Stratis D. Viglas and Jeffrey F. Naughton and Josef Burger},
	year = {2003},
	pages = {285--296}
},

@article{ntarmos_statistical_2009,
	title = {Statistical structures for Internet-scale data management},
	volume = {18},
	url = {http://dx.doi.org/10.1007/s00778-009-0140-7},
	doi = {10.1007/s00778-009-0140-7},
	abstract = {Abstract  Efficient query processing in traditional database management systems relies on statistics on base data. For centralized systems,
there is a rich body of research results on such statistics, from simple aggregates to more elaborate synopses such as sketches
and histograms. For Internet-scale distributed systems, on the other hand, statistics management still poses major challenges.
With the work in this paper we aim to endow peer-to-peer data management over structured overlays with the power associated
with such statistical information, with emphasis on meeting the scalability challenge. To this end, we first contribute efficient,
accurate, and decentralized algorithms that can compute key aggregates such as Count, {CountDistinct,} Sum, and Average. We
show how to construct several types of histograms, such as simple {Equi-Width,} {Average-Shifted} {Equi-Width,} and {Equi-Depth} histograms.
We present a full-fledged open-source implementation of these tools for distributed statistical synopses, and report on a
comprehensive experimental performance evaluation, evaluating our contributions in terms of efficiency, accuracy, and scalability.},
	number = {6},
	journal = {The {VLDB} Journal},
	author = {Nikos Ntarmos and Peter Triantafillou and Gerhard Weikum},
	month = dec,
	year = {2009},
	pages = {1279--1312}
},

@article{fletcher_scalable_2009,
	title = {Scalable indexing of {RDF} graphs for efficient join processing},
	journal = {Proceedings of {CIKM.} {ACM}},
	author = {G. {H.L} Fletcher and P. W Beck},
	year = {2009}
},

@inproceedings{wang_hermes:_2009,
	address = {Providence, Rhode Island, {USA}},
	title = {Hermes: a travel through semantics on the data web},
	isbn = {978-1-60558-551-2},
	shorttitle = {Hermes},
	url = {http://portal.acm.org/citation.cfm?doid=1559845.1560002},
	doi = {10.1145/1559845.1560002},
	abstract = {The Web as a global information space is developing from a Web of documents to a Web of data. This development opens new ways for addressing complex information needs. Search is no longer limited to matching keywords against documents, but instead complex information needs can be expressed in a structured way, with precise answers as results. In this paper, we demonstrate Hermes, an infrastructure for data web search. To provide an end-user oriented interface, we support expressive keyword search by translating user information needs into structured queries. We integrate heterogeneous web data sources with automatically computed mappings. Schema-level mappings are exploited in constructing structured queries against the integrated schema. These structured queries are decomposed into queries against the local web data sources, which are then processed in a distributed way.},
	booktitle = {Proceedings of the 35th {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Haofen Wang and Thomas Penin and Kaifeng Xu and Junquan Chen and Xinruo Sun and Linyun Fu and Qiaoling Liu and Yong Yu and Thanh Tran and Peter Haase and Rudi Studer},
	year = {2009},
	keywords = {data integration, keyword search, structured search, web of data},
	pages = {1135--1138}
},

@inproceedings{markowetz_reachability_2009,
	title = {Reachability Indexes for Relational Keyword Search},
	isbn = {978-0-7695-3545-6},
	url = {http://portal.acm.org/citation.cfm?id=1546683.1547417},
	abstract = {Due to its considerable ease of use, relational keyword search {(R-KWS)} has become increasingly popular. Its simplicity, however, comes at the cost of intensive query processing. Specifically, {R-KWS} explores a vast search space, comprised of all possible combinations of keyword occurrences in any attribute of every table. Existing systems follow two general methodologies for query processing: (i) graph based, which traverses a materialized data graph, and (ii) operator based, which executes relational operator trees on an underlying {DBMS.} In both cases, computations are largely wasted on graph traversals or operator tree executions that fail to return results. Motivated by this observation, we introduce a comprehensive framework for reachability indexing that eliminates such fruitless operations. We describe a range of indexes that capture various types of join reachability. Extensive experiments demonstrate that the proposed techniques significantly improve performance, often by several orders of magnitude.},
	booktitle = {Proceedings of the 2009 {IEEE} International Conference on Data Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Alexander Markowetz and Yin Yang and Dimitris Papadias},
	year = {2009},
	keywords = {keywoed search, reachability indexing, relational databases},
	pages = {1163--1166}
},

@inproceedings{schenk_networked_2008,
	address = {Beijing, China},
	title = {Networked graphs: a declarative mechanism for {SPARQL} rules, {SPARQL} views and {RDF} data integration on the web},
	isbn = {978-1-60558-085-2},
	shorttitle = {Networked graphs},
	url = {http://portal.acm.org/citation.cfm?id=1367497.1367577},
	doi = {10.1145/1367497.1367577},
	abstract = {Easy reuse and integration of declaratively described information in a distributed setting is one of the main motivations for building the Semantic Web. Despite of this claim, reuse and recombination of {RDF} data today is mostly done using data replication and procedural code. A simple declarative mechanism for reusing and combining {RDF} data would help users to generate content for the semantic web. Having such a mechanism, the Semantic Web could better benefit from user generated content, as it is broadly present in the so called Web 2.0, but also from better linkage of existing content.},
	booktitle = {Proceeding of the 17th international conference on World Wide Web},
	publisher = {{ACM}},
	author = {Simon Schenk and Steffen Staab},
	year = {2008},
	keywords = {distributed rules, rules, semantic Web, sparql, views, well founded semantics},
	pages = {585--594}
},

@article{ives_xml_2002,
	title = {An {XML} query engine for network-bound data},
	volume = {11},
	url = {http://portal.acm.org/citation.cfm?id=764199.764206},
	abstract = {{XML} has become the lingua franca for data exchange and integration across administrative and enterprise boundaries. Nearly all data providers are adding {XML} import or export capabilities, and standard {XML} Schemas and {DTDs} are being promoted for all types of data sharing. The ubiquity of {XML} has removed one of the major obstacles to integrating data from widely disparate sources - namely, the heterogeneity of data formats. However, general-purpose integration of data across the wide are a also requires a query processor that can query data sources on demand, receive streamed {XML} data from them, and combine and restructure the data into new {XML} output - while providing good performance for both batch-oriented and ad hoc, interactive queries. This is the goal of the Tukwila data integration system, the first system that focuses on network-bound, dynamic {XML} data sources. In contrast to previous approaches, which must read, parse, and often store entire {XML} objects before querying them, Tukwila can return query results even as the data is streaming into the system. Tukwila is built with a new system architecture that extends adaptive query processing and relational-engine techniques into the {XML} realm, as facilitated by a pair of operators that incrementally evaluate a query's input path expressions as data is read. In this paper, we describe the Tukwila architecture and its novel aspects, and we experimentally demonstrate that Tukwila provides better overall query performance and faster initial answers than existing systems, and has excellent scalability.},
	number = {4},
	journal = {The {VLDB} Journal},
	author = {Zachary G. Ives and A. Y. Halevy and D. S. Weld},
	year = {2002},
	keywords = {data integration, data streams, query processing, web and databases, xml},
	pages = {380--402}
},

@inproceedings{madden_fjording_2002,
	address = {Los Alamitos, {CA,} {USA}},
	title = {Fjording the Stream: An Architecture for Queries Over Streaming Sensor Data},
	volume = {0},
	isbn = {0-7695-1531-2},
	shorttitle = {Fjording the Stream},
	doi = {http://doi.ieeecomputersociety.org/10.1109/ICDE.2002.994774},
	abstract = {If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.},
	booktitle = {Data Engineering, International Conference on},
	publisher = {{IEEE} Computer Society},
	author = {Samuel Madden and Michael J Franklin},
	year = {2002},
	keywords = {databases, fjords, query processing, sensors, streaming},
	pages = {0555}
},

@inproceedings{quanzhong_li_adaptively_2007,
	title = {Adaptively Reordering Joins during Query Execution},
	url = {10.1109/ICDE.2007.367848},
	doi = {10.1109/ICDE.2007.367848},
	abstract = {Traditional query processing techniques based on static query optimization are ineffective in applications where statistics about the data are unavailable at the start of query execution or where the data characteristics are skewed and change dynamically. Several adaptive query processing techniques have been proposed in recent years to overcome the limitations of static query optimizers through either explicit re-optimization of plans during execution or by using a row-routing based approach. In this paper, we present a novel method for processing pipelined join plans that dynamically arranges the join order of both inner and outer-most tables at run-time. We extend the Eddies concept of "moments of symmetry" to reorder indexed nested-loop joins, the join method used by all commercial {DBMSs} for building pipelined query plans for applications for which low latencies are crucial. Unlike row-routing techniques, our approach achieves adaptability by changing the pipeline itself which avoids the bookkeeping and routing decision associated with each row. Operator selectivities monitored during query execution are used to change the execution plan at strategic points, and the change of execution plans utilizes a novel and efficient technique for avoiding duplicates in the query results. Our prototype implementation in a commercial {DBMS} shows a query execution speedup of up to 8 times.},
	booktitle = {Data Engineering, 2007. {ICDE} 2007. {IEEE} 23rd International Conference on},
	author = {Quanzhong Li and Minglong Sha and V. Markl and K. Beyer and L. Colby and G. Lohman},
	year = {2007},
	keywords = {adaptive query processing, adaptive systems, data characteristics, database management system, database management systems, join method, join order, pipelined join plans, pipelined query plans, query execution, query processing, routing decision, static query optimization},
	pages = {26--35}
},

@article{bizer_linked_2009,
	title = {Linked Data - The Story So Far},
	abstract = {The term Linked Data refers to a set of best practices for publishing and connecting
structured data on the Web. These best practices have been adopted by an increasing
number of data providers over the last three years, leading to the creation of a global data
space containing billions of assertions - the Web of Data. In this article we present the
concept and technical principles of Linked Data, and situate these within the broader context
of related technological developments. We describe progress to date in publishing Linked
Data on the Web, review applications that have been developed to exploit the Web of Data,
and map out a research agenda for the Linked Data community as it moves forward.},
	journal = {International Journal on Semantic Web and Information Systems {(IJSWIS)}},
	author = {Christian Bizer and Tom Heath and Tim {Berners-Lee} and T Heath and M Hepp and C Bizer},
	year = {2009},
	keywords = {linkeddata}
},

@article{olken_random_1995,
	title = {Random sampling from databases: a survey},
	volume = {5},
	shorttitle = {Random sampling from databases},
	url = {http://dx.doi.org/10.1007/BF00140664},
	doi = {10.1007/BF00140664},
	abstract = {This paper reviews recent literature on techniques for obtaining random samples from databases. We begin with a discussion of why one would want to include sampling facilities in database management systems. We then review basic sampling techniques used in constructing {DBMS} sampling algorithms, e.g. acceptance/rejection and reservoir sampling. A discussion of sampling from various data structures follows: B+ trees, hash files, spatial data structures (including R-trees and quadtrees). Algorithms for sampling from simple relational queries, e.g. single relational operators such as selection, intersection, union, set difference, projection, and join are then described. We then describe sampling for estimation of aggregates (e.g. the size of query results). Here we discuss both clustered sampling, and sequential sampling approaches. Decision-theoretic approaches to sampling for query optimization are reviewed.},
	number = {1},
	journal = {Statistics and Computing},
	author = {Frank Olken and Doron Rotem},
	month = mar,
	year = {1995},
	pages = {25--42}
},

@article{lipton_efficient_1993,
	title = {Efficient sampling strategies for relational database operations},
	volume = {116},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/B6V1G-45FC428-25/2/61b557f3ca816932a3f8e3637d782972},
	doi = {10.1016/0304-3975(93)90224-H},
	abstract = {Recently, we have proposed an adaptive, random-sampling algorithm for general query size estimation in databases. In an earlier work we analyzed the asymptotic efficiency and accuracy of the algorithm; in this paper we investigate its practicality as applied to the relational database operations select, project, and join. We extend our previous analysis to provide significantly improved bounds on the amount of sampling necessary for a given level of accuracy. Also, we provide "sanity bounds" to deal with queries for which the underlying data are extremely skewed or the query result is very small. We investigate how the existence of indices can be used to generate more efficient sampling algorithms for the operations of project and join. Finally, we report on the performance of the estimation algorithm, both as implemented in "stand alone" C programs and as implemented in a host language on a commericial relational system.},
	number = {1},
	journal = {Theoretical Computer Science},
	author = {Richard J. Lipton and Jeffrey F. Naughton and Donovan A. Schneider and S. Seshadri},
	month = aug,
	year = {1993},
	pages = {195--226}
},

@inproceedings{vlachou_efficient_2008,
	title = {On efficient top-k query processing in highly distributed
               environments},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/sigmod/VlachouDNV08},
	author = {Akrivi Vlachou and Christos Doulkeridis and Kjetil Nørvåg and Michalis Vazirgiannis},
	year = {2008},
	keywords = {top-k},
	pages = {753--764}
},

@inproceedings{wolf_query_2007,
	series = {{VLDB} '07},
	title = {Query processing over incomplete autonomous databases},
	isbn = {978-1-59593-649-3},
	location = {Vienna, Austria},
	url = {http://portal.acm.org/citation.cfm?id=1325851.1325926},
	booktitle = {Proceedings of the 33rd international conference on Very large data bases},
	publisher = {{VLDB} Endowment},
	author = {Garrett Wolf and Hemal Khatri and Bhaumik Chokshi and Jianchun Fan and Yi Chen and Subbarao Kambhampati},
	year = {2007},
	pages = {651–662}
},

@inproceedings{ding_finding_2007,
	title = {Finding Top-k {Min-Cost} Connected Trees in Databases},
	url = {http://search.mpi-inf.mpg.de/dblp/show.php?key=conf/icde/DingYWQZL07},
	author = {Bolin Ding and Jeffrey Xu Yu and Shan Wang and Lu Qin and Xiao Zhang and Xuemin Lin},
	year = {2007},
	keywords = {top-k},
	pages = {836--845}
},

@inproceedings{jin_efficiently_2008,
	address = {Vancouver, Canada},
	title = {Efficiently answering reachability queries on very large directed graphs},
	isbn = {978-1-60558-102-6},
	url = {http://portal.acm.org/citation.cfm?id=1376677&dl=GUIDE,},
	doi = {10.1145/1376616.1376677},
	abstract = {Efficiently processing queries against very large graphs is an important research topic largely driven by emerging real world applications, as diverse as {XML} databases, {GIS,} web mining, social network analysis, ontologies, and bioinformatics. In particular, graph reachability has attracted a lot of research attention as reachability queries are not only common on graph databases, but they also serve as fundamental operations for many other graph queries. The main idea behind answering reachability queries in graphs is to build indices based on reachability labels. Essentially, each vertex in the graph is assigned with certain labels such that the reachability between any two vertices can be determined by their labels. Several approaches have been proposed for building these reachability labels; among them are interval labeling (tree cover) and 2-hop labeling. However, due to the large number of vertices in many real world graphs (some graphs can easily contain millions of vertices), the computational cost and (index) size of the labels using existing methods would prove too expensive to be practical. In this paper, we introduce a novel graph structure, referred to as path-tree, to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. We demonstrate both analytically and empirically the effectiveness of our new approaches.},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Ruoming Jin and Yang Xiang and Ning Ruan and Haixun Wang},
	year = {2008},
	keywords = {graph indexing, maximal directed spanning tree, path-tree cover, reachability queries, transitive closure},
	pages = {595--608}
},

@inproceedings{madden_continuously_2002,
	address = {Madison, Wisconsin},
	title = {Continuously adaptive continuous queries over streams},
	isbn = {1-58113-497-5},
	url = {http://portal.acm.org/citation.cfm?id=564698&dl=},
	doi = {10.1145/564691.564698},
	abstract = {We present a continuously adaptive, continuous query {(CACQ)} implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive cross-query sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph {CACQ} implementation is able to share physical operators --- both selections and join state --- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.},
	booktitle = {Proceedings of the 2002 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Samuel Madden and Mehul Shah and Joseph M. Hellerstein and Vijayshankar Raman},
	year = {2002},
	pages = {49--60}
},

@inproceedings{chen_new_2008,
	address = {Montreal, Quebec, Canada},
	title = {A new method for generating compressed representation of transitive closure},
	isbn = {978-1-60558-101-9},
	url = {http://portal.acm.org/citation.cfm?id=1370293},
	doi = {10.1145/1370256.1370293},
	abstract = {Let {G(V,} E) be a digraph (directed graph) with n nodes and e edges. Digraph G* = {(V,} E*) is the reflexive, transitive closure of G if (v, u) ∈ E* iff there is a path from v to u in G. If we store it as a matrix, O(n2) space is required, not suitable for large graphs. In this paper, we present a new method to reduce the space overhead to O(bn), where b is the G's width, defined to be the size of a largest node subset U of G such that for every pair of nodes u, v ∈ U, there does not exist a path from u to v or from v to u. With such a compressed data structure of transitive closure, the time for checking reachability (whether a node is reachable from another node through a path) is bounded by O(logb). In addition, this data structure can be generated in O(be') time, where e' is the size of a subset of E, which contains only those edges (u, v) such that there is no path of length ≥2 connecting u and v. We are able to show that the average value of e' is on the order of O(n1.5). Our method is suitable for both acyclic and cyclic graphs.},
	booktitle = {Proceedings of the 2008 {C{\textless}sup{\textgreater}3{\textless}/sup{\textgreater}S{\textless}sup{\textgreater}2{\textless}/sup{\textgreater}E} conference},
	publisher = {{ACM}},
	author = {Yangjun Chen},
	year = {2008},
	pages = {229--238}
},

@incollection{ding_joining_2004,
	title = {Joining Punctuated Streams},
	url = {http://www.springerlink.com/content/99t4r800ma97neef},
	abstract = {We focus on stream join optimization by exploiting the constraints that are dynamically embedded into data streams to signal the end of transmitting certain attribute values. These constraints are called punctuations. Our stream join operator, {PJoin,} is able to remove no-longer-useful data from the state in a timely manner based on punctuations, thus reducing memory overhead and improving the efficiency of probing. We equip {PJoin} with several alternate strategies for purging the state and for propagating punctuations to benefit down-stream operators. We also present an extensive experimental study to explore the performance gains achieved by purging state as well as the trade-off between different purge strategies. Our experimental results of comparing the performance of {PJoin} with {XJoin,} a stream join operator without a constraint-exploiting mechanism, show that {PJoin} significantly outperforms {XJoin} with regard to both memory overhead and throughput.},
	booktitle = {Advances in Database Technology - {EDBT} 2004},
	author = {Luping Ding and Nishant Mehta and Elke A. Rundensteiner and George T. Heineman},
	year = {2004},
	pages = {519--520}
},

@article{candea_scalable_2009,
	title = {A scalable, predictable join operator for highly concurrent data warehouses},
	volume = {2},
	url = {http://portal.acm.org/citation.cfm?id=1687627.1687659&coll=GUIDE&dl=GUIDE&idx=J1174&part=journal&WantType=Journals&title=Proceedings%20of%20the%20VLDB%20Endowment&CFID=82166857&CFTOKEN=58994927},
	abstract = {Conventional data warehouses employ the query-at-a-time model, which maps each query to a distinct physical plan. When several queries execute concurrently, this model introduces contention, because the physical plans---unaware of each other---compete for access to the underlying {I/O} and computation resources. As a result, while modern systems can efficiently optimize and evaluate a single complex data analysis query, their performance suffers significantly when multiple complex queries run at the same time.},
	number = {1},
	journal = {Proc. {VLDB} Endow.},
	author = {George Candea and Neoklis Polyzotis and Radek Vingralek},
	year = {2009},
	pages = {277--288}
},

@article{claussen_optimization_2000,
	title = {Optimization and evaluation of disjunctive queries},
	volume = {12},
	issn = {10414347},
	url = {http://www.computer.org/portal/web/csdl/abs/trans/tk/2000/02/k0238abs.htm},
	doi = {10.1109/69.842265},
	number = {2},
	journal = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {J. Claussen and A. Kemper and G. Moerkotte and K. Peithner and M. Steinbrunn},
	month = apr,
	year = {2000},
	pages = {238--260}
},

@inproceedings{babu_adaptive_2004,
	address = {Paris, France},
	title = {Adaptive ordering of pipelined stream filters},
	isbn = {1-58113-859-8},
	url = {http://portal.acm.org/citation.cfm?id=1007615},
	doi = {10.1145/1007568.1007615},
	abstract = {We consider the problem of pipelined filters, where a continuous stream of tuples is processed by a set of commutative filters. Pipelined filters are common in stream applications and capture a large class of multiway stream joins. We focus on the problem of ordering the filters adaptively to minimize processing cost in an environment where stream and filter characteristics vary unpredictably over time. Our core algorithm, {A-Greedy} (for Adaptive Greedy), has strong theoretical guarantees: If stream and filter characteristics were to stabilize, {A-Greedy} would converge to an ordering within a small constant factor of optimal. {(In} experiments {A-Greedy} usually converges to the optimal ordering.) One very important feature of {A-Greedy} is that it monitors and responds to selectivities that are correlated across filters (i.e., that are nonindependent), which provides the strong quality guarantee but incurs run-time overhead. We identify a three-way tradeoff among provable convergence to good orderings, run-time overhead, and speed of adaptivity. We develop a suite of variants of {A-Greedy} that lie at different points on this tradeoff spectrum. We have implemented all our algorithms in the {STREAM} prototype Data Stream Management System and a thorough performance evaluation is presented.},
	booktitle = {Proceedings of the 2004 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Shivnath Babu and Rajeev Motwani and Kamesh Munagala and Itaru Nishizawa and Jennifer Widom},
	year = {2004},
	pages = {407--418}
},

@article{wang_coding-based_2008,
	title = {Coding-based Join Algorithms for Structural Queries on {Graph-Structured} {XML} Document},
	volume = {11},
	url = {http://dx.doi.org/10.1007/s11280-008-0050-4},
	doi = {10.1007/s11280-008-0050-4},
	abstract = {Abstract  In many applications, {XML} documents need to be modelled as graphs. The query processing of graph-structured {XML} documents
brings new challenges. In this paper, we design a method based on labelling scheme for structural queries processing on graph-structured
{XML} documents. We give each node some labels, the reachability labelling scheme. By extending an interval-based reachability
labelling scheme for {DAG} by Rakesh et al., we design labelling schemes to support the judgements of reachability relationships
for general graphs. Based on the labelling schemes, we design graph structural join algorithms to answer the structural queries
with only ancestor-descendant relationship efficiently. For the processing of subgraph query, we design a subgraph join algorithm.
With efficient data structure, the subgraph join algorithm can process subgraph queries with various structures efficiently.
Experimental results show that our algorithms have good performance and scalability.},
	number = {4},
	journal = {World Wide Web},
	author = {Hongzhi Wang and Jianzhong Li and Wei Wang and Xuemin Lin},
	month = dec,
	year = {2008},
	pages = {485--510}
}

@inproceedings{hartig_zero_2011,
   author = {Hartig, Olaf},
   title = {{Zero-Knowledge Query Planning for an Iterator Implementation of Link Traversal Based Query Execution}},
   isbn = {},
   url = {http://dx.doi.org/10.1007/978-3-642-21034-1_11},
   year = {2011},
   booktitle={Proceedings of the 8th Extended Semantic Web Conference (ESWC)}
   
}

@inproceedings{sihjoin_2011,
        author={Ladwig, Günter and Tran, Thanh},
        title={{SIHJoin: Querying Remote and Local Linked Data}},
        year={2011},
        booktitle={Proceedings of the 8th Extended Semantic Web Conference (ESWC)}
}


@inproceedings{boerzsoenyi_skyline_2001,
	title = {The Skyline Operator},
	isbn = {0-7695-1001-9},
	url = {http://portal.acm.org/citation.cfm?id=645484.656550},
	abstract = {Stephan Börzsönyi No contact information provided yet. Bibliometrics: publication history Publication years2001-2001 Publication count2 Citation Count209 Available for download0 Downloads (6 Weeks)0 Downloads (12 Months)0 View colleagues of Stephan Börzsönyi Donald Kossmann {ETH} Systems Group donaldkethz.ch Bibliometrics: publication history Publication years1993-2010 Publication count82 Citation Count1,376 Available for download56 Downloads (6 Weeks)890 Downloads (12 Months)7,026 View colleagues of Donald Kossmann Konrad Stocker No contact information provided yet. Bibliometrics: publication history Publication years2000-2008 Publication count6 Citation Count270 Available for download3 Downloads (6 Weeks)23 Downloads (12 Months)187 View colleagues of Konrad Stocker},
	booktitle = {Proceedings of the 17th International Conference on Data Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Börzsönyi, Stephan and Kossmann, Donald and Stocker, Konrad},
	year = {2001},
	note = {{ACM} {ID:} 656550},
	pages = {421–430}
}


@inproceedings{huang_selectivity_2010,
	address = {New York, {NY}, {USA}},
	series = {{WWW} '10},
	title = {Selectivity estimation for {SPARQL} graph pattern},
	isbn = {978-1-60558-799-8},
	url = {http://doi.acm.org/10.1145/1772690.1772831},
	doi = {10.1145/1772690.1772831},
	booktitle = {Proceedings of the 19th international conference on World wide web},
	publisher = {{ACM}},
	author = {Huang, Hai and Liu, Chengfei},
	year = {2010},
	keywords = {rdf query processing, selectivity estimation},
	pages = {1115–1116}
}

@article{huebsch_architecture_2005,
	title = {The Architecture of {PIER:} an {Internet-Scale} Query Processor},
	shorttitle = {The Architecture of {PIER}},
	url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.2.4502},
	journal = {{IN} {CIDR}},
	author = {Huebsch, Ryan and Chun, Brent and Hellerstein, Joseph M and Loo, Boon Thau and Maniatis, Petros and Roscoe, Timothy and Shenker, Scott and Stoica, Ion and Yumerefendi, Aydan R},
	year = {2005},
	pages = {28---43}
}


@book{zsu_principles_2011,
	edition = {3rd},
	title = {Principles of Distributed Database Systems},
	isbn = {1441988335, 9781441988331},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Özsu, M. Tamer and Valduriez, Patrick},
	year = 2011
},

@article{stonebraker_mariposa:_1996,
	title = {Mariposa: a wide-area distributed database system},
	volume = {5},
	issn = {1066-8888},
	shorttitle = {Mariposa},
	url = {http://dx.doi.org/10.1007/s007780050015},
	doi = {10.1007/s007780050015},
	number = {1},
	journal = {The {VLDB} Journal},
	author = {Stonebraker, Michael and Aoki, Paul M. and Litwin, Witold and Pfeffer, Avi and Sah, Adam and Sidell, Jeff and Staelin, Carl and Yu, Andrew},
	month = jan,
	year = {1996},
	keywords = {Autonomy, databases, distributed systems, Economic site, Name service, Wide-area network},
	pages = {048–063}
},

@article{sheth_federated_1990,
	title = {Federated database systems for managing distributed, heterogeneous, and autonomous databases},
	volume = {22},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/96602.96604},
	doi = {10.1145/96602.96604},
	number = {3},
	journal = {{ACM} Comput. Surv.},
	author = {Sheth, Amit P. and Larson, James A.},
	month = sep,
	year = {1990},
	pages = {183–236}
}



@inproceedings{vance_rapid_1996,
	address = {New York, {NY}, {USA}},
	series = {{SIGMOD} '96},
	title = {Rapid bushy join-order optimization with Cartesian products},
	isbn = {0-89791-794-4},
	url = {http://doi.acm.org/10.1145/233269.233317},
	doi = {10.1145/233269.233317},
	booktitle = {Proceedings of the 1996 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {{ACM}},
	author = {Vance, Bennet and Maier, David},
	year = {1996},
	pages = {35–46}
}

