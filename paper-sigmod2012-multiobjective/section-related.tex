\section{Related Work}
\label{sec:related}

\textbf{Linked Data Query Processing.} The concept of executing SPARQL
queries directly over Linked Data instead of a locally stored and
indexed copy was first introduced in \cite{hartig_executing_2009},
where link traversal is used to discover sources at runtime. In
\cite{harth_data_2010} a local source index based on QTrees is used to
speed up the discovery of relevant sources. Other previous work
\cite{ladwig_linked_2010,sihjoin_2011} proposes methods for ranking
sources at runtime according to their relevancy and a mixed execution
strategy that combines both, link traversal and source indexes, to
report results early. In this work we adopt the approach from
\cite{harth_data_2010} and employ a source index without any runtime
source discovery. %  along with the push-based Symmetric Hash Join
% operator from \cite{ladwig_linked_2010,sihjoin_2011}.


% \textbf{Dynamic Programming.}

% Faster enumeration of plans: \cite{moerkotte_analysis_2006,moerkotte_dynamic_2008}

% Iterative Dynamic Programming (approx):
% \cite{kossmann_iterative_2000}

%\textbf{Federated Databases and Data Integration.} 

\textbf{Query Optimization and Processing.} There is a large amount of
database research on query optimization. The dynamic programming
solution was first proposed in \cite{selinger_access_1979} and remains a
popular approach for query optimization
\cite{moerkotte_analysis_2006,moerkotte_dynamic_2008}. There has also
been work on approximating the DP approach to increase run-time
performance in the context of distributed query processing
\cite{kossmann_iterative_2000}.

Efficiently generating optimal DAG-shaped query plans when performing
operator sharing has been addressed in
\cite{neumann_generating_2009}. In our work we also apply operator
sharing, although in a more limited fashion as we only consider the
sharing of source scan operators. We also adopt the technique for cost
calculation from \cite{neumann_generating_2009}. The efficient
execution of DAG-shaped query plans was discussed in
\cite{Neumann_2005}, where several approaches were proposed, such as
temporary tables and push-based execution. In this work we use a
push-based execution mode, which was also used by previous work on
Linked Data query processing \cite{ladwig_linked_2010,sihjoin_2011}.

Top-k query processing is concerned with efficiently reporting only
the most important (top-k) answers to a given query and connects to
many fields of database research, such as join algorithms, query
optimization and indexing methods. A survey of top-k processing
techniques is provided in \cite{ilyas_survey_2008}. Our approach for
multi-objective optimization differs from top-k processing in two
important ways: 1) while our approach also focuses on reporting only a
subset of all results, this subset is not fixed as in top-k
processing, but the output is whole set of query plans, representing
the trade-off between cost and cardinality; 2) top-k processing
focuses on optimizing a single objective, whereas our approach
supports optimization of several, conflicting objectives.

% state of the art distributed kossmann_state_2000

\textbf{Source Selection.} The problem of selecting relevant sources
has also been a topic in database and data integration research. In
this setting, sources are described not only by their content, but
also their capabilities. Algorithms have been proposed to efficiently
perform source selection by using the source characteristics to prune
the search space \cite{levy_querying_1996}. However, in these
approaches, source selection is a separate step, decoupled from query
optimization.

In \cite{nie_joint_2001} the authors recognize that the decoupling of
source selection and query optimization might lead to overall
sub-optimal plans and propose a solution that optimizes not only for
cost, but also for coverage (i.e., output cardinality). This is
achieved by combining cost and coverage into a single measure by means
of a (weighted) utility function. Then, classic query optimization
algorithms, such as dynamic programming, can be applied. In our work,
we do not use a utility function to combine multiple objectives, but
instead produce a set of Pareto-optimal plans that truly represent the
trade-off between the optimization objectives.


%\textbf{Query Optimization.} 

\textbf{Skyline Queries.} The skyline operation retrieves the Pareto
set from a potentially large set of points and is used in conjunction
with standard relational algebra \cite{boerzsoenyi_skyline_2001}. The
main focus of research has been the efficient execution of such
queries by means of specialized algorithms and indexes. Our approach
is different as we do not perform skylining on the data, but rather on
the query plans. However, techniques developed for skylining can be
adopted to improve the efficiency of the multi-objective DP planner,
e.g. for efficiently selecting the Pareto-optimal (skyline) set of
plans when pruning.


\textbf{Multiobjective Query Optimization.} To the best of our
knowledge, \cite{papadimitriou_multiobjective_2001} is the only work
addressing multi-objective query optimization, where it is discussed
in the context of Mariposa \cite{stonebraker_mariposa:_1996}, a
wide-area database system. The Mariposa optimizer splits the query
tree into subqueries and then obtains \emph{bids} from participating
database sites that specify a delay and cost for delivering the result
of a subquery. The goal of the proposed multi-objective optimzer
\cite{papadimitriou_multiobjective_2001} is to obtain the Pareto
optimal set of plans with respect to cost and delay. While the authors
do employ dynamic programming to show that the Pareto set can be
approximated in polynomial time, it is not based on the classic
dynamic programming algorithm for query optimization proposed in
\cite{selinger_access_1979}. The problem is slightly different as
there is only a single query operation tree and for each operation
node the optimizer is provided a list of alternatives for implementing
the operation. In contrast, the classic dynamic programming
\cite{selinger_access_1979} does not consider only a single query tree
(and therefore a single order of operations), but builds and optimizes
physical query plans from the bottom up and considers all valid query
trees. In this work we extend the classic algorithm to support
multi-objective query optimization.



% \textbf{Data Integration.}

% Bucket algorithm: \cite{levy_querying_1996}

% MiniCon algorithm: \cite{pottinger_minicon:_2001}

% Optimization of cost and coverage (utility function):
% \cite{nie_joint_2001}

% Source selection: \cite{pomares_source_2010}

% Query Planning in the Presence of Overlapping Sources:
% \cite{bleiholder_query_2006}

% survey materialized views: \cite{halevy_answering_2001}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
